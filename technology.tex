\section{Technology}
\label{sec:technology}

The technological foundations of foundation models give rise to the capabilities (\refsec{capabilities}) that determine their potential. 
To understand the technology used in development, we consider the data (\refsec{data}), model architectures (\refsec{modeling}) and systems (\refsec{systems}) used to train (\refsec{training}), and further adapt, (\refsec{adaptation}) these models alongside the theory (\refsec{theory}) that should be developed to understand this paradigm.
To then understand the resulting models, we discuss how to evaluate (\refsec{evaluation}) and interpret (\refsec{interpretability}) alongside the importance of robustness (\refsec{robustness}), security and privacy (\refsec{security}), and long-term AI safety (\refsec{ai-safety}) for ensuring the reliability of these models when deployed in society (\refsec{society}).

\pl{say that much of this section is the focus on the ML community, but really many of these sections has societal implications, and that we have tried to make these connections explicitly
}

% Now we discuss the technology behind building better model architectures,
% training and adaptation procedures, and of course scaling up the systems.
% One crucial but often overlooked topic is data\dash{}where does it come from and
% what is its composition?
% In addition, we want foundation models to be robust to distribution shifts
% and secure against attackers.
% Finally, we wish to understand why foundation models work from both a mathematical perspective
% as well as an empirical perspective.

\input technology/modeling
\input technology/training
\input technology/adaptation
\input technology/evaluation
\input technology/systems
\input technology/data
\input technology/security
\input technology/robustness
\input technology/ai-safety
\input technology/theory
\input technology/interpretability
