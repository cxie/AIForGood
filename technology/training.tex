\newsection
\subsection{Training}
\label{sec:training}
\sectionauthors{Alex Tamkin}

Training objectives are mathematical functions describing how to transform a model architecture and large amount of broad data into a foundation model. For example, GPT-3 was trained with a language modeling objective, which rewards the model for predicting the next word correctly \citep{Shannon1948AMT}. We begin by laying out some goals of these training approaches, describe important design trade-offs in current approaches, and outline important goals for the path ahead.

\subsubsection{Goals of training objectives} 

Here we outline some key goals for training algorithms in light of the recent rapid progress in these methods and models.\footnote{We use ``training" instead of pretraining to emphasize the primacy of the foundation model itself, and because some methods for adapting foundation models to downstream tasks do not involve any later stage of training.}

\paragraph{Leveraging broad data.} 
The rise of \textit{self-supervised} learning algorithms has unlocked the power of internet-scale datasets which would be intractable to annotate by hand. This kind of broad data comes in many forms, including images, audio recordings, and video (\refsec{vision}); robotic and sensor data (\refsec{robotics}); and text, either in isolation or paired with other modalities like images (\refsec{language}). Because this data lacks external annotations, a major focus for researchers is designing bespoke self-supervised algorithms that leverage the unique structure within each kind of data to produce a training signal for a foundation model.

\paragraph{Domain completeness.} An important goal for foundation model training algorithms is to be \textit{domain complete}, in the sense that solving the training task requires capabilities that are broadly useful for downstream tasks in the domain (see \refsec{language}, \refsec{vision}, \refsec{robotics}). This property is crucial for the \textit{generality} of a foundation model. For example, language modeling may require models to acquire capabilities as wide-ranging as coreference, sentiment and translation as the model learns to predict the next word in a document. In contrast, a supervised learning task like sentiment classification may lead to a more narrow set of capabilities (see \refsec{language}). As important as this quality is, it is not obvious \textit{a priori} what tasks will result in a domain complete capabilities, or even how to evaluate the full breadth of a model's capabilities (see \refsec {evaluation} and \refsec{theory}). 

\paragraph{Scaling and compute efficiency.}
Procedures for training foundation models must reliably convert data, a model architecture, and compute into a broadly capable model. To maximize the capability of a foundation model, we can identify the bottlenecks to this process and propose new training algorithms which remove them. The rise of self-supervised algorithms has made model size and compute resources increasingly salient bottlenecks \citep{kaplan2020, henighan2020}, leading to a shift where models are evaluated not solely on their capabilities but rather on the amount and kind of compute needed to reach those capabilities (\refsec{evaluation}). The efficiency of training objectives can vary tremendously,\footnote{\eg 4x for ELECTRA \citep{Clark2020ELECTRAPT} vs BERT \citep{devlin2019bert}, 12x for contrastive vs generative approaches to CLIP training \citep{radford2021learning}} laying in sharp relief how important the design of a training approach is to the emergence of powerful capabilities given a fixed compute budget. Thus, a major goal for training researchers is to design training objectives with a richer training signal, resulting in models which learn faster and attain stronger capabilities.\footnote{Of course, a key goal for computer systems designers is to alleviate compute as a bottleneck for training (see \refsec{systems}) And the choice of a training method is ultimately also constrained by the availability of diverse, high-quality data (\refsec{data}), which continues to be a major challenge for many domains, including robotics (\refsec{robotics}) and low-resource languages (\refsec{language})} One force aiding this development is the surprising predictability of how capabilities scale with different kinds of architectures, data sizes, and compute \citep{Hestness2017DeepLS, kaplan2020},  a striking phenomenon which enables model developers to make choices based on clearer trends instead of more costly random searches.

\subsubsection{Design trade-offs in current SSL methods}
Current self-supervised learning (SSL) methods for training foundation models are diverse, but what unites them is that they produce prediction problems from unlabeled data without the need for human annotators. SSL objectives manufacture a rich training signal from this data through carefully-designed constraints, either on the data itself (\eg redacting or noising) or on the way the model is able to represent or process the data (\eg latent bottlenecks). At some level, these constraints ``bake in" the kinds of capabilities desired when adapting models to downstream tasks (\refsec{adaptation}).\footnote{For example, the causal language modeling objective used to train GPT-3 \citep{brown2020gpt3} enabled conditioning it via prefixes. And the color jitter augmentations used during contrastive learning \citep{chen2020simclr} encourage invariance to properties not thought to be useful for downstream tasks. Better understanding how the particular choice and structure of these constraints influences the capabilities acquired by the model is an important area for future work (\refsec{theory}).}

Here, we describe three important design choices that current models explore, along with their respective tradeoffs in terms of their resulting capabilities.

\paragraph{At what level of abstraction should we model?} A fundamental question is what the input representation of a foundation model should be. One option is to model the input at the level of raw bytes. However, this high dimensionality may cause the model to focus on predicting less semantic aspects of the  input,\footnote{\eg blades of grass, audio compression artifacts, or spellings of words} slowing the rate at which it acquires more generally-useful capabilities. These approaches also become intractable when training models like transformers \citep{vaswani2017attention} whose compute costs grow quadratically with the input size.\footnote{See \refsec{vision} and \refsec{modeling} for discussions of training costs for high-dimensional sequences, such as images and video} Another option is to use domain knowledge to reduce the input space of a model\dash{}such strategies include patch embeddings \citep{visual_transformer} as well as fixed or learned tokenization \citep{Schuster2012JapaneseAK, Sennrich2016NeuralMT, Kudo2018SentencePieceAS, Oord2017NeuralDR, ramesh2021zeroshot}. These methods may alleviate some challenges facing generative approaches, but have the trade-off that they may jettison possibly-useful information in the input.\footnote{For example, tokenizing text may make it harder to learn rhymes, puns, or other tasks that benefit from character-level information \citep{branwen2020gpt}} The choice of a continuous vs discrete input also has trade-offs for adaptation (\refsec{adaptation}); more work is needed to capture the benefits of both approaches.


\paragraph{Generative vs discriminative models}
Generative training approaches are conceptually elegant yet powerful\dash{}they train models to learn joint or conditional distributions over training inputs. Two major families of generative foundation models include autoregressive foundation models \citep{Oord2016WaveNetAG, Radford2018ImprovingLU, chen2020imagegpt, Yang2019XLNetGA, ramesh2021zeroshot}, which generate inputs piece by piece, and denoising foundation models \citep{devlin2019bert, raffel2019exploring}  which corrupt and then recover the inputs. The specific kind of generation performed in the training process determines what kind of interactivity is available during adaptation\footnote{For example, autoregressive models like GPT-3 enable prefix-based conditioning, while denoising models like T5 or BERT facilitate the use of bidirectional context to replace arbitrary-length spans or fix typos.} (see \refsec{interaction} and \refsec{adaptation}), and future models may enable an even richer set  of interactions.\footnote{Other kinds of generative approaches less studied in a foundation modeling context include diffusion and score-based models \citep{SohlDickstein2015DeepUL, Song2019GenerativeMB, Ho2020DenoisingDP}, VAEs \citep{Kingma2014AutoEncodingVB}, flow models \citep{dinh2015nice, Kingma2018GlowGF}, and GANs \citep{goodfellow2014gan}\dash{}it remains to be seen whether these or other future approaches can also enable learning as diverse variety of capabilities as autoregressive or denoising approaches.}

While generative training approaches have their benefits, several discriminative approaches have also recently gained traction. These methods do not enable generation-based interaction, yet they may enable more efficient learning for classification- or regression-based tasks in high-dimensional continuous settings like images, audio, and video. Most of these methods output vectors for (parts of) inputs, which are trained to be similar for different ``views'' of an input \citep{Wu2018UnsupervisedFL, Oord2018RepresentationLW, chen2020simclr, He2020MomentumCF, Grill2020BootstrapYO, Caron2021EmergingPI, convirt, radford2021learning} or used to predict whether parts of inputs are real or fake \citep{Clark2020ELECTRAPT, Iida2021TABBIEPR}. Better understanding the trade-offs between generative and discriminative training, as well as capturing the best of both approaches, remain interesting avenues for future study.

\paragraph{Capturing multimodal relationships.} Another increasingly important research area is capturing the relationships between multiple kinds of data. What this means may differ based on the context and the goals of a modeler. For example, CLIP \citep{radford2021learning} and ViLBERT \citep{Lu2019ViLBERTPT} are both multimodal vision-language, but differ in the precise \textit{way} they are multimodal.\footnote{See \refsec{vision} and \refsec{language} for more discussion of multimodality in vision and language specifically} The former encodes images and text separately into vectors, enabling users who have examples from a single modality to retrieve, score, or classify examples from the other modality. The second processes images and text jointly at an early stage of the model, enabling downstream applications like visual question answering where reasoning over pairs of related images and text (\eg images and questions about them) are provided. Multimodal foundation models remain a nascent research area; much is still unexplored about the different ways a model can be multimodal as well as better understanding the capabilities these additional modalities bring.

\subsubsection{Paths forward}
We close with some important goals for the future of foundation model training.

\paragraph{Out-of-the-box SSL} Right now, SSL objectives are highly domain-specific: different methods currently prevail in natural language processing, computer vision, and speech processing. This has two major disadvantages: First, these different techniques make it challenging to grasp the common threads and scientific principles underlying \textit{why} each of these methods work. Second, this domain-specificity requires developing new foundation model training methods from scratch for each new field, including medical, scientific, and new multimodal settings. A more general objective for efficiently training foundation models on any kind of data would represent a significant milestone for the foundation model training community \citep{Tamkin2021DABS}.

\paragraph{Obtaining a rich training signal} It is clear that not all training objectives are made equal\dash{}some are radically more efficient than others, translating into far more capable foundation models for a given compute budget. Are there training methods orders of magnitude more efficient than those currently known? If so, how can we find them? These investigations will be shaped by many forces, including what future software and hardware advances (\refsec{systems}) make possible. We also need not view data (\refsec{data}) and training algorithms as independent factors: not only does the quality and availability of the data influence the training signal,\footnote{Including any undesirable or biased capabilities (\refsec{fairness})} but the training algorithm itself could adaptively seek out or construct richer training examples as the model improves to accelerate learning \citep{Tamkin2021ViewmakerNL}.

\paragraph{Goal-directed training of foundation models.} Adaptation methods such as prompting (\refsec{adaptation}) draw on emergent properties that result almost as an afterthought of training. Can we train foundation models where the ability to understand and reliably carry out goals in a complex world is part of the model's training objective? A focus on developing general capabilities distinguishes this direction from the goal of adapting an existing foundation model to a specific task via reinforcement learning (\eg \citet{Stiennon2020LearningTS}). Instead, one might imagine more sophisticated versions of current methods which acquire a diverse range of real-world capabilities from raw online \citep{Klyubin2005EmpowermentAU, singh2005intrinsically, Salge2013EmpowermentA, Mohamed2015VariationalIM, Florensa2017StochasticNN, Pathak2017CuriosityDrivenEB, Haber2018LearningTP} or offline  \citep{Precup2000EligibilityTF, lange2012batch, Ajay2021OPALOP, Yang2021RepresentationMO, Schwarzer2021PretrainingRF} interactions, without the need for human annotations or task construction. Such methods might use techniques quite similar to existing SSL algorithms: \eg training sequence models in goal-directed contexts where they can be directly \textit{asked} to carry out certain tasks via conditioning (\eg UDRL \citep{Schmidhuber2019ReinforcementLU, Srivastava2019TrainingAU} or Decision Transformer \citep{Chen2021DecisionTR}; also see \refsec{robotics}). The complex behaviors that have already emerged in simple interactive environments  \citep{Baker2020EmergentTU} suggest multitask, multiagent, and multimodal goal-directed training of foundation models as an interesting avenue for future study.