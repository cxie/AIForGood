\newsection
\subsection{AI safety and alignment}
\label{sec:ai-safety}
\sectionauthors{Alex Tamkin, Geoff Keeling, Jack Ryan, Sydney von Arx}

The field of Artificial Intelligence (AI) Safety concerns itself with potential accidents, hazards, and risks of advanced AI models, especially larger-scale risks to communities or societies. Current foundation models may be far from posing such risks; however, the breadth of their capabilities and potential applications is striking, and a clear shift from previous ML paradigms. 
While AI safety has historically occupied a more marginal position within AI research, the current transition towards foundation models and their corresponding generality offers an opportunity for AI safety researchers to revisit the core questions of the field in a new light and reassess their immediate or near-future relevance.\footnote{See \citet{amodei2016concrete} and \citet{hendryckssafety2021} for broader perspectives on open problems in AI Safety.}

\subsubsection{Traditional problems in AI safety}

A major branch of AI safety research concerns the implications of advanced AI systems, including those that might match or exceed human performance across a broad class of cognitive tasks \citep{everitt2018agi}.\footnote{This is referred to by some as AGI or artificial general intelligence, although terminology use varies \citep[\eg see][]{karnofsky2016potential}.} A central goal of safety research in this context is to mitigate large-scale risks posed by the development of advanced AI.\footnote{Note that this does not require a belief that building certain kinds of advanced AI is a desirable goal, nor even certainty that it is an achievable one.} These risks may be significantly more speculative than those considered in \refsec{misuse}, \refsec{robustness}, and \refsec{security}; however, they are of far greater magnitude, and could at least in principle result from future, highly-capable systems. Of particular concern are global catastrophic risks: roughly, risks that are global or trans-generational in scope—causing death or otherwise significantly reducing the welfare of those affected (\eg a nuclear war or rapid ecological collapse) \citep{bostrom2011global}. What AI safety research amounts to, then, is a family of projects which aim to characterize what (if any) catastrophic risks are posed by the development of advanced AI, and develop plausible technical solutions for mitigating the probability or the severity of these risks. The best-case scenario from the point of view of AI safety is a solution to the control problem: how to develop an advanced AI system that enables us to reap the computational benefits of that system while at the same time leaving us with sufficient control such that the deployment of the system does not result in a global catastrophe \citep{bostrom2011global}. However technical solutions are not sufficient to ensure safety: ensuring that safe algorithms are actually those implemented into real-world systems and that unsafe systems are not deployed may require additional sociotechnical measures and institutions. 

Reinforcement Learning (RL), which studies decision-making agents optimized towards rewards, has been a dominant focus in AI safety for the past decade. What is at issue here is the difficulty of specifying and instantiating a reward function for the AI that aligns with human values, in the minimal sense of not posing a global catastrophic threat.\footnote{See \citet{Hubinger2019RisksFL} for a discussion of some challenges that arise at the threshold between reward specification and reward instantiation.} While this problem, known as value alignment \citep{gabriel2020artificial, yudkowsky2016ai}, may seem trivial at first glance, human values are diverse,\footnote{See \citet{gabriel2020artificial} for an extended discussion of human diversity, ethics, and the value alignment problem} amorphous, and challenging to capture quantitatively. Due to this, a salient concern is reward hacking, where the AI finds an unforeseen policy that maximizes a proxy reward for human wellbeing, but whose misspecification results in a significant harm.\footnote{See \href{https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml}{this spreadsheet} for a list of real-world examples of reward hacking, including an aircraft landing algorithm which achieved a perfect score by outputting large forces that exploited a flaw in the simulator.} Many efforts to combat the value alignment problem have focused on maximizing corrigibility, which is when errors in the design of a system can be corrected once the system is running \citep{soares2015corrigibility}. This can be far from straightforward—in the RL context, an agent with a specified goal would be incentivized to prohibit attempts to alter that goal, as any attempt to alter that goal would likely be suboptimal for the goal’s realization \citep{Omohundro2008TheBA}.


However, pure RL is not the only theorized route to advanced AI. Foundation models can also be trained with simple (self-)supervised objectives like next-token prediction, yet can still be used in interactive and goal-directed ways, with or without additional RL training. Moreover, it appears that many of these methods may result in increased capabilities through straightforward scaling of compute, number of parameters, and dataset size \citep{Hestness2017DeepLS, kaplan2020}. What concepts like value alignment and corrigibility amount to in the broader context of foundation models differ in several respects to the pure RL case, and must accordingly be carefully theorized. 


\subsubsection{Current foundation models and AI safety}

Many of these risks in the RL setting result from models optimized to carry out goals. However, a key challenge for AI safety research on recent foundation models is that goal-directed behavior may emerge despite not being explicitly optimized for (see also \refsec{training}). As an example, large language models may be trained on corpora where agents use language in goal-directed ways, such as in persuasive text. To predict the next token well, a model may acquire a general capability to reason and produce arguments, which could emerge with suitable contexts. Foundation models trained on other kinds of human data may capture other kinds of goal-directed behavior present in the data; \eg robotic agents trained to mimic humans in videos may attempt to punch or knock-out their human operators if their training data includes videos of boxing matches. Recent work has also attempted to directly train agents to produce goal-directed behavior; for example, the Decision Transformer trains a sequence model on trajectories prepended with their returns \citep{Srivastava2019TrainingAU, Schmidhuber2019ReinforcementLU, Chen2021DecisionTR}. One can then generate high-return trajectories by ``prompting'' this model with a high return, which raises similar questions of reward hacking from the RL context. 

However, a major aim of safety research on goal-directed models is to gain more principled control and explainability over the actions being pursued by the agent, as opposed to relying on inscrutable decisions from a blackbox neural network.\footnote{For more on the relationship between understanding and semantics see \refsec{philosophy}} This makes current foundation models an exciting avenue of study for AI safety research, as aligning them may be a useful precursor for aligning more advanced models \citep{christiano2016prosaic, cotra2021narrow, Kenton2021AlignmentOL}. One challenge is the misalignment between the foundation model's training objective and the desired behavior; for example, a language model may be trained to predict the next word of all documents in the training corpus regardless of veracity, but users may want the model to only output true or helpful text. One potential way to steer goal-directed agents towards desired behavior may be to train them with natural language descriptions of actions\dash{}this may enable steering them with language as well as enabling them to output interpretable language describing the task they "believe" they are performing, similar to methods for controllable generation and source attribution \citep[\eg][see also \refsec{robotics}, \refsec{interaction}, and \refsec{interpretability}]{keskar2019ctrl}. However, further advances would be necessary to ensure the reliability and self-consistency of such models in the wild (\refsec{robustness}), as well as gaining a more mechanistic understanding of how these models operate \citep[also see \refsec{interpretability}]{cammarata2020thread}. And even if natural language-based control of future foundation models enables better task specification and monitoring, models may acquire deceptive or otherwise undesirable behavior from human data\dash{}identifying and neutralizing this behavior is another important direction for future study.

While the self-supervised objectives described in the previous paragraph train models to capture human behavior in the data, new training paradigms may produce goal-directed foundation models capable of carrying out a wide range of tasks in complex environments, and which exhibit capabilities superior to humans in different domains (see \refsec{training}). For example, goal-directed foundation models may be trained in an open-ended self-play setting, similar to AlphaGo, or in vast multitask single-agent RL setups. This might lead to emergent capabilities that complicate efforts to get agents to carry out goals, especially if many agents are trained together in a rich world-simulator that encourages the development of skills like deception, misdirection, dissimulation, persuasion, and strategic planning. Aside from countering deceptive behavior, it also remains unclear how to effectively evaluate and control the behavior of very capable models, known as scalable oversight or alignment \citep{amodei2016concrete, Leike2018ScalableAA}; \eg scoring novel reactions proposed by a chemical foundation model (see \refsec{evaluation}). New human-in-the-loop approaches for training, steering, monitoring, and understanding these models are thus exciting future directions.

Finally, even before any of these more advanced capabilities emerge, an important research area for AI safety in the near term is characterizing and forecasting the capabilities of current self-supervised foundation models. There are three aspects which make this challenging. First, the generality of foundation models means that they can be applied to countless different kinds of applications in unexpected ways. Enumerating current and planned applications of foundation models is not sufficient to capture the full range of ways they could be used. Second, even within a particular application, model capabilities are emergent: they grow and change in unexpected ways as models scale. For example, the ability to control GPT-3 via ``prompting" was an emergent phenomenon of which only the barest glimpses were evident in the smaller GPT-2 model \citep{radford2019language,brown2020gpt3}. What the emergent properties of future foundation models will look like is unknown. Third, even within a particular application and scale, a model's capabilities are not easy to characterize. For example, the ability of GPT-3 to perform addition improves dramatically once commas are added to the inputs \citep{branwen2020gpt, brockman2020math}. Similarly, small rewordings of prompts can have large impacts on task performance. Since the space of prompts is intractable to enumerate, it is challenging to definitely assert that any task is outside the reach of current prompt-based foundation models\dash{}this is a major challenge for reasoning about possible catastrophic risks from foundation models.


\subsubsection{Potential catastrophic risks from future foundation models}
The broad and quickly-growing capabilities of current models suggest the benefit of attempting to characterize possible catastrophic risks from more advanced systems. We see at least two ways in which advanced foundation models might contribute to such outcomes. 

\paragraph{Catastrophic robustness failures.} \refsec{robustness} discusses how models may behave in unexpected or harmful ways when confronted with new kinds of data \citep{amodei2016concrete, yudkowsky2008artificial}. These failures may be especially consequential if foundation models are integrated into important systems that leverage foundation models' ability to quickly adapt to many different tasks and situations. Failures could be catastrophic if they occur in warfare systems (resulting in unwanted discharge of weapons, possibly igniting a conflict), critical infrastructure (accidental destruction of critical energy or agricultural capabilities), or if they become essential to a large fraction of economic activity (whose unexpected failure could result in a sudden collapse in living standards and political instability; see also \refsec{economics}). Indeed, the threat of catastrophic robustness failures is particularly pertinent for foundation models in contrast to other kinds of AI. This is because a foundation model consists of a single model that may be adapted for many different use cases, such that robustness failures derived from the statistical associations learned by the model could in principle manifest in a correlated way across several different domains. If the same foundation model is integrated into multiple critical functions, then lack of robustness in the model could lead to correlated failures that span multiple critical functions or failsafes.


\paragraph{Misspecified goals.} The use of foundation models might increase the risks of optimizing misaligned yet easy-to-specify goals, often referred to as Goodhart’s Law \citep{Kenton2021AlignmentOL, goodhart1984}. A current-day example of these risks is the negative effects of some recommender systems (\eg polarization, media addiction) which may optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being \citep{burr2018analysis, milano2020recommender}. Future institutions may leverage uninterpretable foundation models to maximize simple measures such as profit or GDP, due to these models' ability to adapt to the many different subproblems each of these metrics is dependent on. However, at larger scales optimizing for these proxy metrics instead of a more holistic goal designed for human welfare could inadvertently lead to environmental or geopolitical harms \citep{ gabriel2020artificial, creel2021}.

\subsubsection{Conclusion}
In sum, we argue that current and potential future emergent properties of foundation models make them ripe objects of study for the field of AI safety. We encourage future work on characterizing and forecasting the exact capabilities and risks of foundation models; developing new methods to align foundation models to human values and desired goals; and for states, research labs, and businesses to coordinate on proactive measures to mitigate salient risks.
