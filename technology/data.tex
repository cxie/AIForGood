\newsection
\subsection{Data}
\label{sec:data}
\sectionauthors{Laurel Orr, Simran Arora, Karan Goel, Avanika Narayan, Michael Zhang, Christopher Ré}

Foundation models signal a paradigm shift where increasingly massive quantities of data are being ``fed'' to these models for improved adaptation performance~\citep{devlin2019bert,radford2021learning,tolstikhin2021mlpmixer} with the overarching rule-of-thumb being "the more data the better"~\citep{kaplan2020}. As previous sections have mentioned, this focus on data curation has raised concerns around the foundation model data lifecycle including (1) managing the data at such a large scale (\refsec{introduction}), (2) integrating data across new modalities (\refsec{robotics}, \refsec{healthcare}), (3) reasoning over licensing and governance regulations\dash{}especially when considering the massing web-crawls used in foundation models training\dash{}(\refsec{healthcare}, \refsec{legality}), and (4) understanding the data quality (\refsec{evaluation}).

While foundation models add new and difficult facets to these challenges, we see parallels between these issues and core challenges in communities such as data management and data analytics as well as industrial ML pipelines. For example, data management has long studied scalable declarative systems for data analysis, versioning, provenance, and integration\textemdash addressing challenges (1) and (2)~\citep{zaharia2012resilient, cudre2009demonstration, stonebraker2013voltdb, Stonebraker2018dataintegration, hellerstein2005readings}. Industry has pipelines dealing with challenge (3) to manage diverse data licenses and help mitigate data violations. There is an entire ecosystem of research and systems tackling challenge (4) to support interactive data analytics and visualization~\citep{hohman2020understanding}.\footnote{VIS, CHI, SIGGRAPH are a few communities that research interactive data analytics methods and systems. Software systems and libraries such as Pandas, Matplotlib, and Seaborn also aid users in interactive exploration.} While these solutions are not necessarily "foundation model-ready", we believe a path to better management of the foundation model data lifecycle should take inspiration from these existing systems.

In this section, we address managing the foundation model data lifecycle. We first outline four desiderata including data management at scale, support for heterogenous data sources, data governance, and data quality monitoring. We then envision how all of these requirements can be integrated into a holistic data management solution called a {\em data hub}. The data hub is simply a data management toolkit that can be used by the private or public sectors to better support the interactive management of the foundation model data lifecycle.

\subsubsection{Data Management Desiderata}
\label{sec:data-desiderata}

Current practices in foundation model development are generally ad-hoc across the entire lifecycle from data curation and data documentation to model monitoring and patching~\citep{gebru2018datasheets, bandy2021addressing, bender2018data}. Research in the data management community has shown that well-defined data management platforms facilitate ML model development at scale through data ingestion, data versioning, data provenance, efficient analysis, and model monitoring~\citep{hellerstein2005readings, agrawal2019cloudy, vartak2016modeldb, ikeda2010panda}.\footnote{Feature stores like Michelangelo also support end-to-end ML model building \url{https://eng.uber.com/michelangelo-machine-learning-platform/}.} Taking inspiration from the data management community, we consider core desiderata when building a holistic data management platform for foundation models. 
\begin{enumerate}
    \item \textbf{Scalability.} Foundation models are being trained on increasingly massive quantities of data~\citep{kaplan2020} with the WuDao 2.0 model being trained on 4.9 TB of multi-modal data.\footnote{\url{https://www.scmp.com/tech/tech-war/article/3135764/us-china-tech-war-beijing-funded-ai-researchers-surpass-google-and}} This scale is expected to increase as most recent models are trained largely on public facing datasets. Public data represents an extremely small fraction of data compared to the petabytes of business and personal data collected every day and used in industrial foundation model pipelines~\citep{marr2017}. There is therefore a growing need for highly scalable techniques that can handle multi-modal foundation model datasets.

    \item \textbf{Data integration.} 
    Recent work using foundation models demonstrates that leveraging integrated structured and unstructured data can help models better generalize to rare concepts~\citep{orr2020bootleg} and improve factual knowledge recall~\citep{orr2020bootleg, logeswaran2019entdesc, Zhang2019ERNIEEL, Peters2019KnowledgeEC,perner2020ebert}. Despite these recent successes, integrating datasets for foundation models remains a challenge. Many works use unstructured text data with structured entity knowledge or image data~\citep{antol2015vqa}. There is a growing need to integrate datasets across diverse modalities such as text, video, eye-tracking~\citep{hollenstein-etal-2020-zuco}, and robotic simulations~\citep{lynchlanguage} (see \refsec{robotics}). We need data-integration solutions that can be applied at an industrial scale to multiple modalities and to multiple domains, such as government, business, and science.
    
    \item \textbf{Privacy and governance controls.} The training data used for foundation models may risk the violation of the privacy of data subjects; their data may be disclosed, collected, or used without their consent~\citep{jo2020lessons} or outside the context for which consent was originally given. 
    The issue of consent and use is especially relevant for foundation models where downstream applications cannot always be anticipated.
    As explained in \refsec{legality}, these issues are compounded with the prevalence of web scraped datasets for foundation model training. As there are still open legal questions about how web-crawled data will be governed and copyrighted,\footnote{These issues have recently come to bear by the debate surrounding the use of GitHub data in Copilot's Codex tool to help developers code \url{https://www.pwvconsultants.com/blog/questions-around-bias-legalities-in-githubs-copilot/}} the consequences of using web data remain unclear to foundation model providers in the public and private sector. We need tooling to help foundation model providers adapt to emerging regulations and guidelines to ensure safe and responsible data management.
    
    \item \textbf{Understanding data quality.} Data quality impacts model performance~\citep{lee2021deduplicating}; however, toolkits or methods to systematically and scalably understand the training data and relevant data subsets are still in their infancy. The data creation process can be messy, and the data can contain different types of biases~\citep{blodgett_language_2020, bender2021} (see \refsec{fairness}) and consist of poisoned, false, or duplicated information~\citep{chang2020adversarial, carlini2021poisoning, BuchananCSET2021, lee2021deduplicating}. Data is also continuously updated and refined~\citep{kiela2021dynabench} and may have emergent entities~\citep{fetahu2015much}, distribution shift~\citep{chen2021mandoline}, and concept meaning shift~\citep{kenter2015ad, lazaridou2021pitfalls}. Further, once deployed, foundation models may present undesirable behavior on critical, fine-grained sub-populations of data that foundation model providers need to detect and mitigate~\citep{goel2021robustnessgym, hohman2018visual, re2019overton, oakden2019hidden}. We need toolkits that can detect and potentially mitigate different types of undesirable data to improve model performance in an interactive and iterative fashion. Such toolkits also need to adapt to the dynamical nature of training data. 
\end{enumerate}

\subsubsection{Data Hub Solution}
\label{sec:data-solutions}
Pulling on years of work from data management, data science, and data analytics, we envision a foundation model lifecycle data management solution, which we term a \textit{data hub}. While examples of ML-focused data hubs\footnote{Some public data hubs include: \url{https://data.world/}, \url{https://dataverse.harvard.edu/dataverse/harvard}, \url{https://datacommons.org/}, \url{https://www.data.gov/}, \url{https://www.kaggle.com/}, \url{https://huggingface.co/datasets}, \url{https://www.ldc.upenn.edu/}} as well as more traditional data management systems exist,\footnote{Some traditional data management systems for foundation models include: \url{https://aws.amazon.com/big-data/datalakes-and-analytics/}, \url{https://eng.uber.com/michelangelo-machine-learning-platform/}, \url{https://kafka.apache.org/}} they either (1) do not treat data integration as a first class primitive, (2) do not natively support the end-to-end lifecycle with model predictions, or (3) do not allow for interaction-driven data curation and refinement, where foundation model providers can dynamically explore and update possible datasets subject to access control guidelines. We now discuss how the data hub addresses the four desiderata.

\paragraph{Data scale.} To address the management at scale challenge, the data hub will need standard data management solutions~\citep{armbrust2009above} such as infrastructure to store and maintain large-scale datasets as they change over time and scalable interfaces to query, select, and filter datasets. The hub should support heterogenous compute as well as cloud infrastructure to support scalable solutions in different environments. 

\paragraph{Data integration.}
The hub should incorporate data integration as a first class citizen. It will need advanced data integration solutions~\citep{Stonebraker2018dataintegration, abiteboul1997semistructureddata, dong2020multi, rekatsinas2017holoclean}\footnote{\url{https://www.tamr.com/}} to allow for the merging of structured and unstructured knowledge across modalities and domains. Further, this implies the hub will need to support storing and querying over heterogeneous datasets and sources.

\paragraph{Access control.} Considering the access controls of the hub, the hub will need to support diverse documentation, \eg dataset sheets~\citep{gebru2018datasheets} or data statements~\citep{bender2018data}, to allow data curators to reflect on their processes and be transparent about the intended use cases, potential biases, and limitations of their dataset. The data hub will need to decide which documentation is required for data to be uploaded (\eg the data source and data description) and which information is recommended (\eg what tasks the data could be used for). Furthermore, documentation may need to be updated as datasets evolve~\citep{goel2021robustnessgym}.

Data sources are often associated with licenses, and the hub will need to integrate different sources with different legal concerns and conditions~\citep{masur2018datalicensing}.\footnote{\url{https://content.next.westlaw.com/4-532-4243}} Further, certain datasets have legal guidelines to protect the privacy of the data subjects. The hub will need methods to ensure a dataset does not release personally identifiable information (PII),\footnote{\url{https://www.justice.gov/opcl/privacy-act-1974}} that the aggregation of anonymized or de-identified data does not release PII,\footnote{\url{http://www2.ed.gov/policy/gen/guid/fpco/ferpa/library/georgialtr.html}} and that the data subjects have given informed consent for their data to be disseminated.\footnote{\url{https://www.dhs.gov/sites/default/files/publications/privacy-policy-guidance-memorandum-2008-01.pdf}}

Pulling on ideas from data integration~\citep{rekatsinas2017slimfast}, the hub should support mechanisms to enable efficient and safe maintenance and sharing of data resources. Especially as the legality of certain public datasets (\eg web dumps) are still being decided (\refsec{legality}), the hub critically needs tooling to help identify licensing violations and mitigate the impact of any governance violation. As certain violations will likely relate to model behavior, we need systems to support better understanding of model behavior, as we describe next.

\paragraph{Data quality tooling.} Drawing on the field of data analysis and exploration, as users interactively select, filter, and refine the data to use for training or adaptation, the hub will need tools to quickly understand a user's current dataset and its impact on model behavior~\citep{hohman2020understanding}.\footnote{Examples of data-focused interactive toolkits include \url{https://www.tableau.com/} and \url{https://www.paxata.com/}.} Furthermore, these systems can allow end-to-end foundation model monitoring by incorporating model performance through recent work on slice (sub-population) finding~\citep{chung2019slice}, model validation on relevant subsets~\citep{goel2021robustnessgym, ribeiro2020beyond}, and data valuation~\citep{ghorbani2019data}. Recent works also present methods that use the model to detect which subpopulations of data contribute the most to a given output to further aid model debugging~\citep{keskar2019ctrl}.

Once users can monitor model behavior\textemdash especially on rare, yet critical sub-populations\textemdash, the hub should provide methods and guidance for users to maintain models by correcting model errors. Although ``model patching''~\citep{goel2020model} is still an open problem, the work of \citep{orr2020bootleg} provides a first description of using data engineering to maintain a production self-supervised system that corrected for undesirable behavior through changes to the data, not model. We believe the data hub will need to support interfaces for users to inject targeted data modifications for model maintenance.

We also acknowledge that data curation and exploration are not performed in isolation, and believe the data hub should support a community around sharing useful metrics and analysis pipelines. Inspired by similar community sharing platforms like Hugging Face’s ModelHub\footnote{\url{https://huggingface.co/models}} or Tableau Public’s visualization sharing platform,\footnote{\url{https://public.tableau.com/en-us/s/about}} we want users to share insights about foundation model training data.

\paragraph{Open questions.} Although our described data hub is inspired by existing toolkits and solutions, we do not believe they are all ready for the challenges of foundation models. In particular, some open questions revolving around designing a data hub are:
\begin{itemize}
    \item How should we support data versioning so datasets can be updated while maintaining old versions for reproducibility~\citep{agrawal2019cloudy}? Once models are deployed and error buckets are identified, datasets may need to be updated to include more examples from these error buckets. How should these new, targeted examples be collected?
    \item As described in \refsec{training}, we imagine fewer models will be trained from scratch and more will be fine-tuned. How do we support provenance or lineage information to understand where the original data came from, while maintaining subject privacy~\citep{chen2015access}?
    \item In the public sector, a data hub may be organized and run by an open-source community of individuals consisting of data curators and foundation model providers. In this setting, answers to questions such as who stores the data? who pays for any compute? who is liable if licensing is violated? are particularly murky. How can the data hub provide that right tooling so that once answers to such questions are resolved, they can be operationalized with ease?
    \item What is the right set of statistics over the data to provide adequate documentation, without being too costly or difficult to obtain?
    \item How can a data hub support targeted data modifications such as augmentation~\citep{ma2019nlpaug, shorten2019survey} or data programming~\citep{ratner2017snorkel}?
    \item How can monitoring toolkits better detect when a foundation model needs to be updated due to poor performance on dynamically changing evaluation data?
\end{itemize}

Our vision for a data hub is not complete or fully detailed. However, we present initial thoughts on data challenges, and one solution to prompt thinking for how to improve data management for the foundation model lifecycle.