% For references that aren't in refdb/all.bib (see http://github.com/percyliang/refdb).



@article{linzen2021syntactic,
  title={Syntactic structure from deep learning},
  author={Linzen, Tal and Baroni, Marco},
  journal={Annual Review of Linguistics},
  volume={7},
  pages={195--212},
  year={2021},
  publisher={Annual Reviews}
}

@inproceedings{wang2020extending,
  title={Extending Multilingual BERT to Low-Resource Languages},
  author={Wang, Zihan and Karthikeyan, K and Mayhew, Stephen and Roth, Dan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={2649--2656},
  year={2020}
}

@inproceedings{bahdanau2018systematic,
  title={Systematic Generalization: What Is Required and Can It Be Learned?},
  author={Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{kim2020cogs,
  title={COGS: A Compositional Generalization Challenge Based on Semantic Interpretation},
  author={Kim, Najoung and Linzen, Tal},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9087--9105},
  year={2020}
}

@inproceedings{cao2019multilingual,
  title={Multilingual Alignment of Contextual Word Representations},
  author={Cao, Steven and Kitaev, Nikita and Klein, Dan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{chi2020finding,
  title={Finding Universal Grammatical Relations in Multilingual BERT},
  author={Chi, Ethan A and Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5564--5577},
  year={2020}
}

@inproceedings{papadimitriou2021deep,
  title={Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT},
  author={Papadimitriou, Isabel and Chi, Ethan A and Futrell, Richard and Mahowald, Kyle},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={2522--2532},
  year={2021}
}

@article{saffran1996statistical,
  title={Statistical learning by 8-month-old infants},
  author={Saffran, Jenny R and Aslin, Richard N and Newport, Elissa L},
  journal={Science},
  volume={274},
  number={5294},
  pages={1926--1928},
  year={1996},
  publisher={American Association for the Advancement of Science}
}

@article{colonnesi2010relation,
  title={The relation between pointing and language development: A meta-analysis},
  author={Colonnesi, Cristina and Stams, Geert Jan JM and Koster, Irene and Noom, Marc J},
  journal={Developmental Review},
  volume={30},
  number={4},
  pages={352--366},
  year={2010},
  publisher={Elsevier}
}

@book{paris2013natural,
  title={Natural Language Generation in Artificial Intelligence and Computational Linguistics},
  author={Paris, C.L. and Swartout, W.R. and Mann, W.C.},
  isbn={9781475759457},
  series={The Springer International Series in Engineering and Computer Science},
  url={https://books.google.gr/books?id=4vbiBwAAQBAJ},
  year={2013},
  publisher={Springer US}
}

@article{libovicky2019language,
  title={How language-neutral is multilingual BERT?},
  author={Libovick{\`y}, Jind{\v{r}}ich and Rosa, Rudolf and Fraser, Alexander},
  journal={arXiv preprint arXiv:1911.03310},
  year={2019}
}

@article{virtanen2019multilingual,
  title={Multilingual is not enough: BERT for Finnish},
  author={Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
  journal={arXiv preprint arXiv:1912.07076},
  year={2019}
}

@inproceedings{pires2019multilingual,
  title={How Multilingual is Multilingual BERT?},
  author={Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4996--5001},
  year={2019}
}

@article{choenni2020cross,
  title={Cross-neutralising: Probing for joint encoding of linguistic information in multilingual models},
  author={Choenni, Rochelle and Shutova, Ekaterina},
  journal={arXiv preprint arXiv:2010.12825},
  year={2020}
}

@book{jurafsky2009speech,
  title={Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author={Jurafsky, D. and Martin, J.H.},
  isbn={9780131873216},
  lccn={2008010335},
  series={Prentice Hall series in artificial intelligence},
  url={https://books.google.gr/books?id=fZmj5UNK8AQC},
  year={2009},
  publisher={Pearson Prentice Hall}
}

@article{acemoglu2019automation,
  title={Automation and new tasks: How technology displaces and reinstates labor},
  author={Acemoglu, Daron and Restrepo, Pascual},
  journal={Journal of Economic Perspectives},
  volume={33},
  number={2},
  pages={3--30},
  year={2019}
}

@book{acemoglu2021redesigning,
  title={Redesigning AI},
  author={Acemoglu, Daron},
  isbn={9781946511638},
  series={Boston Review / Forum},
  url={https://books.google.com/books?id=HBb6DwAAQBAJ},
  year={2021},
  publisher={MIT Press}
}

@book{reich2021system,
  title={System Error: Where Big Tech Went Wrong and How We Can Reboot},
  author={Reich, Rob and Sahami, Mehran and Weinstein, Jeremy M.},
  isbn={9780063066205},
  url={https://books.google.com/books?id=mU0QEAAAQBAJ},
  year={2021},
  publisher={Harper}
}

@article{david2015there,
  title={Why Are There Still So Many Jobs? The History and Future of Workplace Automation},
  author={Autor, David H.},
  journal={Journal of Economic Perspectives},
  volume={29},
  number={3},
  pages={3--30},
  year={2015}
}



@inproceedings{tan2020vokenization,
  title={Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2066--2080},
  year={2020}
}

@inproceedings{nekoto2020participatory,
  title={Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages},
  author={Nekoto, Wilhelmina and Marivate, Vukosi and Matsila, Tshinondiwa and Fasubaa, Timi E and Fagbohungbe, Taiwo and Akinola, Solomon Oluwole and Muhammad, Shamsuddeen Hassan and Kabenamualu, Salomon Kabongo and Osei, Salomey and Sackey, Freshia and others},
  booktitle={EMNLP (Findings)},
  year={2020}
}

@inproceedings{zhang2021billions,
  title={When Do You Need Billions of Words of Pretraining Data?},
  author={Zhang, Yian and Warstadt, Alex and Li, Haau-Sing and Bowman, Samuel R},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{romer1990endogenous,
  title={Endogenous technological change},
  author={Romer, Paul M},
  journal={Journal of political Economy},
  volume={98},
  number={5, Part 2},
  pages={S71--S102},
  year={1990},
  publisher={The University of Chicago Press}
}

@article{jacovi2020towards,
  title={Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?},
  author={Jacovi, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.03685},
  year={2020}
}

@inproceedings{elton2020self,
  title={Self-explaining AI as an alternative to interpretable AI},
  author={Elton, Daniel C},
  booktitle={International Conference on Artificial General Intelligence},
  pages={95--106},
  year={2020},
  organization={Springer}
}

@article{chen2018looks,
  title={This looks like that: deep learning for interpretable image recognition},
  author={Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
  journal={arXiv preprint arXiv:1806.10574},
  year={2018}
}


@inproceedings{jacovi2021formalizing,
  title={Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai},
  author={Jacovi, Alon and Marasovi{\'c}, Ana and Miller, Tim and Goldberg, Yoav},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={624--635},
  year={2021}
}

@inproceedings{mostafa_parameter_2019,
	title = {Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
	url = {http://proceedings.mlr.press/v97/mostafa19a.html},
	language = {en},
	urldate = {2021-08-11},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mostafa, Hesham and Wang, Xin},
	month = may,
	year = {2019},
	note = {ZSCC: 0000081 
ISSN: 2640-3498},
	pages = {4646--4655},
}

@article{schuur_climate_2015,
	title = {Climate change and the permafrost carbon feedback},
	volume = {520},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/nature14338},
	doi = {10.1038/nature14338},
	abstract = {A large amount of organic carbon stored in frozen Arctic soils (permafrost) could be released as carbon dioxide and methane in a warming climate, which would accelerate the pace of climate change; this review suggests that release of greenhouse gas emissions will be gradual but prolonged.},
	language = {en},
	number = {7546},
	urldate = {2021-08-11},
	journal = {Nature},
	author = {Schuur, E. a. G. and McGuire, A. D. and Schädel, C. and Grosse, G. and Harden, J. W. and Hayes, D. J. and Hugelius, G. and Koven, C. D. and Kuhry, P. and Lawrence, D. M. and Natali, S. M. and Olefeldt, D. and Romanovsky, V. E. and Schaefer, K. and Turetsky, M. R. and Treat, C. C. and Vonk, J. E.},
	month = apr,
	year = {2015},
	note = {ZSCC: NoCitationData[s0] 
Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7546
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Biogeochemistry;Climate sciences;Earth and environmental sciences
Subject\_term\_id: biogeochemistry;climate-sciences;earth-and-environmental-sciences},
	pages = {171--179}
	}

@article{urban_accelerating_2015,
	title = {Accelerating extinction risk from climate change},
	volume = {348},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/348/6234/571},
	doi = {10.1126/science.aaa4984},
	language = {en},
	number = {6234},
	urldate = {2020-09-29},
	journal = {Science},
	author = {Urban, Mark C.},
	month = may,
	year = {2015},
	pmid = {25931559},
	note = {ZSCC: 0000959 
Publisher: American Association for the Advancement of Science
Section: Report},
	pages = {571--573},
}

@article{huang_accelerated_2016,
	title = {Accelerated dryland expansion under climate change},
	volume = {6},
	issn = {1758-6798},
	url = {http://www.nature.com/articles/nclimate2837},
	doi = {10.1038/nclimate2837},
	abstract = {Climate change is causing drylands to expand and this work shows that they will cover half of the land surface by 2100 under a moderate emissions scenario.},
	language = {en},
	number = {2},
	urldate = {2021-08-11},
	journal = {Nature Climate Change},
	author = {Huang, Jianping and Yu, Haipeng and Guan, Xiaodan and Wang, Guoyin and Guo, Ruixia},
	month = feb,
	year = {2016},
	note = {ZSCC: 0001034 
Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 2
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Developing world;Projection and prediction
Subject\_term\_id: developing-world;projection-and-prediction},
	pages = {166--171},
}
@article{holl_tree_2020,
	title = {Tree planting is not a simple solution},
	volume = {368},
	copyright = {Copyright © 2020, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/368/6491/580},
	doi = {10.1126/science.aba8232},
	abstract = {A plethora of articles suggest that tree planting can overcome a host of environmental problems, including climate change, water shortages, and the sixth mass extinction (1–3). Business leaders and politicians have jumped on the tree-planting bandwagon, and numerous nonprofit organizations and governments worldwide have started initiatives to plant billions or even trillions of trees for a host of social, ecological, and aesthetic reasons. Well-planned tree-planting projects are an important component of global efforts to improve ecological and human well-being. But tree planting becomes problematic when it is promoted as a simple, silver bullet solution and overshadows other actions that have greater potential for addressing the drivers of specific environmental problems, such as taking bold and rapid steps to reduce deforestation and greenhouse gas emissions.
Tree planting must be carefully planned and implemented to achieve desired outcomes
Tree planting must be carefully planned and implemented to achieve desired outcomes},
	language = {en},
	number = {6491},
	urldate = {2021-08-11},
	journal = {Science},
	author = {Holl, Karen D. and Brancalion, Pedro H. S.},
	month = may,
	year = {2020},
	pmid = {32381704},
	note = {ZSCC: 0000092 
Publisher: American Association for the Advancement of Science
Section: Perspective},
	pages = {580--581}
}

@article{rickford1994addressee,
  title={Addressee-and topic-influenced style shift: A quantitative sociolinguistic study},
  author={Rickford, John R and McNair-Knox, Faye and others},
  journal={Sociolinguistic perspectives on register},
  pages={235--276},
  year={1994},
  publisher={Oxford University Press New York}
}

@article{bergelson20126,
  title={At 6--9 months, human infants know the meanings of many common nouns},
  author={Bergelson, Elika and Swingley, Daniel},
  journal={Proceedings of the National Academy of Sciences},
  volume={109},
  number={9},
  pages={3253--3258},
  year={2012},
  publisher={National Acad Sciences}
}

@article{nash2008,
  doi = {10.1057/fr.2008.4},
  url = {https://doi.org/10.1057/fr.2008.4},
  year = {2008},
  month = jun,
  publisher = {{SAGE} Publications},
  volume = {89},
  number = {1},
  pages = {1--15},
  author = {Jennifer C. Nash},
  title = {Re-Thinking Intersectionality},
  journal = {Feminist Review}
}

@article{bright2016,
  doi = {10.1086/684173},
  url = {https://doi.org/10.1086/684173},
  year = {2016},
  month = jan,
  publisher = {University of Chicago Press},
  volume = {83},
  number = {1},
  pages = {60--81},
  author = {Liam Kofi Bright and Daniel Malinsky and Morgan Thompson},
  title = {Causally Interpreting Intersectionality Theory},
  journal = {Philosophy of Science}
}

@article{gines2011,
  doi = {10.5840/philtoday201155supplement68},
  url = {https://doi.org/10.5840/philtoday201155supplement68},
  year = {2011},
  publisher = {Philosophy Documentation Center},
  volume = {55},
  number = {9999},
  pages = {275--284},
  author = {Kathryn T. Gines},
  title = {Black Feminism and Intersectional Analyses},
  journal = {Philosophy Today}
}

@article{oconnor2019,
  doi = {10.1080/02691728.2018.1555870},
  url = {https://doi.org/10.1080/02691728.2018.1555870},
  year = {2019},
  month = jan,
  publisher = {Informa {UK} Limited},
  volume = {33},
  number = {1},
  pages = {23--41},
  author = {Cailin O'Connor and Liam Kofi Bright and Justin P. Bruner},
  title = {The Emergence of Intersectional Disadvantage},
  journal = {Social Epistemology}
}

@article{spiel2019,
  doi = {10.1145/3344919},
  url = {https://doi.org/10.1145/3344919},
  year = {2019},
  month = dec,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {26},
  number = {6},
  pages = {1--40},
  author = {Katta Spiel and Christopher Frauenberger and Os Keyes and Geraldine Fitzpatrick},
  title = {Agency of Autistic Children in Technology Research{\textemdash}A Critical Literature Review},
  journal = {{ACM} Transactions on Computer-Human Interaction}
}

@techreport{sullivan_updated_2015,
	title = {Updated {Value} of {Service} {Reliability} {Estimates} for {Electric} {Utility} {Customers} in the {United} {States}},
	url = {http://www.osti.gov/servlets/purl/1172643/},
	language = {en},
	number = {LBNL--6941E, 1172643},
	urldate = {2021-08-10},
	author = {Sullivan, Michael and Schellenberg, Josh and Blundell, Marshall},
	month = jan,
	year = {2015},
	doi = {10.2172/1172643},
	note = {ZSCC: 0000086 },
	pages = {LBNL--6941E, 1172643},
	file = {Sullivan et al. - 2015 - Updated Value of Service Reliability Estimates for.pdf:/Users/leg2015/Zotero/storage/KHBQENHP/Sullivan et al. - 2015 - Updated Value of Service Reliability Estimates for.pdf:application/pdf},
}


@article{baroni2021proper,
  title={On the proper role of linguistically-oriented deep net analysis in linguistic theorizing},
  author={Baroni, Marco},
  journal={arXiv preprint arXiv:2106.08694},
  year={2021}
}

@article{clark2019aristo,
  publtype={informal},
  author={Peter Clark and Oren Etzioni and Daniel Khashabi and Tushar Khot and Bhavana Dalvi Mishra and Kyle Richardson and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord and Niket Tandon and Sumithra Bhakthavatsalam and Dirk Groeneveld and Michal Guerquin and Michael Schmitz},
  title={From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project.},
  year={2019},
  cdate={1546300800000},
  journal={CoRR},
  volume={abs/1909.01958},
  url={http://arxiv.org/abs/1909.01958}
}

@article{priest1984selection,
  title={The selection of disputes for litigation},
  author={Priest, George L and Klein, Benjamin},
  journal={The Journal of Legal Studies},
  volume={13},
  number={1},
  pages={1--55},
  year={1984},
  publisher={The University of Chicago Law School}
}



@article{holzenberger2020dataset,
  title={A dataset for statutory reasoning in tax law entailment and question answering},
  author={Holzenberger, Nils and Blair-Stanek, Andrew and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2005.05257},
  year={2020}
}


@article{lee2014law,
  title={Law Without Lawyers: Access to Civil Justice and the Cost of Legal Services},
  author={Lee III, Emery G},
  journal={U. Miami L. Rev.},
  volume={69},
  pages={499},
  year={2014},
  publisher={HeinOnline}
}


@article{andreas2019measuring,
  title={Measuring Compositionality in Representation Learning},
  author={Jacob Andreas},
  journal={International Conference on Learning Representations},
  year={2019},
}

@article{levine2020limits,
  title={Limits to depth efficiencies of self-attention},
  author={Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
  journal={arXiv e-prints},
  pages={arXiv--2006},
  year={2020}
}

@inproceedings{mac,
  title={Compositional Attention Networks for Machine Reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={International Conference on Learning Representations (ICLR)},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
  keywords = {main}
}

@inproceedings{nsm,
  title={Learning by Abstraction: The neural state machine},
  author={Hudson, Drew and Manning, Christopher D},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5903--5916},
  year={2019}
}

@article{ntm,
 title={Neural {T}uring machines},
 author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
 journal={arXiv preprint arXiv:1410.5401},
 year={2014}
}

@article{dnc,
 title={Hybrid computing using a neural network with dynamic external memory},
 author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
 journal={Nature},
 volume={538},
 number={7626},
 pages={471--476},
 year={2016},
 publisher={Nature Research}
}

@inproceedings{nmn,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={39--48},
  year={2016}
}

@article{andreas2019good,
    title = "Good-Enough Compositional Data Augmentation",
    author = "Andreas, Jacob",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    journal = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.acl-main.676",
    pages = "7556--7566",
}

@article{bottou,
  title={From machine learning to machine reasoning},
  author={Bottou, L{\'e}on},
  journal={Machine learning},
  volume={94},
  number={2},
  pages={133--149},
  year={2014},
  publisher={Springer}
}

@inproceedings{kv,
  title={Key-Value Memory Networks for Directly Reading Documents},
  author={Miller, Alexander and Fisch, Adam and Dodge, Jesse and Karimi, Amir-Hossein and Bordes, Antoine and Weston, Jason},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={1400--1409},
  year={2016}
}

@article{yasunaga2021qagnn,
  title={QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering},
  author={Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  journal={arXiv preprint arXiv:2104.06378},
  year={2021}
}

@article{Verga2020FactsAE,
  title={Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge},
  author={Verga, Pat and Sun, Haitian and Soares, Livio Baldini and Cohen, William W},
  journal={arXiv preprint arXiv:2007.00849},
  year={2020}
}
@article{Jiang2020HowCW,
  title={How can we know what language models know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020},
  publisher={MIT Press}
}

@article{knn,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:1911.00172},
  year={2019}
}

@article{kepler,
  title={KEPLER: A unified model for knowledge embedding and pre-trained language representation},
  author={Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={176--194},
  year={2021},
  publisher={MIT Press}
}

@article{jaket,
  title={Jaket: Joint pre-training of knowledge graph and language understanding},
  author={Yu, Donghan and Zhu, Chenguang and Yang, Yiming and Zeng, Michael},
  journal={arXiv preprint arXiv:2010.00796},
  year={2020}
}
 
@techreport{kbert,
  title={KBERT. Knowledge Based Estimation of Material Release Transients},
  author={Washington, K and Browitt, DS and Murata, K and Monroe, D and Heames, T},
  year={1995},
  institution={Sandia National Labs., Albuquerque, NM (United States)}
}

@inproceedings{ensemble,
  title={Ensemble methods in machine learning},
  author={Dietterich, Thomas G},
  booktitle={International workshop on multiple classifier systems},
  pages={1--15},
  year={2000},
  organization={Springer}
}

@article{colake,
  title={Colake: Contextualized language and knowledge embedding},
  author={Sun, Tianxiang and Shao, Yunfan and Qiu, Xipeng and Guo, Qipeng and Hu, Yaru and Huang, Xuanjing and Zhang, Zheng},
  journal={arXiv preprint arXiv:2010.00309},
  year={2020}
}

@inproceedings{redwine,
  title={From red wine to red tomato: Composition with context},
  author={Misra, Ishan and Gupta, Abhinav and Hebert, Martial},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1792--1801},
  year={2017}
}

@article{ernie,
  title={ERNIE: Enhanced language representation with informative entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  journal={arXiv preprint arXiv:1905.07129},
  year={2019}
}

@article{yasunaga2021qa,
  title={QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering},
  author={Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  journal={arXiv preprint arXiv:2104.06378},
  year={2021}
}

@article{ent,
  title={Tracking the world state with recurrent entity networks},
  author={Henaff, Mikael and Weston, Jason and Szlam, Arthur and Bordes, Antoine and LeCun, Yann},
  journal={arXiv preprint arXiv:1612.03969},
  year={2016}
}

@article{humanthink,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and Brain Sciences},
  volume={40},
  year={2017},
  publisher={Cambridge University Press}
}

@inproceedings{dmn,
  title={Dynamic memory networks for visual and textual question answering},
  author={Xiong, Caiming and Merity, Stephen and Socher, Richard},
  booktitle={International conference on machine learning},
  pages={2397--2406},
  year={2016}
}

@article{Weston2015MemoryN,
  title={Memory networks},
  author={Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal={arXiv preprint arXiv:1410.3916},
  year={2014}
}

@inproceedings{Sukhbaatar2015WeaklySM,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  booktitle={Advances in neural information processing systems},
  pages={2440--2448},
  year={2015}
}


@incollection{compositionality,
  title={Compositionality},
  author={Janssen, Theo MV and Partee, Barbara H},
  booktitle={Handbook of logic and language},
  pages={417--473},
  year={1997},
  publisher={Elsevier}
}

@article{xue2020mt5,
  title={mt5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@book{glottolog,
  address      = {Leipzig},
  author       = {Harald Hammarström and Robert Forkel and Martin Haspelmath and Sebastian Bank},
  howpublished = {Max Planck Institute for Evolutionary Anthropology},
  title        = {Glottolog 4.4},
  url          = {https://glottolog.org/ accessed 2021-08-08},
  year         = {2021},
  doi          = {10.5281/zenodo.4761960}
}

@inproceedings{nordhoff2011glottolog,
  title={Glottolog/Langdoc: Defining dialects, languages, and language families as collections of resources},
  author={Nordhoff, Sebastian and Hammarstr{\"o}m, Harald},
  booktitle={First International Workshop on Linked Science 2011-In conjunction with the International Semantic Web Conference (ISWC 2011)},
  year={2011}
}

@misc{brynolfsson2011race,
  title={Race against the Machine},
  author={Brynjolfsson, Erik and McAfee, Andrew},
  year={2011},
  publisher={Lexington, Mass.: Digital Frontier Press}
}

@article{brynjolfsson2017can,
  title={What can machine learning do? Workforce implications},
  author={Brynjolfsson, Erik and Mitchell, Tom},
  journal={Science},
  volume={358},
  number={6370},
  pages={1530--1534},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{du2020few,
  title={Few-shot learning via learning the representation, provably},
  author={Du, Simon S and Hu, Wei and Kakade, Sham M and Lee, Jason D and Lei, Qi},
  journal={arXiv preprint arXiv:2002.09434},
  year={2020}
}

@article{tripuraneni2020theory,
  title={On the theory of transfer learning: The importance of task diversity},
  author={Tripuraneni, Nilesh and Jordan, Michael I and Jin, Chi},
  journal={arXiv preprint arXiv:2006.11650},
  year={2020}
}

@article{xie2020n,
  title={In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness},
  author={Xie, Sang Michael and Kumar, Ananya and Jones, Robbie and Khani, Fereshte and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2012.04550},
  year={2020}
}

@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT (1)},
  year={2019}
}

@article{loevinger1957,
author = {Jane Loevinger},
title ={Objective Tests as Instruments of Psychological Theory},
journal = {Psychological Reports},
volume = {3},
number = {3},
pages = {635-694},
year = {1957},
doi = {10.2466/pr0.1957.3.3.635},
URL = {https://doi.org/10.2466/pr0.1957.3.3.635},
eprint = {https://doi.org/10.2466/pr0.1957.3.3.635}
}

@book{cheney2009provenance,
  title={Provenance in databases: Why, how, and where},
  author={Cheney, James and Chiticariu, Laura and Tan, Wang-Chiew},
  year={2009},
  publisher={Now Publishers Inc}
}

@article{brofenbrenner1977,
  title={Toward an Experimental Ecology of Human Development.},
  author={Urie Bronfenbrenner},
  journal={American Psychologist},
  year={1977},
  volume={32},
  pages={513-531}
}

@incollection{goodhart1984,
  title={{Problems of monetary management: the UK experience}},
  author={Goodhart, Charles A.E.},
  booktitle={Monetary Theory and Practice},
  pages={91--121},
  year={1984},
  publisher={Springer},
  url={https://link.springer.com/chapter/10.1007/978-1-349-17295-5_4}
}

@article{messick1987,
  title={Validity},
  author={Messick, Samuel},
  journal={ETS Research Report Series},
  volume={1987},
  number={2},
  pages={i--208},
  year={1987},
  publisher={Wiley Online Library},
  url={https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1987.tb00244.x}
}

@article{messick1988,
  title={The once and future issues of validity: Assessing the meaning and consequences of measurement.},
  author={Messick, Samuel},
  year={1988},
  publisher={Lawrence Erlbaum Associates, Inc},
  url={https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1986.tb00185.x},
  journal = {ETS Research Report Series}
}

@article{strathern1997,
  title={{‘Improving ratings’: audit in the British University system}},
  author={Strathern, Marilyn},
  journal={European Review},
  volume={5},
  number={3},
  pages={305--321},
  year={1997},
  publisher={Cambridge University Press},
  url={https://www.cambridge.org/core/journals/european-review/article/abs/improving-ratings-audit-in-the-british-university-system/FC2EE640C0C44E3DB87C29FB666E9AAB}
}

@book{jackman2008,
  title={Measurement},
  author={Jackman, Simon},
  series={The Oxford Handbook of Political Methodology},
  url={https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199286546.001.0001/oxfordhb-9780199286546-e-6},
  year={2008},
  publisher={Oxford Handbooks}
}

@book{hand2010,
  title={Measurement Theory and Practice: The World Through Quantification},
  author={Hand, D.J.},
  isbn={9780470685679},
  lccn={2005297614},
  url={https://books.google.com/books?id=rap0PwAACAAJ},
  year={2010},
  publisher={Wiley}
}

@Book{bishop2006,
  author = 	 "Christopher M. Bishop",
  title = 	 "Pattern Recognition and Machine Learning",
  publisher = 	 "Springer",
  year = 	 "2006",
}

@article{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@article{vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={arXiv preprint arXiv:1908.02265},
  year={2019}
}

@article{convirt,
  title={Contrastive learning of medical visual representations from paired images and text},
  author={Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D and Langlotz, Curtis P},
  journal={arXiv preprint arXiv:2010.00747},
  year={2020}
}

@article{wavenet,
  title={WaveNet: A Generative Model for Raw Audio},
  author={van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv e-prints},
  pages={arXiv--1609},
  year={2016}
}

@article{elmo,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{attention,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inbook{brewer2014, place={Cambridge}, edition={2}, title={Research Design and Issues of Validity}, DOI={10.1017/CBO9780511996481.005}, booktitle={Handbook of Research Methods in Social and Personality Psychology}, publisher={Cambridge University Press}, author={Brewer, Marilynn B. and Crano, William D.}, editor={Reis, Harry T. and Judd, Charles M.Editors}, year={2014}, pages={11–26}}

@inproceedings{paperno2016,
    title = "The {LAMBADA} dataset: Word prediction requiring a broad discourse context",
    author = "Paperno, Denis  and
      Kruszewski, Germ{\'a}n  and
      Lazaridou, Angeliki  and
      Pham, Ngoc Quan  and
      Bernardi, Raffaella  and
      Pezzelle, Sandro  and
      Baroni, Marco  and
      Boleda, Gemma  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1144",
    doi = "10.18653/v1/P16-1144",
    pages = "1525--1534",
}



@misc{rogers2020,
  title = { Peer review in NLP: resource papers},
  journal = {Hacking Semantics},
  url = { https://hackingsemantics.xyz/2020/reviewing-data/ },
  author = {Rogers, Anna},
  day = { 16 },
  month = { Apr },
  year = { 2020 }
}

@inproceedings{nangia2020,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
}

@article{vries2020,
  title={Towards Ecologically Valid Research on Language User Interfaces},
  author={Harm de Vries and Dzmitry Bahdanau and Christopher D. Manning},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.14435}
}

@article{bragg2021,
  title={FLEX: Unifying Evaluation for Few-Shot NLP},
  author={Jonathan Bragg and Arman Cohan and Kyle Lo and Iz Beltagy},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.07170}
}

@article{corbett-davies2018,
  title={The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning},
  author={Sam Corbett-Davies and Sharad Goel},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.00023}
}

@inproceedings{milli2021,
author = {Milli, Smitha and Belli, Luca and Hardt, Moritz},
title = {From Optimizing Engagement to Measuring Value},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445933},
doi = {10.1145/3442188.3445933},
abstract = {Most recommendation engines today are based on predicting user engagement, e.g. predicting
whether a user will click on an item or not. However, there is potentially a large
gap between engagement signals and a desired notion of value that is worth optimizing
for. We use the framework of measurement theory to (a) confront the designer with
a normative question about what the designer values, (b) provide a general latent
variable model approach that can be used to operationalize the target construct and
directly optimize for it, and (c) guide the designer in evaluating and revising their
operationalization. We implement our approach on the Twitter platform on millions
of users. In line with established approaches to assessing the validity of measurements,
we perform a qualitative evaluation of how well our model captures a desired notion
of "value".},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {714–722},
numpages = {9},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{see2019,
    title = "Do Massively Pretrained Language Models Make Better Storytellers?",
    author = "See, Abigail  and
      Pappu, Aneesh  and
      Saxena, Rohun  and
      Yerukola, Akhila  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1079",
    doi = "10.18653/v1/K19-1079",
    pages = "843--861",
    abstract = "Large neural language models trained on massive amounts of text have emerged as a formidable strategy for Natural Language Understanding tasks. However, the strength of these models as Natural Language Generators is less clear. Though anecdotal evidence suggests that these models generate better quality text, there has been no detailed study characterizing their generation abilities. In this work, we compare the performance of an extensively pretrained model, OpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan et al., 2018). By evaluating the generated text across a wide variety of automatic metrics, we characterize the ways in which pretrained models do, and do not, make better storytellers. We find that although GPT2-117 conditions more strongly on context, is more sensitive to ordering of events, and uses more unusual words, it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing decoding algorithms.",
}


@inproceedings{jacobs2021,
author = {Jacobs, Abigail Z. and Wallach, Hanna},
title = {Measurement and Fairness},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445901},
doi = {10.1145/3442188.3445901},
abstract = {We propose measurement modeling from the quantitative social sciences as a framework
for understanding fairness in computational systems. Computational systems often involve
unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness,
and risk of recidivism. Such constructs cannot be measured directly and must instead
be inferred from measurements of observable properties (and other unobservable theoretical
constructs) thought to be related to them---i.e., operationalized via a measurement
model. This process, which necessarily involves making assumptions, introduces the
potential for mismatches between the theoretical understanding of the construct purported
to be measured and its operationalization. We argue that many of the harms discussed
in the literature on fairness in computational systems are direct results of such
mismatches. We show how some of these harms could have been anticipated and, in some
cases, mitigated if viewed through the lens of measurement modeling. To do this, we
contribute fairness-oriented conceptualizations of construct reliability and construct
validity that unite traditions from political science, education, and psychology and
provide a set of tools for making explicit and testing assumptions about constructs
and their operationalizations. We then turn to fairness itself, an essentially contested
construct that has different theoretical understandings in different contexts. We
argue that this contestedness underlies recent debates about fairness definitions:
although these debates appear to be about different operationalizations, they are,
in fact, debates about different theoretical understandings of fairness. We show how
measurement modeling can provide a framework for getting to the core of these debates.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {375–385},
numpages = {11},
keywords = {measurement, construct reliability, fairness, construct validity},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@book{Spirtes2001,
    title = {Causation, Prediction, and Search},
    year = {2001},
    author = {Spirtes, Peter and Glymour, Clark N and Scheines, Richard},
    edition = {2nd},
    publisher = {MIT Press},
}

@techreport{BCII2020,
	author = {Elias Bareinboim and Juan D. Correa and Duligur Ibeling and Thomas Icard},
	institution = {Causal AI Lab, Columbia University},
	number = {R-60},
	title = {On {P}earl's Hierarchy and the Foundations of Causal Inference},
	year = {2020},
	note = {Forthcoming in \emph{Probabilistic and Causal Inference: The Works of Judea Pearl} (ACM Books)}
}

@article{zellers2021piglet,
  title={PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World},
  author={Zellers, Rowan and Holtzman, Ari and Peters, Matthew and Mottaghi, Roozbeh and Kembhavi, Aniruddha and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:2106.00188},
  year={2021}
}

@article{arora2015latent,
  title={A latent variable model approach to PMI-based word embeddings},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  year={2016}
}
@article{chaabouni2021can,
  title={Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN},
  author={Chaabouni, Rahma and Dess{\`\i}, Roberto and Kharitonov, Eugene},
  journal={arXiv preprint arXiv:2107.01366},
  year={2021}
}

@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@article{pineau2020,
  title={Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)},
  author={Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and V. Larivi{\`e}re and A. Beygelzimer and Florence d'Alch{\'e}-Buc and E. Fox and H. Larochelle},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.12206}
}

@article{saunshi2020mathematical,
	title={A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks},
	author={Saunshi, Nikunj and Malladi, Sadhika and Arora, Sanjeev},
	journal={arXiv preprint arXiv:2010.03648},
	year={2020}
}

@article{zhang2021inductive,
	title={On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies},
	author={Zhang, Tianyi and Hashimoto, Tatsunori},
	journal={arXiv preprint arXiv:2104.05694},
	year={2021}
}

@book{cigna_2018, title={Cigna U.S. Loneliness Index}, url={https://www.cigna.com/assets/docs/newsroom/loneliness-survey-2018-full-report.pdf}, author={Cigna},year={2018}}

@article{einsteinVision,
  title={A Vision of AI for Joyful Education},
  url={https://blogs.scientificamerican.com/observations/a-vision-of-ai-for-joyful-education/},
  journal={Scientific American},
  author={Piech, Chris and Einstein, Lisa}, year={2020}, month={Feb}
}

@inproceedings{zhang2017dynamic,
  title={Dynamic key-value memory networks for knowledge tracing},
  author={Zhang, Jiani and Shi, Xingjian and King, Irwin and Yeung, Dit-Yan},
  booktitle={Proceedings of the 26th international conference on World Wide Web},
  pages={765--774},
  year={2017}
}

@article{wuvariational,
  title={Variational Item Response Theory: Fast, Accurate, and Expressive},
  author={Wu, Mike and Davis, Richard L and Domingue, Benjamin W and Piech, Chris and Goodman, Noah}
}

@inproceedings{dkt2015,
  title={Deep Knowledge Tracing},
  author={Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas J and Sohl-Dickstein, Jascha},
  booktitle={NeurIPS},
  year={2015}
}

@article{wei2020theoretical,
	title={Theoretical analysis of self-training with deep networks on unlabeled data},
	author={Wei, Colin and Shen, Kendrick and Chen, Yining and Ma, Tengyu},
	journal={arXiv preprint arXiv:2010.03622},
	year={2020}
}

@article{tosh2020contrastive,
	title        = {Contrastive estimation reveals topic posterior information to linear models},
	author       = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
	year         = 2020,
	journal      = {arXiv:2003.02234}
}

@inproceedings{tosh2021contrastive,
	title={Contrastive learning, multi-view redundancy, and linear models},
	author={Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
	booktitle={Algorithmic Learning Theory},
	pages={1179--1206},
	year={2021},
	organization={PMLR}
}

@article{bansal2020self,
	title={For self-supervised learning, Rationality implies generalization, provably},
	author={Bansal, Yamini and Kaplun, Gal and Barak, Boaz},
	journal={arXiv preprint arXiv:2010.08508},
	year={2020}
}

@inproceedings{wang2020understanding,
	title={Understanding contrastive representation learning through alignment and uniformity on the hypersphere},
	author={Wang, Tongzhou and Isola, Phillip},
	booktitle={International Conference on Machine Learning},
	pages={9929--9939},
	year={2020},
	organization={PMLR}
}

@article{Leike2018ScalableAA,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={J. Leike and David Krueger and Tom Everitt and Miljan Martic and Vishal Maini and S. Legg},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.07871}
}

@article{yudkowsky2016ai,
  title={The AI alignment problem: why it is hard, and where to start},
  author={Yudkowsky, Eliezer},
  journal={Symbolic Systems Distinguished Speaker},
  year={2016}
}

@article{cai2021theory,
	title={A Theory of Label Propagation for Subpopulation Shift},
	author={Cai, Tianle and Gao, Ruiqi and Lee, Jason D and Lei, Qi},
	journal={arXiv preprint arXiv:2102.11203},
	year={2021}
}

@article{zimmermann2021contrastive,
	title={Contrastive Learning Inverts the Data Generating Process},
	author={Zimmermann, Roland S and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	journal={arXiv preprint arXiv:2102.08850},
	year={2021}
}

@article{lee2020predicting,
	title        = {Predicting what you already know helps: Provable self-supervised learning},
	author       = {Lee, Jason D and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2008.01064}
}

@inproceedings{dao2019kernel,
	title={A kernel theory of modern data augmentation},
	author={Dao, Tri and Gu, Albert and Ratner, Alexander and Smith, Virginia and De Sa, Chris and R{\'e}, Christopher},
	booktitle={International Conference on Machine Learning},
	pages={1528--1537},
	year={2019},
	organization={PMLR}
}

@article{tsai2020self,
	title={Self-supervised learning from a multi-view perspective},
	author={Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	journal={arXiv preprint arXiv:2006.05576},
	year={2020}
}

@article{tian2020makes,
	title={What makes for good views for contrastive learning},
	author={Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
	journal={arXiv preprint arXiv:2005.10243},
	year={2020}
}

@article{tian2020understanding,
	title={Understanding self-supervised learning with dual deep networks},
	author={Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
	journal={arXiv preprint arXiv:2010.00578},
	year={2020}
}


@misc{gan2020threedworld,
    title={ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation},
    author={Chuang Gan and Jeremy Schwartz and Seth Alter and Martin Schrimpf and James Traer and Julian De Freitas and Jonas Kubilius and Abhishek Bhandwaldar and Nick Haber and Megumi Sano and Kuno Kim and Elias Wang and Damian Mrowca and Michael Lingelbach and Aidan Curtis and Kevin Feigelis and Daniel M. Bear and Dan Gutfreund and David Cox and James J. DiCarlo and Josh McDermott and Joshua B. Tenenbaum and Daniel L. K. Yamins},
    year={2020},
    eprint={2007.04954},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{arora2019theoretical,
	title={A theoretical analysis of contrastive unsupervised representation learning},
	author={Arora, Sanjeev and Khandeparkar, Hrishikesh and Khodak, Mikhail and Plevrakis, Orestis and Saunshi, Nikunj},
	journal={arXiv preprint arXiv:1902.09229},
	year={2019}
}

@inproceedings{roberts2020much,
  title={How Much Knowledge Can You Pack into the Parameters of a Language Model?},
  author={Roberts, Adam and Raffel, Colin and Shazeer, Noam},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={5418--5426},
  year={2020}
}
{"mode":"full","isActive":false}

@misc{bikowski2018demystifying,
    title={Demystifying MMD GANs},
    author={Mikołaj Bińkowski and Danica J. Sutherland and Michael Arbel and Arthur Gretton},
    year={2018},
    eprint={1801.01401},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{nie2018deeptag,
  title={DeepTag: inferring diagnoses from veterinary clinical notes},
  author={Nie, Allen and Zehnder, Ashley and Page, Rodney L and Zhang, Yuhui and Pineda, Arturo Lopez and Rivas, Manuel A and Bustamante, Carlos D and Zou, James},
  journal={NPJ digital medicine},
  volume={1},
  number={1},
  pages={1--8},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{zhang2019vettag,
  title={VetTag: improving automated veterinary diagnosis coding via large-scale language modeling},
  author={Zhang, Yuhui and Nie, Allen and Zehnder, Ashley and Page, Rodney L and Zou, James},
  journal={NPJ digital medicine},
  volume={2},
  number={1},
  pages={1--8},
  year={2019},
  publisher={Nature Publishing Group}
}

@article {rives2021,
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	title = {Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	number = {15},
	elocation-id = {e2016239118},
	year = {2021},
	doi = {10.1073/pnas.2016239118},
	publisher = {National Academy of Sciences},
	abstract = {Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue{\textendash}residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction.In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.Pretrained models and datasets are available at https://github.com/facebookresearch/esm.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/118/15/e2016239118},
	eprint = {https://www.pnas.org/content/118/15/e2016239118.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@InProceedings{radford2021,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@inproceedings{squad2,
  title={Know What You Don’t Know: Unanswerable Questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={784--789},
  year={2018}
}

@inproceedings{glue2,
  title={SuperGLUE: a stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={3266--3280},
  year={2019}
}

@article{ganformer,
  title={Generative Adversarial Transformers},
  author={Hudson, Drew A and Zitnick, C. Lawrence},
  journal={Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021},
  year={2021}
}

@inproceedings{biggan,
  title={Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@book{bengiobook,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{lunch,
  title={No free lunch theorems for optimization},
  author={Wolpert, David H and Macready, William G},
  journal={IEEE transactions on evolutionary computation},
  volume={1},
  number={1},
  pages={67--82},
  year={1997},
  publisher={IEEE}
}

@inproceedings{expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  booktitle={international conference on machine learning},
  pages={2847--2854},
  year={2017},
  organization={PMLR}
}

@article{styleclip,
  title={StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery},
  author={Patashnik, Or and Wu, Zongze and Shechtman, Eli and Cohen-Or, Daniel and Lischinski, Dani},
  journal={arXiv e-prints},
  pages={arXiv--2103},
  year={2021}
}

@inproceedings{stylegan2,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8110--8119},
  year={2020}
}

@inproceedings{visual_transformer,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@InProceedings{ramesh2021,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}


@article{ouyang2020video,
  title={Video-based AI for beat-to-beat assessment of cardiac function},
  author={Ouyang, David and He, Bryan and Ghorbani, Amirata and Yuan, Neal and Ebinger, Joseph and Langlotz, Curtis P and Heidenreich, Paul A and Harrington, Robert A and Liang, David H and Ashley, Euan A and others},
  journal={Nature},
  volume={580},
  number={7802},
  pages={252--256},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{abid2018exploring,
  title={Exploring patterns enriched in a dataset with contrastive principal component analysis},
  author={Abid, Abubakar and Zhang, Martin J and Bagaria, Vivek K and Zou, James},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--7},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{liu2021evaluating,
  title={Evaluating eligibility criteria of oncology trials using real-world data and AI},
  author={Liu, Ruishan and Rizzo, Shemra and Whipple, Samuel and Pal, Navdeep and Pineda, Arturo Lopez and Lu, Michael and Arnieri, Brandon and Lu, Ying and Capra, William and Copping, Ryan and others},
  journal={Nature},
  volume={592},
  number={7855},
  pages={629--633},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{wu2021babel,
  title={BABEL enables cross-modality translation between multiomic profiles at single-cell resolution},
  author={Wu, Kevin E and Yost, Kathryn E and Chang, Howard Y and Zou, James},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  year={2021},
  publisher={National Acad Sciences}
}

@inproceedings{fanbuch2020rubiks,
  title={RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition},
  author={Linxi Fan* and Shyamal Buch* and Guanzhi Wang and Ryan Cao and Yuke Zhu and Juan Carlos Niebles and Li Fei-Fei},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2020}
}

@inproceedings{wang2020negative,
  title={On Negative Interference in Multilingual Language Models},
  author={Wang, Zirui and Lipton, Zachary C and Tsvetkov, Yulia},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4438--4450},
  year={2020}
}

@inproceedings{jiang2020generalizing,
  title={Generalizing Natural Language Analysis through Span-relation Representations},
  author={Jiang, Zhengbao and Xu, Wei and Araki, Jun and Neubig, Graham},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={2120--2133},
  year={2020}
}

@article{Thorpe1996,
  doi = {10.1038/381520a0},
  url = {https://doi.org/10.1038/381520a0},
  year = {1996},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {381},
  number = {6582},
  pages = {520--522},
  author = {Simon Thorpe and Denis Fize and Catherine Marlot},
  title = {Speed of processing in the human visual system},
  journal = {Nature}
}


@article{tippett2021does,
  title={Does Lawyering Matter? Predicting Judicial Decisions from Legal Briefs, and What That Means for Access to Justice},
  author={Tippett, Elizabeth Chika and Alexander, Charlotte and Branting, L Karl},
  journal={Texas Law Review, Forthcoming},
  year={2021}
}


@article{brescia2014embracing,
  title={Embracing disruption: How technological change in the delivery of legal services can improve access to justice},
  author={Brescia, Raymond H and McCarthy, Walter and McDonald, Ashley and Potts, Kellan and Rivais, Cassandra},
  journal={Alb. L. Rev.},
  volume={78},
  pages={553},
  year={2014},
  publisher={HeinOnline}
}


@article{queudot2020improving,
  title={Improving Access to Justice with Legal Chatbots},
  author={Queudot, Marc and Charton, {\'E}ric and Meurs, Marie-Jean},
  journal={Stats},
  volume={3},
  number={3},
  pages={356--375},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}


@inproceedings{Moravec1998WhenWC,
  title={When will computer hardware match the human brain},
  author={H. Moravec},
  year={1998}
}

@inproceedings{westermann2019using,
  title={Using factors to predict and analyze landlord-tenant decisions to increase access to justice},
  author={Westermann, Hannes and Walker, Vern R and Ashley, Kevin D and Benyekhlef, Karim},
  booktitle={Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law},
  pages={133--142},
  year={2019}
}


@article{biederman_perceiving_1972,
	title = {Perceiving real-world scenes},
	volume = {177},
	issn = {0036-8075},
	doi = {10.1126/science.177.4043.77},
	abstract = {When a briefly presented real-world scene was jumbled, the accuracy of identifying a single, cued object was less than that when the scene was coherent. Jumbling remained an effective variable even when the subject knew where to look and what to look for. Thus an object's meaningful context may affect the course of perceptual recognition and not just peripheral scanning or memory.},
	language = {eng},
	number = {4043},
	journal = {Science (New York, N.Y.)},
	author = {Biederman, I.},
	month = jul,
	year = {1972},
	pmid = {5041781},
	keywords = {Cues, Humans, Perceptual Distortion, Visual Perception},
	pages = {77--80},
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}
{"mode":"full","isActive":false}

@inproceedings{qi-etal-2019-answering,
    title = "Answering Complex Open-domain Questions Through Iterative Query Generation",
    author = "Qi, Peng  and
      Lin, Xiaowen  and
      Mehr, Leo  and
      Wang, Zijian  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1261",
    doi = "10.18653/v1/D19-1261",
    pages = "2590--2602",
    abstract = "It is challenging for current one-step retrieve-and-read question answering (QA) systems to answer questions like {``}Which novel by the author of {`}Armada{'} will be adapted as a feature film by Steven Spielberg?{''} because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GoldEn (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GoldEn Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GoldEn Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GoldEn Retriever on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.",
}


@InProceedings{pmlr-v126-gopinath20a,
  title = 	 {Fast, Structured Clinical Documentation via Contextual Autocomplete},
  author =       {Gopinath, Divya and Agrawal, Monica and Murray, Luke and Horng, Steven and Karger, David and Sontag, David},
  booktitle = 	 {Proceedings of the 5th Machine Learning for Healthcare Conference},
  pages = 	 {842--870},
  year = 	 {2020},
  editor = 	 {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  volume = 	 {126},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--08 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v126/gopinath20a/gopinath20a.pdf},
  url = 	 {http://proceedings.mlr.press/v126/gopinath20a.html},
  abstract = 	 {We present a system that uses a learned autocompletion mechanism to facilitate rapid creation of semi-structured clinical documentation. We dynamically suggest relevant clinical concepts as a doctor drafts a note by leveraging features from both unstructured and structured medical data. By constraining our architecture to shallow neural networks, we are able to make these suggestions in real time. Furthermore, as our algorithm is used to write a note, we can automatically annotate the documentation with clean labels of clinical concepts drawn from medical vocabularies, making notes more structured and readable for physicians, patients, and future algorithms. To our knowledge, this system is the only machine learning-based documentation utility for clinical notes deployed in a live hospital setting, and it reduces keystroke burden of clinical concepts by 67\% in real environments.}
}


@article{goyal2021larger,
  title={Larger-Scale Transformers for Multilingual Masked Language Modeling},
  author={Goyal, Naman and Du, Jingfei and Ott, Myle and Anantharaman, Giri and Conneau, Alexis},
  journal={arXiv preprint arXiv:2105.00572},
  year={2021}
}

@article{sankoff2018change,
author = {Sankoff, Gillian},
title = {Language Change Across the Lifespan},
journal = {Annual Review of Linguistics},
volume = {4},
number = {1},
pages = {297-316},
year = {2018},
doi = {10.1146/annurev-linguistics-011817-045438},
URL = { https://doi.org/10.1146/annurev-linguistics-011817-045438},
eprint = {https://doi.org/10.1146/annurev-linguistics-011817-045438}
}

@techreport{stern2021social,
  title={The social cost of carbon, risk, distribution, market failures: An alternative approach},
  author={Stern, Nicholas and Stiglitz, Joseph E},
  year={2021},
  institution={National Bureau of Economic Research}
}


@article{hegel2021law,
  title={The Law of Large Documents: Understanding the Structure of Legal Contracts Using Visual Cues},
  author={Hegel, Allison and Shah, Marina and Peaslee, Genevieve and Roof, Brendan and Elwany, Emad},
  journal={arXiv preprint arXiv:2107.08128},
  year={2021}
}


@article{lazaridou2021pitfalls,
  title={Pitfalls of Static Language Modelling},
  author={Lazaridou, Angeliki and Kuncoro, Adhiguna and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and d'Autume, Cyprien de Masson and Ruder, Sebastian and Yogatama, Dani and others},
  journal={arXiv preprint arXiv:2102.01951},
  year={2021}
}

@inproceedings{nguer2020sencorpus,
  title={SENCORPUS: A French-Wolof Parallel Corpus},
  author={Nguer, Elhadji Mamadou and Lo, Alla and Dione, Cheikh M Bamba and Ba, Sileye O and Lo, Moussa},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={2803--2811},
  year={2020}
}

@article{lee2019patentbert,
  title={Patentbert: Patent classification with fine-tuning a pre-trained bert model},
  author={Lee, Jieh-Sheng and Hsiang, Jieh},
  journal={arXiv preprint arXiv:1906.02124},
  year={2019}
}


@article{goh2021multimodal,
  title={Multimodal neurons in artificial neural networks},
  author={Goh, Gabriel and Cammarata, Nick and Voss, Chelsea and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  journal={Distill},
  volume={6},
  number={3},
  pages={e30},
  year={2021}
}


@article{carter2019activation,
  title={Activation atlas},
  author={Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  journal={Distill},
  volume={4},
  number={3},
  pages={e15},
  year={2019}
}


@article{mu2020compositional,
  title={Compositional explanations of neurons},
  author={Mu, Jesse and Andreas, Jacob},
  journal={arXiv preprint arXiv:2006.14032},
  year={2020}
}


@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}


@article{miller1985egotism,
  title={Egotism in group members: Public and private attributions of responsibility for group performance},
  author={Miller, Rowland S and Schlenker, Barry R},
  journal={Social Psychology Quarterly},
  pages={85--89},
  year={1985},
  publisher={JSTOR}
}



@article{shepperd2008exploring,
  title={Exploring causes of the self-serving bias},
  author={Shepperd, James and Malone, Wendi and Sweeny, Kate},
  journal={Social and Personality Psychology Compass},
  volume={2},
  number={2},
  pages={895--908},
  year={2008},
  publisher={Wiley Online Library}
}


@inproceedings{jin2020bert,
  title={Is bert really robust? a strong baseline for natural language attack on text classification and entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8018--8025},
  year={2020}
}


@article{garg2020bae,
  title={Bae: Bert-based adversarial examples for text classification},
  author={Garg, Siddhant and Ramakrishnan, Goutham},
  journal={arXiv preprint arXiv:2004.01970},
  year={2020}
}

@inproceedings{joshi2020state,
  title={The State and Fate of Linguistic Diversity and Inclusion in the NLP World},
  author={Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={6282--6293},
  year={2020}
}

@article{bender2011achieving,
  title={On achieving and evaluating language-independence in NLP},
  author={Bender, Emily M},
  journal={Linguistic Issues in Language Technology},
  volume={6},
  number={3},
  pages={1--26},
  year={2011}
}

@inproceedings{lauscher2020zero,
  title={From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers},
  author={Lauscher, Anne and Ravishankar, Vinit and Vuli{\'c}, Ivan and Glava{\v{s}}, Goran},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4483--4499},
  year={2020}
}

@article{ponti2019modeling,
    author = {Ponti, Edoardo Maria and O’Horan, Helen and Berzak, Yevgeni and Vulić, Ivan and Reichart, Roi and Poibeau, Thierry and Shutova, Ekaterina and Korhonen, Anna},
    title = "{Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing}",
    journal = {Computational Linguistics},
    volume = {45},
    number = {3},
    pages = {559-601},
    year = {2019},
    month = {09},
    issn = {0891-2017},
    doi = {10.1162/coli_a_00357},
    url = {https://doi.org/10.1162/coli\_a\_00357},
    eprint = {https://direct.mit.edu/coli/article-pdf/45/3/559/1847397/coli\_a\_00357.pdf},
}

@article{yoshikawa2021retrosynthesis-twitter,
  doi = {10.1186/s13321-021-00527-x},
  url = {https://doi.org/10.1186/s13321-021-00527-x},
  year = {2021},
  month = jul,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {13},
  number = {1},
  author = {Naruki Yoshikawa and Ryuichi Kubo and Kazuki Z. Yamamoto},
  title = {Twitter integration of chemistry software tools},
  journal = {Journal of Cheminformatics}
}

@article{genheden2020retrosynthesis,
  doi = {10.1186/s13321-020-00472-1},
  url = {https://doi.org/10.1186/s13321-020-00472-1},
  year = {2020},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {12},
  number = {1},
  author = {Samuel Genheden and Amol Thakkar and Veronika Chadimov{\'{a}} and Jean-Louis Reymond and Ola Engkvist and Esben Bjerrum},
  title = {{AiZynthFinder}: a fast,  robust and flexible open-source software for retrosynthetic planning},
  journal = {Journal of Cheminformatics}
}

@inproceedings{geiger-etal-2020-neural,
    title = "Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation",
    author = "Geiger, Atticus  and
      Richardson, Kyle  and
      Potts, Christopher",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.16",
    doi = "10.18653/v1/2020.blackboxnlp-1.16",
    pages = "163--173"
}

@article{vig2020causal,
  title={Causal mediation analysis for interpreting neural {NLP}: {T}he case of gender bias},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Sakenis, Simas and Huang, Jason and Singer, Yaron and Shieber, Stuart},
  journal={arXiv preprint arXiv:2004.12265},
  year={2020}
}

@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  pages={30046--30054},
  year={2020}
}

@article{li2020exploring,
  title={Exploring the role of argument structure in online debate persuasion},
  author={Li, Jialu and Durmus, Esin and Cardie, Claire},
  journal={arXiv preprint arXiv:2010.03538},
  year={2020}
}


@article{vieira2020understanding,
  title={Understanding the societal impacts of machine translation: a critical review of the literature on medical and legal use cases},
  author={Vieira, Lucas Nunes and O’Hagan, Minako and O’Sullivan, Carol},
  journal={Information, Communication \& Society},
  pages={1--18},
  year={2020},
  publisher={Taylor \& Francis}
}


@inproceedings{bender2020climbing,
  title={Climbing towards {NLU}: {O}n meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5185--5198},
  year={2020}
}

@article{merrill2021provable,
  title={Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?},
  author={Merrill, William and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A},
  journal={arXiv preprint arXiv:2104.10809},
  year={2021}
}

@book{elbourne2011meaning,
  title={Meaning: a slim guide to semantics},
  author={Elbourne, Paul},
  year={2011},
  publisher={Oxford University Press}
}

@InCollection{grimm2021understanding,
	author       =	{Grimm, Stephen},
	title        =	{{Understanding}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2021/entries/understanding/}},
	year         =	{2021},
	edition      =	{{S}ummer 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@misc{thehaiadaptiveagentsgroup2021when, title={When Artificial Agents Lie, Defame, and Defraud, Who Is to Blame?}, url={https://hai.stanford.edu/news/when-artificial-agents-lie-defame-and-defraud-who-blame}, author={{The HAI Adaptive Agents Group}}, year={2021}, month={Apr}
}

@misc{rong2021extrapolating, title={Extrapolating to Unnatural Language Processing with GPT-3's In-context Learning: The Good, the Bad, and the Mysterious}, url={http://ai.stanford.edu/blog/in-context-learning/}, author={Rong, Frieda}, year={2021}, month={May}
}



@article{jackendoff2011human,
 ISSN = {00978507, 15350665},
 URL = {http://www.jstor.org/stable/23011656},
 author = {Ray Jackendoff},
 journal = {Language},
 number = {3},
 pages = {586--624},
 publisher = {Linguistic Society of America},
 title = {What is the human language faculty? Two views},
 volume = {87},
 year = {2011}
}

@article{elwany2019bert,
  title={Bert goes to law school: Quantifying the competitive advantage of access to large legal corpora in contract understanding},
  author={Elwany, Emad and Moore, Dave and Oberoi, Gaurav},
  journal={arXiv preprint arXiv:1911.00473},
  year={2019}
}

@article{dickinson2018computational,
  title={A Computational Analysis of Oral Argument in the Supreme Court},
  author={Dickinson, Gregory M},
  journal={Cornell JL \& Pub. Pol'y},
  volume={28},
  pages={449},
  year={2018},
  publisher={HeinOnline}
}

@misc{anthony2020carbontracker,
  title={Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models},
  author={Lasse F. Wolff Anthony and Benjamin Kanding and Raghavendra Selvan},
  howpublished={ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems},
  month={July},
  note={arXiv:2007.03051},
  year={2020}}

@article{rolnick2019tackling,
  title={Tackling climate change with machine learning},
  author={Rolnick, David and Donti, Priya L and Kaack, Lynn H and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and others},
  journal={arXiv preprint arXiv:1906.05433},
  year={2019}
}


@article{rice2019racial,
  title={Racial bias in legal language},
  author={Rice, Douglas and Rhodes, Jesse H and Nteta, Tatishe},
  journal={Research \& Politics},
  volume={6},
  number={2},
  pages={2053168019848930},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}



@article{cui2018application,
  title={Application Of Zero-Knowledge Proof In Resolving Disputes Of Privileged Documents In E-Discovery},
  author={Cui, Yuqing},
  journal={Harv. JL \& Tech.},
  volume={32},
  pages={633},
  year={2018},
  publisher={HeinOnline}
}


@article{yang2021goldilocks,
  title={Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review},
  author={Yang, Eugene and MacAvaney, Sean and Lewis, David D and Frieder, Ophir},
  journal={arXiv preprint arXiv:2105.01044},
  year={2021}
}


@article{oard2018jointly,
  title={Jointly minimizing the expected costs of review for responsiveness and privilege in E-discovery},
  author={Oard, Douglas W and Sebastiani, Fabrizio and Vinjumur, Jyothi K},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={37},
  number={1},
  pages={1--35},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{gradient-checkpointing,
  author    = {Tianqi Chen and
               Bing Xu and
               Chiyuan Zhang and
               Carlos Guestrin},
  title     = {Training Deep Nets with Sublinear Memory Cost},
  journal   = {CoRR},
  volume    = {abs/1604.06174},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.06174},
  archivePrefix = {arXiv},
  eprint    = {1604.06174},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChenXZG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}

@article{chalkidis2020legal,
  title={LEGAL-BERT: The muppets straight out of law school},
  author={Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  journal={arXiv preprint arXiv:2010.02559},
  year={2020}
}

@inproceedings{chalkidis2019large,
    title = "Large-Scale Multi-Label Text Classification on {EU} Legislation",
    author = "Chalkidis, Ilias  and
      Fergadiotis, Emmanouil  and
      Malakasiotis, Prodromos  and
      Androutsopoulos, Ion",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1636",
    doi = "10.18653/v1/P19-1636",
    pages = "6314--6322",
    abstract = "We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with ∼4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT{'}s maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.",
}


@article{choi2020empirical,
  title={An Empirical Study of Statutory Interpretation in Tax Law},
  author={Choi, Jonathan H},
  journal={NYUL Rev.},
  volume={95},
  pages={363},
  year={2020},
  publisher={HeinOnline}
}

@article{chalkidis2019neural,
  title={Neural legal judgment prediction in English},
  author={Chalkidis, Ilias and Androutsopoulos, Ion and Aletras, Nikolaos},
  journal={arXiv preprint arXiv:1906.02059},
  year={2019}
}

@article{nyarko2020statistical,
  title={A Statistical Test for Legal Interpretation: Theory and Applications},
  author={Nyarko, Julian and Sanga, Sarath},
  journal={Available at SSRN 3737292},
  year={2020}
}

@article{re2019developing,
  title={Developing artificially intelligent justice},
  author={Re, Richard M and Solow-Niederman, Alicia},
  journal={Stan. Tech. L. Rev.},
  volume={22},
  pages={242},
  year={2019},
  publisher={HeinOnline}
}


@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}

@book{comrie1989language,
  title={Language universals and linguistic typology: Syntax and morphology},
  author={Comrie, Bernard},
  year={1989},
  publisher={University of Chicago press}
}
@book{chomsky2014minimalist,
  title={The minimalist program},
  author={Chomsky, Noam},
  year={2014},
  publisher={MIT press}
}

@book{croft2001radical,
  title={Radical construction grammar: Syntactic theory in typological perspective},
  author={Croft, William},
  year={2001},
  publisher={Oxford University Press on Demand}
}

@article{dupoux2018cognitive,
  title={Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner},
  author={Dupoux, Emmanuel},
  journal={Cognition},
  volume={173},
  pages={43--59},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{wu-dredze-2019-beto,
    title = "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1077",
    doi = "10.18653/v1/D19-1077",
    pages = "833--844",
}

@inproceedings{
Khandelwal2020Generalization,
title={Generalization through Memorization: Nearest Neighbor Language Models},
author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HklBjCEKvH}
}

@article{poplin2018prediction,
  title={Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning},
  author={Poplin, Ryan and Varadarajan, Avinash V and Blumer, Katy and Liu, Yun and McConnell, Michael V and Corrado, Greg S and Peng, Lily and Webster, Dale R},
  journal={Nature Biomedical Engineering},
  volume={2},
  number={3},
  pages={158--164},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{wang2015visual,
  title={Visual tracking with fully convolutional networks},
  author={Wang, Lijun and Ouyang, Wanli and Wang, Xiaogang and Lu, Huchuan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3119--3127},
  year={2015}
}

@article{li2015visualizing,
  title={Visualizing and understanding neural models in nlp},
  author={Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1506.01066},
  year={2015}
}

@article{zintgraf2017visualizing,
  title={Visualizing deep neural network decisions: Prediction difference analysis},
  author={Zintgraf, Luisa M and Cohen, Taco S and Adel, Tameem and Welling, Max},
  journal={arXiv preprint arXiv:1702.04595},
  year={2017}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Proceedings of the 31st international conference on neural information processing systems},
  pages={4768--4777},
  year={2017}
}

@article{lapuschkin2019unmasking,
  title={Unmasking Clever Hans predictors and assessing what machines really learn},
  author={Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--8},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{nguyen2016synthesizing,
  title={Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
  author={Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3387--3395},
  year={2016}
}

@article{thomas2019analyzing,
  title={Analyzing neuroimaging data through recurrent deep learning models},
  author={Thomas, Armin W and Heekeren, Hauke R and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={Frontiers in neuroscience},
  volume={13},
  pages={1321},
  year={2019},
  publisher={Frontiers},
  doi={10.3389/fnins.2019.01321},
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{bach2015pixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
}

@article{samek2021explaining,
  title={Explaining deep neural networks and beyond: A review of methods and applications},
  author={Samek, Wojciech and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Anders, Christopher J and M{\"u}ller, Klaus-Robert},
  journal={Proceedings of the IEEE},
  volume={109},
  number={3},
  pages={247--278},
  year={2021},
  publisher={IEEE}
}

@article{xie2020n,
  title={In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness},
  author={Xie, Sang Michael and Kumar, Ananya and Jones, Robbie and Khani, Fereshte and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2012.04550},
  year={2020}
}

@article{wu2021identifying,
  title={Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models},
  author={Wu, Zhengxuan and Liu, Nelson F and Potts, Christopher},
  journal={arXiv preprint arXiv:2104.08410},
  year={2021}
}

@inproceedings{kataoka2020pre,
  title={Pre-training without natural images},
  author={Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  year={2020}
}

@article{lu2021pretrained,
  title={Pretrained transformers as universal computation engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}

@article{papadimitriou2020learning,
  title={Learning music helps you read: Using transfer to study linguistic structure in language models},
  author={Papadimitriou, Isabel and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2004.14601},
  year={2020}
}

@misc{ye2021oodbench,
      title={OoD-Bench: Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms}, 
      author={Nanyang Ye and Kaican Li and Lanqing Hong and Haoyue Bai and Yiting Chen and Fengwei Zhou and Zhenguo Li},
      year={2021},
}

@inproceedings{candela2009when,
  author={Quiñonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
  booktitle={Dataset Shift in Machine Learning}, 
  title={When Training and Test Sets Are Different: Characterizing Learning Transfer}, 
  year={2009},
  volume={},
  number={},
  pages={3-28},
  doi={}
  }

@article{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Tony Z and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={arXiv preprint arXiv:2102.09690},
  year={2021}
}

@book{marr1982vision,
  title={Vision: A computational investigation into the human representation and processing of visual information},
  author={Marr, David},
  year={1982},
  publisher={W.H. Freeman},
  address={San Francisco}
 }

@article{friedman1996,
author = {Friedman, Batya and Nissenbaum, Helen},
title = {Bias in Computer Systems},
year = {1996},
issue_date = {July 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/230538.230561},
doi = {10.1145/230538.230561},
abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
journal = {ACM Transactions on Information Systems},
month = jul,
pages = {330–347},
numpages = {18},
keywords = {computers and society, standards, human values, universal design, ethics, computer ethics, social impact, design methods, system design, bias, social computing, values}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{lacoste2019quantifying,
  title={Quantifying the carbon emissions of machine learning},
  author={Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019}
}

@article{henderson2020towards,
  title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={248},
  pages={1--43},
  year={2020}
}

@misc{pineau2020improving,
      title={Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)}, 
      author={Joelle Pineau and Philippe Vincent-Lamarre and Koustuv Sinha and Vincent Larivière and Alina Beygelzimer and Florence d'Alché-Buc and Emily Fox and Hugo Larochelle},
      year={2020},
      eprint={2003.12206},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{schwartz2019green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={arXiv preprint arXiv:1907.10597},
  year={2019}
}

@article{sweeney2013,
 author = {Sweeney, Latanya},
 title = {Discrimination in Online Ad Delivery},
 journal = {Queue},
 issue_date = {March 2013},
 volume = {11},
 number = {3},
 month = mar,
 year = {2013},
 issn = {1542-7730},
 pages = {10:10--10:29},
 articleno = {10},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/2460276.2460278},
 doi = {10.1145/2460276.2460278},
 acmid = {2460278},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{cao2020towards,
  title={Towards accurate and reliable energy measurement of NLP models},
  author={Cao, Qingqing and Balasubramanian, Aruna and Balasubramanian, Niranjan},
  journal={arXiv preprint arXiv:2010.05248},
  year={2020}
}


@article{parcollet2021energy,
  title={The Energy and Carbon Footprint of Training End-to-End Speech Recognizers},
  author={Parcollet, Titouan and Ravanelli, Mirco},
  year={2021}
}


@inproceedings{kay2015,
 author = {Kay, Matthew and Matuszek, Cynthia and Munson, Sean A.},
 title = {Unequal Representation and Gender Stereotypes in Image Search Results for Occupations},
 booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
 series = {CHI '15},
 year = {2015},
 isbn = {978-1-4503-3145-6},
 location = {Seoul, Republic of Korea},
 pages = {3819--3828},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2702123.2702520},
 doi = {10.1145/2702123.2702520},
 acmid = {2702520},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bias, gender, image search, inequality, representation, stereotypes},
}

@inproceedings{bolukbasi2016,
 author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
 url = {https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume = {29},
 year = {2016}
}



@article{caliskan2017,
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	number = {6334},
	pages = {183--186},
	year = {2017},
	doi = {10.1126/science.aal4230},
	publisher = {American Association for the Advancement of Science},
	abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs{\textemdash}for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.Science, this issue p. 183; see also p. 133Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/356/6334/183},
	eprint = {https://science.sciencemag.org/content/356/6334/183.full.pdf},
	journal = {Science}
}


@InProceedings{buolamwini2018, 
title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification}, author = {Joy Buolamwini and Timnit Gebru}, booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency}, pages = {77--91}, year = {2018}, editor = {Sorelle A. Friedler and Christo Wilson}, volume = {81}, series = {Proceedings of Machine Learning Research}, address = {New York, NY, USA}, month = {23--24 Feb}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf}, url = {http://proceedings.mlr.press/v81/buolamwini18a.html}, abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.} }

@article{kaplan2020,
  title={Scaling Laws for Neural Language Models},
  author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.08361},
  url={https://arxiv.org/abs/2001.08361}
}

@inproceedings{blodgett2020,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
}

@inproceedings{hutchinson2020,
    title = "Social Biases in {NLP} Models as Barriers for Persons with Disabilities",
    author = "Hutchinson, Ben  and
      Prabhakaran, Vinodkumar  and
      Denton, Emily  and
      Webster, Kellie  and
      Zhong, Yu  and
      Denuyl, Stephen",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.487",
    doi = "10.18653/v1/2020.acl-main.487",
    pages = "5491--5501",
    abstract = "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",
}

@inproceedings{linzen2020,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.465",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",
}

@inproceedings{ethayarajh2020,
    title = "Utility is in the Eye of the User: A Critique of {NLP} Leaderboards",
    author = "Ethayarajh, Kawin  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.393",
    doi = "10.18653/v1/2020.emnlp-main.393",
    pages = "4846--4853",
    abstract = "Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards {--} in their current form {--} can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model{'}s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).",
}

@article{henighan2020,
  title={Scaling Laws for Autoregressive Generative Modeling},
  author={Tom Henighan and Jared Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and T. Brown and Prafulla Dhariwal and Scott Gray and Chris Hallacy and Benjamin Mann and Alec Radford and Aditya Ramesh and Nick Ryder and Daniel M. Ziegler and John Schulman and Dario Amodei and Sam McCandlish},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.14701},
  url={https://arxiv.org/abs/2010.14701}
}

@article{zhou2020hulk,
  title={Hulk: An energy efficiency benchmark platform for responsible natural language processing},
  author={Zhou, Xiyou and Chen, Zhiyu and Jin, Xiaoyong and Wang, William Yang},
  journal={arXiv preprint arXiv:2002.05829},
  year={2020}
}


@article{codecarbon,
  author={Victor Schmidt and Kamal Goyal and Aditya Joshi and Boris Feld and Liam Conell and Nikolas Laskaris and Doug Blank and Jonathan Wilson and Sorelle Friedler and Sasha Luccioni},
  title={{CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing}},
  year={2021},
  howpublished={\url{https://github.com/mlco2/codecarbon}},
  DOI={10.5281/zenodo.4658424},
  publisher={Zenodo},
}

@article{chugg2021enhancing,
  title={Enhancing Environmental Enforcement with Near Real-Time Monitoring: Likelihood-Based Detection of Structural Expansion of Intensive Livestock Farms},
  author={Chugg, Ben and Anderson, Brandon and Eicher, Seiji and Lee, Sandy and Ho, Daniel E},
  journal={arXiv preprint arXiv:2105.14159},
  year={2021}
}

@article{ray2014government,
  title={A government success story: How data analysis by the Social Security Appeals Council (with a push from the Administrative Conference of the United States) is transforming social security disability adjudication},
  author={Ray, Gerald K and Lubbers, Jeffrey S},
  journal={Geo. Wash. L. Rev.},
  volume={83},
  pages={1575},
  year={2014},
  publisher={HeinOnline}
}


@article{chaudhary2020topicbert,
  title={TopicBERT for energy efficient document classification},
  author={Chaudhary, Yatin and Gupta, Pankaj and Saxena, Khushbu and Kulkarni, Vivek and Runkler, Thomas and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2010.16407},
  year={2020}
}


@article{stewart2014uneven,
  title={The uneven distribution of environmental burdens and benefits in Silicon Valley's backyard},
  author={Stewart, Iris T and Bacon, Christopher M and Burke, William D},
  journal={Applied Geography},
  volume={55},
  pages={266--277},
  year={2014},
  publisher={Elsevier}
}


@inproceedings{gupta2021chasing,
  title={Chasing Carbon: The Elusive Environmental Footprint of Computing},
  author={Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={854--867},
  year={2021},
  organization={IEEE}
}


@article{birhane2020,
  title={The Underlying Values of Machine Learning Research},
  author={Abeba Birhane and Pratyusha Kalluri and Dallas Card and William Agnew and Ravit Dotan and Michelle Bao},
  year={2020},
  url={https://drive.google.com/file/d/1tjrm3Bf1hxV8iuPSiCcM1IazITGp-GZj/view}
}

@article{lottick2019nergy,
    title={Energy Usage Reports: Environmental awareness as part of algorithmic
  accountability},
    author={Kadan Lottick and Silvia Susai and Sorelle A. Friedler and Jonathan P. Wilson},
    year={2019},
    journal={Workshop on Tackling Climate Change with Machine Learning at NeurIPS 2019}
}

@article{hong2020divergent,
  title={Divergent responses of soil organic carbon to afforestation},
  author={Hong, Songbai and Yin, Guodong and Piao, Shilong and Dybzinski, Ray and Cong, Nan and Li, Xiangyi and Wang, Kai and Pe{\~n}uelas, Josep and Zeng, Hui and Chen, Anping},
  journal={Nature Sustainability},
  volume={3},
  number={9},
  pages={694--700},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{heilmayr2020impacts,
  title={Impacts of Chilean forest subsidies on forest cover, carbon and biodiversity},
  author={Heilmayr, Robert and Echeverr{\'\i}a, Cristian and Lambin, Eric F},
  journal={Nature Sustainability},
  volume={3},
  number={9},
  pages={701--709},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{lannelongue2021green,
  title={Green algorithms: Quantifying the carbon footprint of computation},
  author={Lannelongue, Lo{\"\i}c and Grealey, Jason and Inouye, Michael},
  journal={Advanced Science},
  pages={2100707},
  year={2021},
  publisher={Wiley Online Library}
}

@article{koh2020,
  title={{WILDS: A Benchmark of in-the-Wild Distribution Shifts}},
  author={Pang Wei Koh and Shiori Sagawa and H. Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Wei-hua Hu and Michihiro Yasunaga and Richard L. Phillips and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.07421},
  url={https://arxiv.org/abs/2012.07421}
}

@article{de2021editing,
  title={Editing Factual Knowledge in Language Models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}
@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
}

@misc{duerr2021persuasive,
      title={Persuasive Natural Language Generation -- A Literature Review}, 
      author={Sebastian Duerr and Peter A. Gloor},
      year={2021},
      eprint={2101.05786},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{dai2019transformer,
  title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
  author = "Dai, Zihang  and
  Yang, Zhilin  and
  Yang, Yiming  and
  Carbonell, Jaime  and
  Le, Quoc  and
  Salakhutdinov, Ruslan",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1285",
  doi = "10.18653/v1/P19-1285",
  pages = "2978--2988"
}

@article{logan-cutting-2021,
  author    = {Robert L. Logan IV and
               Ivana Balazevic and
               Eric Wallace and
               Fabio Petroni and
               Sameer Singh and
               Sebastian Riedel},
  title     = {Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with
               Language Models},
  journal   = {CoRR},
  volume    = {abs/2106.13353},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.13353},
  archivePrefix = {arXiv},
  eprint    = {2106.13353},
  timestamp = {Wed, 30 Jun 2021 16:14:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-13353.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hambardzumyan-etal-2021-warp,
    title = "{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming",
    author = "Hambardzumyan, Karen  and
      Khachatrian, Hrant  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.381",
    doi = "10.18653/v1/2021.acl-long.381",
    pages = "4921--4933",
}

@misc{min2021noisy,
      title={Noisy Channel Language Model Prompting for Few-Shot Text Classification}, 
      author={Sewon Min and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2021},
      eprint={2108.04106},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{perez2021true,
  title={True Few-Shot Learning with Language Models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:2105.11447},
  year={2021}
}

@article{gretz2020workweek,
  title={The workweek is the best time to start a family--A Study of GPT-2 Based Claim Generation},
  author={Gretz, Shai and Bilu, Yonatan and Cohen-Karlik, Edo and Slonim, Noam},
  journal={arXiv preprint arXiv:2010.06185},
  year={2020}
}


@article{brown2020language,
  title={{Language Models are Few-Shot Learners}},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{grossman2010technology,
  title={Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review},
  author={Grossman, Maura R and Cormack, Gordon V},
  journal={Rich. JL \& Tech.},
  volume={17},
  pages={1},
  year={2010},
  publisher={HeinOnline}
}


@article{voigt2017language,
  title={Language from police body camera footage shows racial disparities in officer respect},
  author={Voigt, Rob and Camp, Nicholas P and Prabhakaran, Vinodkumar and Hamilton, William L and Hetey, Rebecca C and Griffiths, Camilla M and Jurgens, David and Jurafsky, Dan and Eberhardt, Jennifer L},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={25},
  pages={6521--6526},
  year={2017},
  publisher={National Acad Sciences}
}


@article{katz2017general,
  title={A general approach for predicting the behavior of the Supreme Court of the United States},
  author={Katz, Daniel Martin and Bommarito, Michael J and Blackman, Josh},
  journal={PloS one},
  volume={12},
  number={4},
  pages={e0174698},
  year={2017},
  publisher={Public Library of Science San Francisco, CA USA}
}

@techreport{chohlas2020blind,
  title={Blind justice: Algorithmically masking race in charging decisions},
  author={Chohlas-Wood, Alex and Nudell, Joe and Lin, Zhiyuan Jerry and Nyarko, Julian and Goel, Sharad},
  year={2020},
  institution={Technical report}
}

@inproceedings{shin-etal-2020-autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
}

@article{jiang-etal-2020-know,
    title = "How Can We Know What Language Models Know?",
    author = "Jiang, Zhengbao  and
      Xu, Frank F.  and
      Araki, Jun  and
      Neubig, Graham",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://aclanthology.org/2020.tacl-1.28",
    doi = "10.1162/tacl_a_00324",
    pages = "423--438",
}


@article{betts2017dawn,
  title={The dawn of fully automated contract drafting: Machine learning breathes new life into a decades-old promise},
  author={Betts, Kathryn D and Jaep, Kyle R},
  journal={Duke L. \& Tech. Rev.},
  volume={15},
  pages={216},
  year={2017},
  publisher={HeinOnline}
}


@article{lippi2019claudette,
  title={CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service},
  author={Lippi, Marco and Pa{\l}ka, Przemys{\l}aw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo},
  journal={Artificial Intelligence and Law},
  volume={27},
  number={2},
  pages={117--139},
  year={2019},
  publisher={Springer}
}

@inproceedings{ren2020beta,
  title={Beta embeddings for multi-hop logical reasoning in knowledge graphs},
  author={Ren, Hongyu and Leskovec, Jure},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{ren2020query2box,
  title={Query2box: Reasoning over knowledge graphs in vector space using box embeddings},
  author={Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{boniol2020performance,
  title={Performance in the courtroom: Automated processing and visualization of appeal court decisions in france},
  author={Boniol, Paul and Panagopoulos, George and Xypolopoulos, Christos and Hamdani, Rajaa El and Amariles, David Restrepo and Vazirgiannis, Michalis},
  journal={arXiv preprint arXiv:2006.06251},
  year={2020}
}


@article{medvedeva2020using,
  title={Using machine learning to predict decisions of the European Court of Human Rights},
  author={Medvedeva, Masha and Vols, Michel and Wieling, Martijn},
  journal={Artificial Intelligence and Law},
  volume={28},
  number={2},
  pages={237--266},
  year={2020},
  publisher={Springer}
}

@inproceedings{park2012facilitative,
  title={Facilitative moderation for online participation in eRulemaking},
  author={Park, Joonsuk and Klingel, Sally and Cardie, Claire and Newhart, Mary and Farina, Cynthia and Vallb{\'e}, Joan-Josep},
  booktitle={Proceedings of the 13th Annual International Conference on Digital Government Research},
  pages={173--182},
  year={2012}
}


@book{rhode2004access,
  title={Access to justice},
  author={Rhode, Deborah L},
  year={2004},
  publisher={Oxford University Press}
}


@article{rhode2014access,
  title={Access to justice: A roadmap for reform},
  author={Rhode, Deborah L},
  journal={Fordham Urb. LJ},
  year={2014},
  volume={41},
  pages={1227}
}


@article{zhong2020does,
  title={How does NLP benefit legal system: A summary of legal artificial intelligence},
  author={Zhong, Haoxi and Xiao, Chaojun and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2004.12158},
  year={2020}
}


@article{vold2021using,
  title={Using Transformers to Improve Answer Retrieval for Legal Questions},
  author={Vold, Andrew and Conrad, Jack G},
  year={2021}
}


@article{hendrycks2021cuad,
  title={Cuad: An expert-annotated nlp dataset for legal contract review},
  author={Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
  journal={arXiv preprint arXiv:2103.06268},
  year={2021}
}

@inproceedings{hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}


@misc{bommarito2018lexnlp,
  title={LexNLP: Natural language processing and information extraction for legal and regulatory texts}, 
  author={Michael J Bommarito and Daniel Martin Katz and Eric M Detterman},
  year={2018},
  eprint={1806.03688},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}


@article{savelka2017sentence,
  title={Sentence boundary detection in adjudicatory decisions in the united states},
  author={Savelka, Jaromir and Walker, Vern R and Grabmair, Matthias and Ashley, Kevin D},
  journal={Traitement automatique des langues},
  volume={58},
  pages={21},
  year={2017}
}


@article{ostendorff2021evaluating,
  title={Evaluating Document Representations for Content-based Legal Literature Recommendations},
  author={Ostendorff, Malte and Ash, Elliott and Ruas, Terry and Gipp, Bela and Moreno-Schneider, Julian and Rehm, Georg},
  journal={arXiv preprint arXiv:2104.13841},
  year={2021}
}


@article{huang2021context,
  title={Context-Aware Legal Citation Recommendation using Deep Learning},
  author={Huang, Zihan and Low, Charles and Teng, Mengqiu and Zhang, Hongyi and Ho, Daniel E and Krass, Mark S and Grabmair, Matthias},
  journal={arXiv preprint arXiv:2106.10776},
  year={2021}
}

@article{mora2018bitcoin,
  title={Bitcoin emissions alone could push global warming above 2 C},
  author={Mora, Camilo and Rollins, Randi L and Taladay, Katie and Kantar, Michael B and Chock, Mason K and Shimada, Mio and Franklin, Erik C},
  journal={Nature Climate Change},
  volume={8},
  number={11},
  pages={931--933},
  year={2018},
  publisher={Nature Publishing Group}
}



@inproceedings{zheng2021does,
  title={When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset},
  author={Zheng, Lucia and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Daniel E},
  journal={arXiv preprint arXiv:2104.08671},
  year={2021}
}


@article{bell2021recon,
  title={The Recon Approach: A New Direction for Machine Learning in Criminal Law},
  author={Bell, Kristen and Hong, Jenny and McKeown, Nick and Voss, Catalin},
  journal={Berkeley Technology Law Journal},
  volume={37},
  year={2021}
}


@misc{aba2021lawyer,
    title={National Lawyer Population Survey 2021},
    author={{American Bar Association}},
    year={2021},
    url={https://www.americanbar.org/content/dam/aba/administrative/market_research/2021-national-lawyer-population-survey.pdf}
}

@misc{marketline2020legalservices,
    title={Legal Services in the United States},
    author={{MarketLine}},
    month={11},
    year={2021},
    url={https://www.marketresearch.com/MarketLine-v3883/Legal-Services-United-States-14193556/}
}


@techreport{lsc2017justice,
    title={The Justice Gap: Measuring the Unmet Civil Legal Needs of Low-income Americans},
    author={{Legal Services Corporation}},
    year={2017},
    institution={Prepared by NORC at the University of Chicago for Legal Services Corporation},
    address={Washington, DC}
}

@article{leivaditi2020benchmark,
  title={A Benchmark for Lease Contract Review},
  author={Leivaditi, Spyretta and Rossi, Julien and Kanoulas, Evangelos},
  journal={arXiv preprint arXiv:2010.10386},
  year={2020}
}


@techreport{nrcc2009justice,
    title={Justice Denied: America's Continuing Neglect of Our Constitutional Right to Counsel},
    author={Lefstein, Norman and Spagenberg, Robert L},
    month={4},
    year={2009},
    institution={National Right to Counsel Committee, The Constitution Project, National Legal Aid \& Defender Association}
}

@article{cabral2012using,
  title={Using technology to enhance access to justice},
  author={Cabral, James E and Chavan, Abhijeet and Clarke, Thomas M and Greacen, John},
  journal={Harv. JL \& Tech.},
  year={2012},
  volume={26},
  pages={241}
}


@techreport{nacdl2012national,
    title={National Indigent Defense Reform: The Solution is Multifaceted},
    author={Schumm, Joel M.},
    year={2012},
    institution={National Association of Criminal Defense Lawyers, American Bar Association}
}


@techreport{aba2004gideon,
    title={Gideon's Broken Promise: America's Continuing Quest for Equal Justice},
    author={{American Bar Association}},
    month={12},
    year={2004},
    institution={American Bar Association}
}


@techreport{doj2010county,
    title={County-based and Local Public Defender
    Offices, 2007},
    author={Farole, Jr., Donald J and Langston, Lynn},
    month={9},
    year={2010},
    institution={U.S. Department of Justice Bureau of Justice Statistics}
}


@techreport{doj2010state,
    title={State Public Defender Programs, 2007},
    author={Langston, Lynn and Farole, Jr., Donald J},
    month={9},
    year={2010},
    institution={U.S. Department of Justice Bureau of Justice Statistics}
}


@article{carter1978speech,
    title={Excerpts From Carter's Speech to the Bar Association},
    author={Carter, Jimmy},
    journal={The New York Times},
    month={May},
    year={1978}
}


@article{engstrom2020government,
  title={Government by algorithm: Artificial intelligence in federal administrative agencies},
  author={Engstrom, David Freeman and Ho, Daniel E and Sharkey, Catherine M and Cu{\'e}llar, Mariano-Florentino},
  journal={NYU School of Law, Public Law Research Paper},
  number={20-54},
  year={2020}
}


@article{berk2021justice,
  author = {Richard Berk and Hoda Heidari and Shahin Jabbari and Michael Kearns and Aaron Roth},
  title ={Fairness in Criminal Justice Risk Assessments: The State of the Art},
  journal = {Sociological Methods \& Research},
  volume = {50},
  number = {1},
  pages = {3-44},
  year = {2021},
  doi = {10.1177/0049124118782533}
}

@article{batterbury2012,
author = {Batterbury, Sarah},
year = {2012},
month = {08},
pages = {},
title = {Language justice for Sign Language Peoples: The UN Convention on the Rights of Persons with Disabilities},
volume = {11},
journal = {Language Policy},
doi = {10.1007/s10993-012-9245-8}
}

@book{allport1954,
  title={The Nature of Prejudice},
  author={Allport, Gordon W.},
  isbn={9780201001754},
  lccn={54005626},
  url={https://books.google.com/books?id=u94XUyRuDl4C},
  year={1954},
  publisher={Addison-Wesley Publishing Company}
}


@article {crenshaw1989,
  title={Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics},
  author={Crenshaw, Kimberl{\'e}},
  journal={University of Chicago Legal Forum},
  volume={Vol.1989, Article 8},
  year={1989},
  url={https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1052&context=uclf}
}

@article{hyde2019,
  title={The Future of Sex and Gender in Psychology: Five Challenges to the Gender Binary},
  author={Janet Shibley Hyde and Rebecca S. Bigler and Daphna Joel and Charlotte Chucky Tate and Sari M. van Anders},
  journal={American Psychologist},
  year={2019},
  volume={74},
  pages={171–193}
}

@article {penner2008,
	author = {Penner, Andrew M. and Saperstein, Aliya},
	title = {How social status shapes race},
	volume = {105},
	number = {50},
	pages = {19628--19630},
	year = {2008},
	doi = {10.1073/pnas.0805762105},
	publisher = {National Academy of Sciences},
	abstract = {We show that racial perceptions are fluid; how individuals perceive their own race and how they are perceived by others depends in part on their social position. Using longitudinal data from a representative sample of Americans, we find that individuals who are unemployed, incarcerated, or impoverished are more likely to be seen and identify as black and less likely to be seen and identify as white, regardless of how they were classified or identified previously. This is consistent with the view that race is not a fixed individual attribute, but rather a changeable marker of status.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/105/50/19628},
	eprint = {https://www.pnas.org/content/105/50/19628.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{freeman2011,
    doi = {10.1371/journal.pone.0025107},
    author = {Freeman, Jonathan B. AND Penner, Andrew M. AND Saperstein, Aliya AND Scheutz, Matthias AND Ambady, Nalini},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Looking the Part: Social Status Cues Shape Race Perception},
    year = {2011},
    month = {09},
    volume = {6},
    url = {https://doi.org/10.1371/journal.pone.0025107},
    pages = {1-10},
    abstract = {It is commonly believed that race is perceived through another's facial features, such as skin color. In the present research, we demonstrate that cues to social status that often surround a face systematically change the perception of its race. Participants categorized the race of faces that varied along White–Black morph continua and that were presented with high-status or low-status attire. Low-status attire increased the likelihood of categorization as Black, whereas high-status attire increased the likelihood of categorization as White; and this influence grew stronger as race became more ambiguous (Experiment 1). When faces with high-status attire were categorized as Black or faces with low-status attire were categorized as White, participants' hand movements nevertheless revealed a simultaneous attraction to select the other race-category response (stereotypically tied to the status cue) before arriving at a final categorization. Further, this attraction effect grew as race became more ambiguous (Experiment 2). Computational simulations then demonstrated that these effects may be accounted for by a neurally plausible person categorization system, in which contextual cues come to trigger stereotypes that in turn influence race perception. Together, the findings show how stereotypes interact with physical cues to shape person categorization, and suggest that social and contextual factors guide the perception of race.},
    number = {9},

}

@article{saperstein2012,
author = {Saperstein, Aliya and Penner, Andrew M.},
title = {Racial Fluidity and Inequality in the United States},
journal = {American Journal of Sociology},
volume = {118},
number = {3},
pages = {676-727},
year = {2012},
doi = {10.1086/667722},

URL = { 
        https://doi.org/10.1086/667722
    
},
eprint = { 
        https://doi.org/10.1086/667722
    
}
,
    abstract = { The authors link the literature on racial fluidity and inequality in the United States and offer new evidence of the reciprocal relationship between the two processes. Using two decades of longitudinal data from a national survey, they demonstrate that not only does an individual’s race change over time, it changes in response to myriad changes in social position, and the patterns are similar for both self-identification and classification by others. These findings suggest that, in the contemporary United States, microlevel racial fluidity serves to reinforce existing disparities by redefining successful or high-status people as white (or not black) and unsuccessful or low-status people as black (or not white). Thus, racial differences are both an input and an output in stratification processes; this relationship has implications for theorizing and measuring race in research, as well as for crafting policies that attempt to address racialized inequality. }
}

@article{ghavami2013,
author = {Negin Ghavami and Letitia Anne Peplau},
title ={An Intersectional Analysis of Gender and Ethnic Stereotypes: Testing Three Hypotheses},
journal = {Psychology of Women Quarterly},
volume = {37},
number = {1},
pages = {113-127},
year = {2013},
doi = {10.1177/0361684312464203},

URL = { 
        https://doi.org/10.1177/0361684312464203
    
},
eprint = { 
        https://doi.org/10.1177/0361684312464203
    
}
,
    abstract = { We compared perceived cultural stereotypes of diverse groups varying by gender and ethnicity. Using a free-response procedure, we asked 627 U.S. undergraduates to generate 10 attributes for 1 of 17 groups: Asian Americans, Blacks, Latinos, Middle Eastern Americans, or Whites; men or women; or 10 gender-by-ethnic groups (e.g., Black men or Latina women). Based on intersectionality theory and social dominance theory, we developed and tested three hypotheses. First, consistent with the intersectionality hypothesis, gender-by-ethnic stereotypes contained unique elements that were not the result of adding gender stereotypes to ethnic stereotypes. Second, in support of an ethnicity hypothesis, stereotypes of ethnic groups were generally more similar to stereotypes of the men than of the women in each group. Third, a gender hypothesis postulated that stereotypes of men and women will be most similar to stereotypes of White men and White women, less similar to ethnic minority men and ethnic minority women, and least similar to Black men and Black women. This hypothesis was confirmed for target women, but results for target men were mixed. Collectively, our results contribute to research, theory, and practice by demonstrating that ethnic and gender stereotypes are complex and that the intersections of these social categories produce meaningful differences in the way groups are perceived. }
}

@article{penner2013,
author = {Andrew M. Penner and Aliya Saperstein},
title ={Engendering Racial Perceptions: An Intersectional Analysis of How Social Status Shapes Race},
journal = {Gender \& Society},
volume = {27},
number = {3},
pages = {319-344},
year = {2013},
doi = {10.1177/0891243213480262},

URL = { 
        https://doi.org/10.1177/0891243213480262
    
},
eprint = { 
        https://doi.org/10.1177/0891243213480262
    
}
,
    abstract = { Intersectionality emphasizes that race, class, and gender distinctions are inextricably intertwined, but fully interrogating the co-constitution of these axes of stratification has proven difficult to implement in large-scale quantitative analyses. We address this gap by exploring gender differences in how social status shapes race in the United States. Building on previous research showing that changes in the racial classifications of others are influenced by social status, we use longitudinal data to examine how differences in social class position might affect racial classification differently for women and men. In doing so, we provide further support for the claim that race, class, and gender are not independent axes of stratification; rather they intersect, creating dynamic feedback loops that maintain the complex structure of social inequality in the United States. }
}

@article{saperstein2013,
author = {Saperstein, Aliya and Penner, Andrew M. and Light, Ryan},
title = {Racial Formation in Perspective: Connecting Individuals, Institutions, and Power Relations},
journal = {Annual Review of Sociology},
volume = {39},
number = {1},
pages = {359-378},
year = {2013},
doi = {10.1146/annurev-soc-071312-145639},

URL = { 
        https://doi.org/10.1146/annurev-soc-071312-145639
    
},
eprint = { 
        https://doi.org/10.1146/annurev-soc-071312-145639
    
}
,
    abstract = { Over the past 25 years, since the publication of Omi \& Winant's Racial Formation in the United States, the statement that race is socially constructed has become a truism in sociological circles. Yet many struggle to describe exactly what the claim means. This review brings together empirical literature on the social construction of race from different levels of analysis to highlight the variety of approaches to studying racial formation processes. For example, macro-level scholarship often focuses on the creation of racial categories, micro-level studies examine who comes to occupy these categories, and meso-level research captures the effects of institutional and social context. Each of these levels of analysis has yielded important contributions to our understanding of the social construction of race, yet there is little conversation across boundaries. Scholarship that bridges methodological and disciplinary divides is needed to continue to advance the racial formation perspective and demonstrate its broader relevance. }
}

@article{westbrook2015,
author = {Laurel Westbrook and Aliya Saperstein},
title ={New Categories Are Not Enough: Rethinking the Measurement of Sex and Gender in Social Surveys},
journal = {Gender \& Society},
volume = {29},
number = {4},
pages = {534-560},
year = {2015},
doi = {10.1177/0891243215584758},

URL = { 
        https://doi.org/10.1177/0891243215584758
    
},
eprint = { 
        https://doi.org/10.1177/0891243215584758
    
}
,
    abstract = { Recently, scholars and activists have turned their attention toward improving the measurement of sex and gender in survey research. The focus of this effort has been on including answer options beyond “male” and “female” to questions about the respondent’s gender. This is an important step toward both reflecting the diversity of gendered lives and better aligning survey measurement practice with contemporary gender theory. However, our systematic examination of questionnaires, manuals, and other technical materials from four of the largest and longest-running surveys in the United States indicates that there are a number of other issues with how gender is conceptualized and measured in social surveys that also deserve attention, including essentialist practices that treat sex and gender as synonymous, easily determined by others, obvious, and unchanging over the life course. We find that these understandings extend well beyond direct questions about the respondent’s gender, permeating the surveys. A hyper-gendered world of “males” and “females,” “brothers” and “sisters,” and “husbands” and “wives” shapes what we can see in survey data. If not altered, surveys will continue to reproduce statistical representations that erase important dimensions of variation and likely limit understanding of the processes that perpetuate social inequality. }
}

@article{penner2015,
  title={Disentangling the effects of racial self-identification and classification by others: {T}he case of arrest},
  author={Penner, Andrew M. and Saperstein, Aliya},
  journal={Demography},
  volume={52},
  number={3},
  pages={1017--1024},
  year={2015},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s13524-015-0394-1}
}

@inproceedings{may2019,
    title = "On Measuring Social Biases in Sentence Encoders",
    author = "May, Chandler  and
      Wang, Alex  and
      Bordia, Shikha  and
      Bowman, Samuel R.  and
      Rudinger, Rachel",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1063",
    doi = "10.18653/v1/N19-1063",
    pages = "622--628",
    abstract = "The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test{'}s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.",
}

@inproceedings{dinan2020,
    title = "Multi-Dimensional Gender Bias Classification",
    author = "Dinan, Emily  and
      Fan, Angela  and
      Wu, Ledell  and
      Weston, Jason  and
      Kiela, Douwe  and
      Williams, Adina",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.23",
    doi = "10.18653/v1/2020.emnlp-main.23",
    pages = "314--331",
    abstract = "Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",
}

@inproceedings{cao2020,
    title = "Toward Gender-Inclusive Coreference Resolution",
    author = "Cao, Yang Trista  and
      Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.418",
    doi = "10.18653/v1/2020.acl-main.418",
    pages = "4568--4595",
    abstract = "Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.",
}

@book{lindsey2015,
  title={The sociology of Gender Theoretical Perspectives and Feminist Frameworks},
  author={Linda L. Lindsey},
  series={Gender Roles},
  url={https://www.routledge.com/Gender-Sociological-Perspectives/Lindsey/p/book/9781138103696},
  year={2015},
  publisher={Routledge}
}

@book{richards2017,
  title={Genderqueer and Non-Binary Genders},
  author={Richards, C. and Bouman, W.P. and Barker, M.J.},
  isbn={9781137510532},
  lccn={2017949533},
  series={Critical and Applied Approaches in Sexuality, Gender and Identity},
  url={https://books.google.com/books?id=qFJDDwAAQBAJ},
  year={2017},
  publisher={Palgrave Macmillan UK}
}

@article{darwin2017,
author = {Darwin, Helana},
title = {Doing Gender Beyond the Binary: A Virtual Ethnography},
journal = {Symbolic Interaction},
volume = {40},
number = {3},
pages = {317-334},
keywords = {Reddit, virtual ethnography, genderqueer, transgender, nonbinary},
doi = {https://doi.org/10.1002/symb.316},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/symb.316},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/symb.316},
abstract = {This article advances the “doing gender” framework by highlighting some unique interactive challenges that nonbinary individuals encounter within the binary gender system. In order to access testimony about these experiences from a large group of people, this study turns to a genderqueer community on the social media site Reddit. Discourse analysis of discussion threads and content analysis of selfies reveal various symbolic mechanisms through which nonbinary people do, redo, and undo gender. These findings illuminate a range of strategies that people utilize to negotiate gender attribution within the gender binary system. A video abstract is available at http://tinyurl.com/y7odrxbd.},
year = {2017}
}

@inproceedings{guo2020,
author = {Guo, Wei and Caliskan, Aylin},
title = {Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462536},
doi = {10.1145/3461702.3462536},
abstract = {With the starting point that implicit human biases are reflected in the statistical
regularities of language, it is possible to measure biases in English static word
embeddings. State-of-the-art neural language models generate dynamic word embeddings
dependent on the context in which the word appears. Current methods measure pre-defined
social and intersectional biases that occur in contexts defined by sentence templates.
Dispensing with templates, we introduce the Contextualized Embedding Association Test
(CEAT), that can summarize the magnitude of overall bias in neural language models
by incorporating a random-effects model. Experiments on social and intersectional
biases show that CEAT finds evidence of all tested biases and provides comprehensive
information on the variance of effect magnitudes of the same bias in different contexts.
All the models trained on English corpora that we study contain biased representations.
GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then
ELMo, negatively correlating with the contextualization levels of the models.Furthermore,
we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional
Bias Detection (EIBD), to automatically identify the intersectional biases and emergent
intersectional biases from static word embeddings in addition to measuring them in
contextualized word embeddings. We present the first algorithmic bias detection findings
on how intersectional group members are strongly associated with unique emergent biases
that do not overlap with the biases of their constituent minority identities. IBD
achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional
biases of African American females and Mexican American females, where the random
correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7%
and 65.3%, respectively, when detecting the emergent intersectional biases unique
to African American females and Mexican American females, where the random correct
identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases
associated with members of multiple minority groups, such as African American females
and Mexican American females, have the highest magnitude across all neural language
models.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {122–133},
numpages = {12},
keywords = {AI ethics, intersectionality, bias, social psychology, word embeddings, language models},
location = {Virtual Event, USA},
series = {AIES '21}
}


@misc{laufer2020feedback,
  title={Feedback Effects in Repeat-Use Criminal Risk Assessments}, 
  author={Benjamin Laufer},
  year={2020},
  eprint={2011.14075},
  archivePrefix={arXiv},
  primaryClass={cs.CY}
}


@article{coglianese2020ai,
  title={AI in Adjudication and Administration},
  author={Coglianese, Cary and Ben Dor, Lavi},
  journal={Brooklyn Law Review, Forthcoming, University of Pennsylvania Law School, Public Law Research Paper},
  number={19-41},
  year={2020}
}

@article{cuellar_2019, 
  title={How to ensure equal access to the law when we speak 200 different languages}, url={https://law.stanford.edu/2019/02/05/how-to-ensure-equal-access-to-the-law-when-we-speak-200-different-languages/},
  journal={CalMatters},
  author={Cuéllar, Mariano-Florentino}, year={2019},
  month={Feb}
}


@inproceedings{li-etal-2020-exploring,
    title = "Exploring the Role of Argument Structure in Online Debate Persuasion",
    author = "Li, Jialu  and
      Durmus, Esin  and
      Cardie, Claire",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.716",
    doi = "10.18653/v1/2020.emnlp-main.716",
    pages = "8905--8912",
    abstract = "Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extracted from the debate text and features encoding the characteristics of the audience are both critical in persuasion studies. In this paper, we aim to further investigate the role of discourse structure of the arguments from online debates in their persuasiveness. In particular, we use the factor graph model to obtain features for the argument structure of debates from an online debating platform and incorporate these features to an LSTM-based model to predict the debater that makes the most convincing arguments. We find that incorporating argument structure features play an essential role in achieving the best predictive performance in assessing the persuasiveness of the arguments on online debates.",
}

@article{liu2020understanding,
  title={Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning},
  author={Liu, Xuebo and Wang, Longyue and Wong, Derek F and Ding, Liang and Chao, Lidia S and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2012.14768},
  year={2020}
}

@article{attention_bottlenecks,
  title={Attention Bottlenecks for Multimodal Fusion},
  author={Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  journal={arXiv preprint arXiv:2107.00135},
  year={2021}
}

@article{kuditipudi2020explaining,
  title={Explaining landscape connectivity of low-cost solutions for multilayer nets},
  author={Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Arora, Sanjeev and Ge, Rong},
  journal={arXiv preprint arXiv:1906.06247},
  year={2019}
}

@inproceedings{helfrich2018orthogonal,
  title={Orthogonal recurrent neural networks with scaled Cayley transform},
  author={Helfrich, Kyle and Willmott, Devin and Ye, Qiang},
  booktitle={International Conference on Machine Learning},
  pages={1969--1978},
  year={2018},
  organization={PMLR}
}

@article{catastroph,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}
}

@inproceedings{fewshot,
  title={Learning to compare: Relation network for few-shot learning},
  author={Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip HS and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1199--1208},
  year={2018}
}

@article{2009_06489,
  title={The hardware lottery},
  author={Hooker, Sara},
  journal={arXiv preprint arXiv:2009.06489},
  year={2020}
}

@article{longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{bigbird,
  title={Big Bird: Transformers for Longer Sequences.},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{dosovitskiy2021ima,
  title={Efficientnetv2: Smaller models and faster training},
  author={Tan, Mingxing and Le, Quoc V},
  journal={arXiv preprint arXiv:2104.00298},
  year={2021}
}

@article{approx,
  title={Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1910.03193},
  year={2019}
}

@article{moe,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@misc{tolstikhin2021mlpmixer,
      title={MLP-Mixer: An all-MLP Architecture for Vision}, 
      author={Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
      year={2021},
      eprint={2105.01601},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{longpre-etal-2019-persuasion,
    title = "Persuasion of the Undecided: Language vs. the Listener",
    author = "Longpre, Liane  and
      Durmus, Esin  and
      Cardie, Claire",
    booktitle = "Proceedings of the 6th Workshop on Argument Mining",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4519",
    doi = "10.18653/v1/W19-4519",
    pages = "167--176",
    abstract = "This paper examines the factors that govern persuasion for a priori UNDECIDED versus DECIDED audience members in the context of on-line debates. We separately study two types of influences: linguistic factors {---} features of the language of the debate itself; and audience factors {---} features of an audience member encoding demographic information, prior beliefs, and debate platform behavior. In a study of users of a popular debate platform, we find first that different combinations of linguistic features are critical for predicting persuasion outcomes for UNDECIDED versus DECIDED members of the audience. We additionally find that audience factors have more influence on predicting the side (PRO/CON) that persuaded UNDECIDED users than for DECIDED users that flip their stance to the opposing side. Our results emphasize the importance of considering the undecided and decided audiences separately when studying linguistic factors of persuasion.",
}


@article{paullada2020,
  title={Data and its (dis)contents: A survey of dataset development and use in machine learning research},
  author={Amandalynne Paullada and Inioluwa Deborah Raji and Emily M. Bender and Emily L. Denton and Alex Hanna},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.05345},
  url={https://arxiv.org/abs/2012.05345}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}


@inproceedings{bender2021,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{creel2021,
author = {Creel, Kathleen and Hellman, Deborah},
title = {The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision Making Systems},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445942},
doi = {10.1145/3442188.3445942},
abstract = {Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are "fair" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {816},
numpages = {1},
keywords = {arbitrariness, algorithmic decision making, opportunity, fairness, machine learning, automated hiring},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@book{Gandy2021,
    title = {The {Panoptic} {Sort}: A {Political} {Economy} of {Personal} {Information}},
    author = {Gandy, Jr., Oscar H.},
    year = {2021},
    month = {06},
    day = {24},
    publisher = {Oxford University Press},
    edition = {2},
    pagecount = {352},
    isbn = {9780197579428}
}

@article{abid2021,
  title={Persistent Anti-Muslim Bias in Large Language Models},
  author={Abubakar Abid and M. Farooqi and J. Zou},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.05783},
  url={https://arxiv.org/abs/2101.05783}
}

@article{caswell2021,
  title={Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets},
  author={Isaac Caswell and Julia Kreutzer and Lisa Wang and Ahsan Wahab and Daan van Esch and Nasanbayar Ulzii-Orshikh and Allahsera Tapo and Nishant Subramani and Artem Sokolov and Claytone Sikasote and Monang Setyawan and Supheakmungkol Sarin and Sokhar Samb and Benoît Sagot and Clara Rivera and Annette Rios and Isabel Papadimitriou and Salomey Osei and Pedro Javier Ortiz Suárez and Iroro Orife and Kelechi Ogueji and Rubungo Andre Niyongabo and Toan Q. Nguyen and Mathias Müller and André Müller and Shamsuddeen Hassan Muhammad and Nanda Muhammad and Ayanda Mnyakeni and Jamshidbek Mirzakhalov and Tapiwanashe Matangira and Colin Leong and Nze Lawson and Sneha Kudugunta and Yacine Jernite and Mathias Jenny and Orhan Firat and Bonaventure F. P. Dossou and Sakhile Dlamini and Nisansa de Silva and Sakine Çabuk Ballı and Stella Biderman and Alessia Battisti and Ahmed Baruwa and Ankur Bapna and Pallavi Baljekar and Israel Abebe Azime and Ayodele Awokoya and Duygu Ataman and Orevaoghene Ahia and Oghenefego Ahia and Sweta Agrawal and Mofetoluwa Adeyemi},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.12028},
  url={https://arxiv.org/abs/2103.12028}
}

@inproceedings{bowman2021,
    title = "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    author = "Bowman, Samuel R.  and
      Dahl, George",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.385",
    doi = "10.18653/v1/2021.naacl-main.385",
    pages = "4843--4855",
    abstract = "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",
}

@inproceedings{futrell2019,
    title = "Neural language models as psycholinguistic subjects: Representations of syntactic state",
    author = "Futrell, Richard  and
      Wilcox, Ethan  and
      Morita, Takashi  and
      Qian, Peng  and
      Ballesteros, Miguel  and
      Levy, Roger",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1004",
    doi = "10.18653/v1/N19-1004",
    pages = "32--42",
    abstract = "We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.",
}

@article{dehgani2021,
  title={The Benchmark Lottery},
  author={Mostafa Dehghani and Yi Tay and Alexey Gritsenko and Zhe Zhao and Neil Houlsby and Fernando Diaz and Donald Metzler and Oriol Vinyals},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.07002}
}

@inproceedings{kiela2021,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.324",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
}

@inproceedings{bommasani2020,
    title = "{I}nterpreting {P}retrained {C}ontextualized {R}epresentations via {R}eductions to {S}tatic {E}mbeddings",
    author = "Bommasani, Rishi  and
      Davis, Kelly  and
      Cardie, Claire",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.431",
    doi = "10.18653/v1/2020.acl-main.431",
    pages = "4758--4781",
    abstract = "Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings {---} while more diverse and mature than those available for their dynamic counterparts {---} are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",
}

@inproceedings{ethayarajh2019,
    title = "Understanding Undesirable Word Embedding Associations",
    author = "Ethayarajh, Kawin  and
      Duvenaud, David  and
      Hirst, Graeme",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1166",
    doi = "10.18653/v1/P19-1166",
    pages = "1696--1705",
    abstract = "Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.",
}

@article{lechner2021,
  title={Impossibility results for fair representations},
  author={Tosca Lechner and Shai Ben-David and Sushant Agarwal and Nivasini Ananthakrishnan},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03483}
}


@book{galliers1993,
  title={Evaluating Natural Language Processing Systems},
  author={Galliers, Julia and Sp\"arck Jones, Karen},
  series={Computer Laboratory Cambridge: Technical report},
  url={https://books.google.com/books?id=ZxklAQAAIAAJ},
  year={1993},
  publisher={University of Cambridge, Computer Laboratory}
}


@article{lipton2019,
author = {Lipton, Zachary C. and Steinhardt, Jacob},
title = {Troubling Trends in Machine Learning Scholarship: Some ML Papers Suffer from Flaws That Could Mislead the Public and Stymie Future Research.},
year = {2019},
issue_date = {January-February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1542-7730},
url = {https://doi.org/10.1145/3317287.3328534},
doi = {10.1145/3317287.3328534},
abstract = {Flawed scholarship threatens to mislead the public and stymie future research by compromising
ML’s intellectual foundations. Indeed, many of these problems have recurred cyclically
throughout the history of AI and, more broadly, in scientific research. In 1976, Drew
McDermott chastised the AI community for abandoning self-discipline, warning prophetically
that "if we can’t criticize ourselves, someone else will save us the trouble." The
current strength of machine learning owes to a large body of rigorous research to
date, both theoretical and empirical. By promoting clear scientific thinking and communication,
our community can sustain the trust and investment it currently enjoys.},
journal = {Queue},
month = feb,
pages = {45–77},
numpages = {33}
}

@inproceedings{ribeiro2020,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.442",
    doi = "10.18653/v1/2020.acl-main.442",
    pages = "4902--4912",
    abstract = "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
}

@article{nissim2020,
    title = "Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor",
    author = "Nissim, Malvina  and
      van Noord, Rik  and
      van der Goot, Rob",
    journal = "Computational Linguistics",
    volume = "46",
    number = "2",
    month = jun,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-2.7",
    doi = "10.1162/coli_a_00379",
    pages = "487--497",
    abstract = "Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.",
}

@inproceedings{raji2019,
author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314244},
doi = {10.1145/3306618.3314244},
abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases
embedded in software platforms, we struggle to understand the real-world impact of
these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic
fairness and transparency in commercial systems is nascent. To analyze the impact
of publicly naming and disclosing performance results of biased AI systems, we investigate
the commercial impact of Gender Shades, the first algorithmic audit of gender and
skin type performance disparities in commercial facial analysis models. This paper
1) outlines the audit design and structured disclosure procedure used in the Gender
Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft
and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3)
provides performance results on PPB by non-target companies Amazon and Kairos and,
4) explores differences in company responses as shared through corporate communications
that contextualize differences in performance on PPB. Within 7 months of the original
audit, we find that all three targets released new API versions. All targets reduced
accuracy disparities between males and females and darker and lighter-skinned subgroups,
with the most significant update occurring for the darker-skinned female subgroup,
that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing
these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments
Benchmark (PPB) for target corporation APIs. The overall performance of non-targets
Amazon and Kairos lags significantly behind that of the targets, with error rates
of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female
subgroup, respectively.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {429–435},
numpages = {7},
keywords = {computer vision, artificial intelligence, fairness, commercial applications, machine learning, ethics, facial recognition},
location = {Honolulu, HI, USA},
series = {AIES '19}
}


@inproceedings{goldfarb-tarrant2021,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940",
    abstract = "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.",
}

@inproceedings{blodgett2021,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@inproceedings{field2021,
    title = "A Survey of Race, Racism, and Anti-Racism in {NLP}",
    author = "Field, Anjalie  and
      Blodgett, Su Lin  and
      Waseem, Zeerak  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.149",
    doi = "10.18653/v1/2021.acl-long.149",
    pages = "1905--1925",
    abstract = "Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",
}


@article{liu2021prompt,
  author    = {Xiao Liu and
               Yanan Zheng and
               Zhengxiao Du and
               Ming Ding and
               Yujie Qian and
               Zhilin Yang and
               Jie Tang},
  title     = {{GPT} Understands, Too},
  journal   = {CoRR},
  volume    = {abs/2103.10385},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.10385},
  archivePrefix = {arXiv},
  eprint    = {2103.10385},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-10385.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2021,
  title={Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches},
  author={Nelson F. Liu and Tony Lee and Robin Jia and Percy Liang},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.01065},
  url={https://arxiv.org/abs/2102.01065}
}

@article{zhou2021,
  title={Frequency-based Distortions in Contextualized Word Embeddings},
  author={Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08465},
  url={https://arxiv.org/abs/2104.08465}
}

@article{rogers2021,
  title={Changing the World by Changing the Data},
  author={Anna Rogers},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.13947},
  url={https://arxiv.org/abs/2105.13947}
}

@article{ma2021,
  title={Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking},
  author={Zhiyi Ma and Kawin Ethayarajh and Tristan Thrush and Somya Jain and Ledell Wu and Robin Jia and Christopher Potts and Adina Williams and Douwe Kiela},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.06052},
  url={https://arxiv.org/abs/2106.06052}
}

@article{kleinberg2021,
	author = {Kleinberg, Jon and Raghavan, Manish},
	title = {Algorithmic monoculture and social welfare},
	volume = {118},
	number = {22},
	elocation-id = {e2018340118},
	year = {2021},
	doi = {10.1073/pnas.2018340118},
	publisher = {National Academy of Sciences},
	abstract = {Algorithmic monoculture is a growing concern in the use of algorithms for high-stakes screening decisions in areas such as employment and lending. If many firms use the same algorithm, even if it is more accurate than the alternatives, the resulting {\textquotedblleft}monoculture{\textquotedblright} may be susceptible to correlated failures, much as a monocultural system is in biological settings. To investigate this concern, we develop a model of selection under monoculture. We find that even without any assumption of shocks or correlated failures{\textemdash}i.e., under {\textquotedblleft}normal operations{\textquotedblright}{\textemdash}the quality of decisions may decrease when multiple firms use the same algorithm. Thus, the introduction of a more accurate algorithm may decrease social welfare{\textemdash}a kind of {\textquotedblleft}Braess{\textquoteright} paradox{\textquotedblright} for algorithmic decision-making.As algorithms are increasingly applied to screen applicants for high-stakes decisions in employment, lending, and other domains, concerns have been raised about the effects of algorithmic monoculture, in which many decision-makers all rely on the same algorithm. This concern invokes analogies to agriculture, where a monocultural system runs the risk of severe harm from unexpected shocks. Here, we show that the dangers of algorithmic monoculture run much deeper, in that monocultural convergence on a single algorithm by a group of decision-making agents, even when the algorithm is more accurate for any one agent in isolation, can reduce the overall quality of the decisions being made by the full collection of agents. Unexpected shocks are therefore not needed to expose the risks of monoculture; it can hurt accuracy even under {\textquotedblleft}normal{\textquotedblright} operations and even for algorithms that are more accurate when used by only a single decision-maker. Our results rely on minimal assumptions and involve the development of a probabilistic framework for analyzing systems that use multiple noisy estimates of a set of alternatives.There are no data underlying this work.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/118/22/e2018340118},
	eprint = {https://www.pnas.org/content/118/22/e2018340118.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@inproceedings{shokri2017membership,
	author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	booktitle = {IEEE Symposium on Security and Privacy},
	date-added = {2021-05-31 13:30:52 -0700},
	date-modified = {2021-06-22 14:55:27 +0200},
	pages = {3--18},
	title = {Membership inference attacks against machine learning models},
	year = {2017}
}

@inproceedings{fredrikson2015model,
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	booktitle = {{ACM} {SIGSAC} Conference on Computer and Communications Security},
	date-added = {2021-05-31 13:30:37 -0700},
	date-modified = {2021-06-22 15:00:49 +0200},
	title = {Model inversion attacks that exploit confidence information and basic countermeasures},
	year = {2015}
}

@inproceedings{carlini2019secret,
	author = {Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
	booktitle = {USENIX Security Symposium},
	date-added = {2021-05-31 13:30:52 -0700},
	date-modified = {2021-05-31 14:22:42 -0700},
	pages = {267--284},
	title = {The secret sharer: Evaluating and testing unintended memorization in neural networks},
	year = {2019}
}

@inproceedings{carlini2020extracting,
	author = {Carlini, Nicholas and Tram{\`e}r, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, {\'U}lfar and Oprea, Alina and Raffel, Colin},
	booktitle = {USENIX Security Symposium},
	date-added = {2021-06-01 11:07:38 -0700},
	date-modified = {2021-06-22 16:17:24 +0200},
	title = {Extracting Training Data from Large Language Models},
	year = {2021}
}

@article{shumailov2020sponge,
  title={Sponge examples: Energy-latency attacks on neural networks},
  author={Shumailov, Ilia and Zhao, Yiren and Bates, Daniel and Papernot, Nicolas and Mullins, Robert and Anderson, Ross},
  journal={arXiv preprint arXiv:2006.03463},
  year={2020}
}


@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@article{carlini2021poisoning,
  title={Poisoning and Backdooring Contrastive Learning},
  author={Carlini, Nicholas and Terzis, Andreas},
  journal={arXiv preprint arXiv:2106.09667},
  year={2021}
}

@inproceedings{schuster2021you,
  title={You autocomplete me: Poisoning vulnerabilities in neural code completion},
  author={Schuster, Roei and Song, Congzheng and Tromer, Eran and Shmatikov, Vitaly},
  booktitle={30th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 21)},
  year={2021}
}

@article{song2019overlearning,
  title={Overlearning reveals sensitive attributes},
  author={Song, Congzheng and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:1905.11742},
  year={2019}
}

@article{elsayed2018adversarial,
  title={Adversarial reprogramming of neural networks},
  author={Elsayed, Gamaleldin F and Goodfellow, Ian and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1806.11146},
  year={2018}
}


@inproceedings{mcmahan2017learning,
	author = {McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
	booktitle = {International Conference on Learning Representations},
	title = {Learning differentially private recurrent language models},
	year = {2018}}


@inproceedings{tramer2021dp,
	author = {Tram{\`e}r, Florian and Boneh, Dan},
	booktitle = {International Conference on Learning Representations},
	date-added = {2021-06-07 15:24:15 -0700},
	date-modified = {2021-06-07 15:24:35 -0700},
	title = {Differentially Private Learning Needs Better Features (or Much More Data)},
	year = {2021},
	Bdsk-Url-1 = {https://arxiv.org/abs/2011.11660}}

@article{basu2021benchmarking,
      title={Benchmarking Differential Privacy and Federated Learning for BERT Models}, 
      author={Priyam Basu and Tiasa Singha Roy and Rakshit Naidu and Zumrut Muftuoglu and Sahib Singh and Fatemehsadat Mireshghallah},
      year={2021},
      journal={arXiv preprint arXiv:2106.13973}
}
@inproceedings{bommasani19towards,
    title={Towards Private Synthetic Text Generation},
    author={Rishi Bommasani and Steven Wu and Xanda Schofield},
    booktitle={NeurIPS 2019 Machine Learning with Guarantees Workshop},
    year={2019}
}

@article{krishna2019thieves,
  title={Thieves on sesame street! model extraction of bert-based apis},
  author={Krishna, Kalpesh and Tomar, Gaurav Singh and Parikh, Ankur P and Papernot, Nicolas and Iyyer, Mohit},
  journal={arXiv preprint arXiv:1910.12366},
  year={2019}
}

@article{bubeck2021universal,
    title={A Universal Law of Robustness via Isoperimetry},
    author={Sébastien Bubeck and Mark Sellke},
    year={2021},
    journal={arXiv preprint arXiv:2105.12806}
}

@article{nissenbaum2004privacy,
  title={Privacy as contextual integrity},
  author={Nissenbaum, Helen},
  journal={Wash. L. Rev.},
  volume={79},
  pages={119},
  year={2004},
  publisher={HeinOnline}
}

@book{nissenbaum.2009,
author = {Nissenbaum, Helen},
title = {Privacy in Context: Technology, Policy, and the Integrity of Social Life},
year = {2009},
publisher = {Stanford University Press},
}

  



@article{shafahi2019adversarially,
  title={Adversarially robust transfer learning},
  author={Shafahi, Ali and Saadatpanah, Parsa and Zhu, Chen and Ghiasi, Amin and Studer, Christoph and Jacobs, David and Goldstein, Tom},
  journal={arXiv preprint arXiv:1905.08232},
  year={2019}
}

@article{radiya2021data,
  title={Data Poisoning Won't Save You From Facial Recognition},
  author={Radiya-Dixit, Evani and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2106.14851},
  year={2021}
}

@misc{
Fort2021CLIPadversarial,
title={Adversarial examples for the OpenAI CLIP in its zero-shot classification regime and their semantic generalization},
url={https://stanislavfort.github.io/2021/01/12/OpenAI_CLIP_adversarial_examples.html},
author={Stanislav Fort},
year={2021},
month={Jan}
}

@inproceedings{wallace2019universal,
    Author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
    Booktitle = {Empirical Methods in Natural Language Processing},
    Year = {2019},
    Title = {Universal Adversarial Triggers for Attacking and Analyzing {NLP}}}
    
@article{qi2021hidden,
  title={Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger},
  author={Qi, Fanchao and Li, Mukai and Chen, Yangyi and Zhang, Zhengyan and Liu, Zhiyuan and Wang, Yasheng and Sun, Maosong},
  journal={arXiv preprint arXiv:2105.12400},
  year={2021}
}

@article{zellers2021merlot,
  title={MERLOT: Multimodal Neural Script Knowledge Models},
  author={Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:2106.02636},
  year={2021}
}

@article{Monti2019FakeND,
  title={Fake News Detection on Social Media using Geometric Deep Learning},
  author={Federico Monti and F. Frasca and D. Eynard and Damon Mannion and M. Bronstein},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06673}
}

@inproceedings{Zellers2019DefendingAN,
  title={Defending Against Neural Fake News},
  author={Rowan Zellers and Ari Holtzman and Hannah Rashkin and Yonatan Bisk and Ali Farhadi and Franziska Roesner and Yejin Choi},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{Holtzman2020TheCC,
  title={The Curious Case of Neural Text Degeneration},
  author={Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year={2020},
}

@inproceedings{Dinan2019BuildIB,
  title={Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack},
  author={Emily Dinan and Samuel Humeau and Bharath Chintagunta and J. Weston},
  booktitle={EMNLP/IJCNLP},
  year={2019}
}

@article {Vosoughi1146,
	author = {Vosoughi, Soroush and Roy, Deb and Aral, Sinan},
	title = {The spread of true and false news online},
	volume = {359},
	number = {6380},
	pages = {1146--1151},
	year = {2018},
	doi = {10.1126/science.aap9559},
	publisher = {American Association for the Advancement of Science},
	abstract = {There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by \~{}3 million people. False news reached more people than the truth; the top 1\% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed.Science, this issue p. 1146We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98\% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/359/6380/1146},
	eprint = {https://science.sciencemag.org/content/359/6380/1146.full.pdf},
	journal = {Science}
}

@article{Schick2021SelfDiagnosisAS,
  title={Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP},
  author={Timo Schick and Sahana Udupa and H. Schutze},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.00453}
}

@article{kreps_mccain_brundage_2020, 
    title={All the News That’s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation}, 
    DOI={10.1017/XPS.2020.37}, 
    journal={Journal of Experimental Political Science}, 
    publisher={Cambridge University Press}, author={Kreps, Sarah and McCain, R. Miles and Brundage, Miles}, 
    year={2020}, 
    pages={1–14}
}

@book{BuchananCSET2021,
    author={Buchanan, Ben and Lohn, Andrew and Musser, Micah and Sedova, Katerina},
    title={Truth, Lies, and Automation: How Language Models Could Change Disinformation},
    publisher={Center for Security and Emerging Technology},
    year={2021}, 
    DOI={10.51593/2021CA003}
}

@article{clark2021all,
  title={All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text},
  author={Clark, Elizabeth and August, Tal and Serrano, Sofia and Haduong, Nikita and Gururangan, Suchin and Smith, Noah A},
  journal={arXiv preprint arXiv:2107.00061},
  year={2021}
}

@inproceedings{Ippolito2020AutomaticDO,
  title={Automatic Detection of Generated Text is Easiest when Humans are Fooled},
  author={Daphne Ippolito and Daniel Duckworth and Chris Callison-Burch and D. Eck},
  booktitle={ACL},
  year={2020}
}

@article{ciancaglini2020malicious,
  title={Malicious uses and abuses of artificial intelligence},
  author={Ciancaglini, V and Gibson, C and Sancho, D and McCarthy, O and Eira, M and Amann, P and Klayn, A and McArdle, R and Beridze, I and Amann, P},
  journal={Trend Micro Research},
  year={2020},
  url={https://documents.trendmicro.com/assets/white_papers/wp-malicious-uses-and-abuses-of-artificial-intelligence.pdf}
}

@inproceedings{DiResta2019PotemkinP,
  title={Potemkin Pages \& Personas: Assessing GRU Online Operations, 2014-2019},
  author={Ren{\'e}e DiResta and Shelby Grossman},
  year={2019}
}

@inproceedings{DiResta2018TheT,
  title={The tactics \& tropes of the Internet Research Agency},
  author={Ren{\'e}e DiResta and K. Shaffer and Becky Ruppel and David Sullivan and Robert C. Matney and Ryan Fox and Jonathan Albright and Ben Johnson},
  year={2018},
  url={https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1003&context=senatedocs}
}

@article{Starbird2018, 
    title={Ecosystem or Echo-System? Exploring Content Sharing across Alternative Media Domains}, 
    volume={12}, 
    url={https://ojs.aaai.org/index.php/ICWSM/article/view/15009}, 
    abstractNote={ &lt;p&gt; This research examines the competing narratives about the role and function of Syria Civil Defence, a volunteer humanitarian organization popularly known as the White Helmets, working in war-torn Syria. Using a mixed-method approach based on seed data collected from Twitter, and then extending out to the websites cited in that data, we examine content sharing practices across distinct media domains that functioned to construct, shape, and propagate these narratives. We articulate a predominantly alternative media “echo-system” of websites that repeatedly share content about the White Helmets. Among other findings, our work reveals a small set of websites and authors generating content that is spread across diverse sites, drawing audiences from distinct communities into a shared narrative. This analysis also reveals the integration of government-funded media and geopolitical think tanks as source content for anti-White Helmets narratives. More broadly, the analysis demonstrates the role of alternative newswire-like services in providing content for alternative media websites. Though additional work is needed to understand these patterns over time and across topics, this paper provides insight into the dynamics of this multi-layered media ecosystem. &lt;/p&gt; }, 
    number={1}, 
    journal={Proceedings of the International AAAI Conference on Web and Social Media}, 
    author={Starbird, Kate and Arif, Ahmer and Wilson, Tom and Van Koevering, Katherine and Yefimova, Katya and Scarnecchia, Daniel}, 
    year={2018}, 
    month={Jun.} 
}

@inproceedings{lecun1990optimal,
  title={{Optimal Brain Damage}},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in Neural Information Processing Systems},
  pages={598--605},
  year={1990}
}

@article{shazeer2018mesh,
  title={{Mesh-TensorFlow: Deep Learning for Supercomputers}},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={10414--10423},
  year={2018}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX: Composable Transformations of Python+NumPy Programs}},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}

@article{lepikhin2020gshard,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{wang2021pet,
  title={{PET: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections}},
  author={Wang, Haojie and Zhai, Jidong and Gao, Mingyu and Ma, Zixuan and Tang, Shizhi and Zheng, Liyan and Li, Yuanzhi and Rong, Kaiyuan and Chen, Yuanyong and Jia, Zhihao},
  booktitle={15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)},
  pages={37--54},
  year={2021}
}

@Misc{fairscale,
  author =       {Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam Sheiffer, Anjali Sridhar, Min Xu},
  title =        {{FairScale:  A General Purpose Modular PyTorch Library for High Performance and Large Scale Training}},
  howpublished = {\url{https://github.com/facebookresearch/fairscale}},
  year =         {2021}
}

@article{narayanan2021efficient,
  title={{Efficient Large-Scale Language Model Training on GPU Clusters}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  journal={arXiv preprint arXiv:2104.04473},
  year={2021}
}

@inproceedings{narayanan2021memory,
  title={{Memory-Efficient Pipeline-Parallel DNN Training}},
  author={Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
  booktitle={International Conference on Machine Learning},
  pages={7937--7947},
  year={2021},
  organization={PMLR}
}

@inproceedings{santhanam2021distir,
  title={{DistIR: An Intermediate Representation for Optimizing Distributed Neural Networks}},
  author={Santhanam, Keshav and Krishna, Siddharth and Tomioka, Ryota and Fitzgibbon, Andrew and Harris, Tim},
  booktitle={Proceedings of the 1st Workshop on Machine Learning and Systems},
  pages={15--23},
  year={2021}
}

@article{hinton2015distilling,
  title={{Distilling the Knowledge in a Neural Network}},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{gholami2021survey,
  title={{A Survey of Quantization Methods for Efficient Neural Network Inference}},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2103.13630},
  year={2021}
}

@article{Diskin2021collab,
  title={{Distributed Deep Learning in Open Collaborations}},
  author={Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and Lhoest, Quentin and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry and Kashirin, Maxim and Borzunov, Alexander and others},
  journal={arXiv preprint arXiv:2106.10207},
  year={2021}
}

@article{molino2019ludwig,
  title={{Ludwig: A Type-Based Declarative Deep Learning Toolbox}},
  author={Molino, Piero and Dudin, Yaroslav and Miryala, Sai Sumanth},
  journal={arXiv preprint arXiv:1909.07930},
  year={2019}
}

@inproceedings{narayanan2018accelerating,
  title={{Accelerating Deep Learning Workloads through Efficient Multi-Model Execution}},
  author={Narayanan, Deepak and Santhanam, Keshav and Phanishayee, Amar and Zaharia, Matei},
  booktitle={NeurIPS Workshop on Systems for Machine Learning},
  pages={20},
  year={2018}
}

@inproceedings{shen2019nexus,
  title={{Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis}},
  author={Shen, Haichen and Chen, Lequn and Jin, Yuchen and Zhao, Liangyu and Kong, Bingyu and Philipose, Matthai and Krishnamurthy, Arvind and Sundaram, Ravi},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={322--337},
  year={2019}
}

@article{Ryabinin2020Learninghome,
  title={{Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts}},
  author={Ryabinin, Max and Gusev, Anton},
  journal={arXiv preprint arXiv:2002.04013},
  year={2020}
}


@article{huang2019gpipe,
  title={{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={103--112},
  year={2019}
}

@inproceedings{narayanan2019pipedream,
  title={{PipeDream: Generalized Pipeline Parallelism for DNN Training}},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}

@article{rajbhandari2019zero,
  title={{ZeRO: Memory Optimization Towards Training A Trillion Parameter Models}},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  journal={arXiv preprint arXiv:1910.02054},
  year={2019}
}

@misc{m2m100,
  title = {{M2M-100: AI Model That Translates 100 Languages Without Relying on English}},
  author = {M2M-100},
  howpublished = {\url{https://about.fb.com/news/2020/10/first-multilingual-machine-translation-model/}},
  year={2020}
}

@misc{turingnlg,
  title = {{Turing-NLG: A 17-Billion-Parameter Language Model by Microsoft}},
  author = {Turing-NLG},
  howpublished = {\url{https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft//}},
  year={2020}
}

@misc{pytorchjit,
  title = {{PyTorch JIT}},
  author = {PyTorch},
  howpublished = {\url{https://pytorch.org/docs/stable/jit.html}},
  year = {2021}
}

@misc{nccl,
  title = {{NVIDIA Collective Communication Library (NCCL)}},
  author = {NVIDIA},
  howpublished = {\url{https://developer.nvidia.com/nccl}},
  year = {2021}
}

@inproceedings{jouppi2017datacenter,
  title={{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={1--12},
  year={2017}
}

@misc{selene,
  title = {{Selene Supercomputer}},
  author = {Selene},
  howpublished = {\url{https://www.top500.org/system/179842/}},
  year = {2021}
}

@inproceedings{coleman2017dawnbench,
  title={{DAWNBench: An End-to-End Deep Learning Benchmark and Competition}},
  author={Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and R{\'e}, Chris and Zaharia, Matei},
  booktitle={NeurIPS Workshop on Systems for Machine Learning},
  year={2017}
}

@inproceedings{rasley2020deepspeed,
  title={{DeepSpeed: System Optimizations Enable Training Deep Learning Models with over 100 Billion Parameters}},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@inproceedings{rajbhandari2020zero,
  title={{ZeRO: Memory Optimizations toward Training Trillion Parameter Models}},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{li2021prefix,
  title={{Prefix-Tuning: Optimizing Continuous Prompts for Generation}},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{lin2016increased,
  title={Increased risk of respiratory mortality associated with the high-tech manufacturing industry: A 26-Year study},
  author={Lin, Ro-Ting and Christiani, David C and Kawachi, Ichiro and Chan, Ta-Chien and Chiang, Po-Huang and Chan, Chang-Chuan},
  journal={International journal of environmental research and public health},
  volume={13},
  number={6},
  pages={557},
  year={2016},
  publisher={Multidisciplinary Digital Publishing Institute}
}


@inproceedings{tu2009ineffective,
  title={Ineffective environmental laws in regulating electronic manufacturing pollution: Examining water pollution disputes in Taiwan},
  author={Tu, Wenling and Lee, Yujung},
  booktitle={2009 IEEE International Symposium on Sustainable Systems and Technology},
  pages={1--6},
  year={2009},
  organization={IEEE}
}


@misc{verge_copilot,
    title={{GitHub’s Automatic Coding Tool Rests on Untested Legal Ground}},
    author={David Gershgorn},
    howpublished = {\url{https://www.theverge.com/2021/7/7/22561180/github-copilot-legal-copyright-fair-use-public-code}},
    year={2021}
}

@article{kang2020model,
  title={{Model Assertions for Monitoring and Improving ML Models}},
  author={Kang, Daniel and Raghavan, Deepti and Bailis, Peter and Zaharia, Matei},
  journal={arXiv preprint arXiv:2003.01668},
  year={2020}
}

@article{anil2020scalable,
  title={{Scalable Second Order Optimization for Deep Learning}},
  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
  journal={arXiv preprint arXiv:2002.09018},
  year={2020}
}

@inproceedings{shazeer2018adafactor,
  title={{Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{su2021roformer,
  title={{Roformer: Enhanced Transformer with Rotary Position Embedding}},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{xue2021byt5,
  title={{ByT5: Towards a Token-Free Future with Pre-Trained Byte-to-Byte Models}},
  author={Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal={arXiv preprint arXiv:2105.13626},
  year={2021}
}

@article{tay2021charformer,
  title={{Charformer: Fast Character Transformers via Gradient-based Subword Tokenization}},
  author={Tay, Yi and Tran, Vinh Q and Ruder, Sebastian and Gupta, Jai and Chung, Hyung Won and Bahri, Dara and Qin, Zhen and Baumgartner, Simon and Yu, Cong and Metzler, Donald},
  journal={arXiv preprint arXiv:2106.12672},
  year={2021}
}

@article{child2019generating,
  title={{Generating Long Sequences with Sparse Transformers}},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@InProceedings{pmlr-v97-so19a, title = {{The Evolved Transformer}}, author = {So, David and Le, Quoc and Liang, Chen}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {5877--5886}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/so19a/so19a.pdf}, url = { http://proceedings.mlr.press/v97/so19a.html }
}

@article{wang2020linformer,
  title={{Linformer: Self-Attention with Linear Complexity}},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{beltagy2020longformer,
  title={{Longformer: The Long-Document Transformer}},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{tay2020efficient,
  title={{Efficient Transformers: A Survey}},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@article{kitaev2020reformer,
  title={{Reformer: The Efficient Transformer}},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{lee2021fnet,
  title={{FNet: Mixing Tokens with Fourier Transforms}},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}

@article{gordon2020compressing,
  title={{Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning}},
  author={Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas},
  journal={arXiv preprint arXiv:2002.08307},
  year={2020}
}

@article{mccarley2019structured,
  title={{Structured Pruning of a BERT-Based Question Answering Model}},
  author={McCarley, JS and Chakravarti, Rishav and Sil, Avirup},
  journal={arXiv preprint arXiv:1910.06360},
  year={2019}
}

@article{wang2019structured,
  title={{Structured Pruning of Large Language Models}},
  author={Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
  journal={arXiv preprint arXiv:1910.04732},
  year={2019}
}

@article{sajjad2020effect,
  title={{On the Effect of Dropping Layers of Pre-trained Transformer Models}},
  author={Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  journal={arXiv preprint arXiv:2004.03844},
  year={2020}
}

@article{sanh2019distilbert,
  title={{DistilBERT, A Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter}},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{li2020pytorch,
  title={{PyTorch Distributed: Experiences on Accelerating Data Parallel Training}},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@article{dettmers2019sparse,
  title={{Sparse Networks from Scratch: Faster Training Without Losing Performance}},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{li2020train,
  title={{Train Large, then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers}},
  author={Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2002.11794},
  year={2020}
}

@misc{deepspeed,
  title = {{DeepSpeed}},
  howpublished = {\url{https://www.deepspeed.ai/}}
}

@misc{rajbhandari2021zeroinfinity,
      title={{ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning}}, 
      author={Samyam Rajbhandari and Olatunji Ruwase and Jeff Rasley and Shaden Smith and Yuxiong He},
      year={2021},
      eprint={2104.07857},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}


@misc{megatron,
  title = {{Megatron}},
  howpublished = {\url{https://github.com/nvidia/megatron-lm}}
}

@article{guu2020realm,
  title={{REALM: Retrieval-Augmented Language Model Pre-Training}},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2002.08909},
  year={2020}
}

@article{saeed2003medicaldatamgmt,
  title={{TM4: A Free, Open-Source System for
Microarray Data Management and Analysis}},
  author={Saeed, A.I. and Sharov, V. and White, J. and Li, J. and W. Liang, N. Bhagabati, J. Braisted, M. Klapa, T. Currier, M. Thiagarajan, Sturn, A. and Snuffin, M. and Rezantsev, A. and Popov, D. and A. Ryltsov, E. Kostukovich, I. Borisovsky, Z. Liu, A. Vinsavich and Trush, V. and Quackenbush, J.},
  journal={BioTechniques},
  year={2003}
}

@article{huang2018agridatamgmt,
  title={Agricultural remote sensing big data: Management and applications},
  author={Huang, Yanbo and Zhong-xin, Chen and Tao, Yu and Huang, Xiang-zhi, and Gu, Xing-fa},
  journal={Journal of Integrative Agriculture},
  year={2018}
}

@misc{castle2020entrdatamgmt, 
    title = {Enterprise Data Management — Driving Large-Scale Change in Your Organization},
    author={Scott Castle},
    howpublished = {\url{https://www.sisense.com/blog/enterprise-data-management-driving-large-scale-change-in-your-organization/}}, 
    year={2020},
    month={June}
}

@inproceedings{Sun2021ERNIE3L,
  title={ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},
  author={Yu Sun and Shuohuan Wang and Shikun Feng and Siyu Ding and Chao Pang and Junyuan Shang and Jiaxiang Liu and Xuyi Chen and Yanbin Zhao and Yuxiang Lu and Weixin Liu and Zhihua Wu and Weibao Gong and Jianzhong Liang and Zhizhou Shang and Peng Sun and Wei Liu and Xuan Ouyang and Dianhai Yu and Hao Tian and Hua Wu and Haifeng Wang},
  year={2021}
}

@inproceedings{Zhang2019ERNIEEL,
  title={ERNIE: Enhanced Language Representation with Informative Entities},
  author={Zhengyan Zhang and Xu Han and Zhiyuan Liu and Xin Jiang and Maosong Sun and Qun Liu},
  booktitle={ACL},
  year={2019}
}

@inproceedings{Peters2019KnowledgeEC,
  title={Knowledge Enhanced Contextual Word Representations},
  author={Matthew E. Peters and Mark Neumann and IV RobertLLogan and Roy Schwartz and V. Joshi and Sameer Singh and Noah A. Smith},
  booktitle={EMNLP/IJCNLP},
  year={2019}
}

@article{Wang2021KEPLERAU,
  title={KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation},
  author={Xiaozhi Wang and Tianyu Gao and Zhaocheng Zhu and Zhiyuan Liu and Juan-Zi Li and Jian Tang},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={9},
  pages={176-194}
}

@article{Liu2020KBERTEL,
  title={K-BERT: Enabling Language Representation with Knowledge Graph},
  author={Weijie Liu and Peng Zhou and Zhe Zhao and Zhiruo Wang and Qi Ju and Haotang Deng and P. Wang},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.07606}
}

@inproceedings{Sun2020CoLAKECL,
  title={CoLAKE: Contextualized Language and Knowledge Embedding},
  author={Tianxiang Sun and Yunfan Shao and Xipeng Qiu and Qipeng Guo and Yaru Hu and X. Huang and Zheng Zhang},
  booktitle={COLING},
  year={2020}
}

@article{Yu2020JAKETJP,
  title={JAKET: Joint Pre-training of Knowledge Graph and Language Understanding},
  author={Donghan Yu and Chenguang Zhu and Yiming Yang and Michael Zeng},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.00796}
}

@inproceedings{Fvry2020EntitiesAE,
  title={Entities as Experts: Sparse Memory Access with Entity Supervision},
  author={Thibault F{\'e}vry and Livio Baldini Soares and Nicholas FitzGerald and Eunsol Choi and T. Kwiatkowski},
  booktitle={EMNLP},
  year={2020}
}


@inproceedings{Bosselut2019COMETCT,
  title={COMET: Commonsense Transformers for Automatic Knowledge Graph Construction},
  author={Antoine Bosselut and Hannah Rashkin and Maarten Sap and Chaitanya Malaviya and A. Çelikyilmaz and Yejin Choi},
  booktitle={ACL},
  year={2019}
}

@inproceedings{Hwang2021COMETATOMIC2O,
  title={COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs},
  author={Jena D. Hwang and Chandra Bhagavatula and Ronan Le Bras and Jeff Da and Keisuke Sakaguchi and Antoine Bosselut and Yejin Choi},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{Petroni2019LanguageMA,
  title={Language Models as Knowledge Bases?},
  author={Fabio Petroni and Tim Rockt{\"a}schel and Patrick Lewis and A. Bakhtin and Yuxiang Wu and Alexander H. Miller and S. Riedel},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{Kassner2021MultilingualLI,
  title={Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models},
  author={Nora Kassner and Philipp Dufter and Hinrich Sch{\"u}tze},
  booktitle={EACL},
  year={2021}
}

@inproceedings{Lample2019LargeML,
  title={Large Memory Layers with Product Keys},
  author={Guillaume Lample and Alexandre Sablayrolles and Marc'Aurelio Ranzato and Ludovic Denoyer and H. J{\'e}gou},
  booktitle={NeurIPS},
  year={2019}
}

@article{Geva2020TransformerFL,
  title={Transformer Feed-Forward Layers Are Key-Value Memories},
  author={Mor Geva and R. Schuster and Jonathan Berant and Omer Levy},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.14913}
}

@article{Graves2016HybridCU,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={A. Graves and Greg Wayne and Malcolm Reynolds and Tim Harley and Ivo Danihelka and Agnieszka Grabska-Barwinska and Sergio Gomez Colmenarejo and Edward Grefenstette and Tiago Ramalho and J. Agapiou and Adri{\`a} Puigdom{\`e}nech Badia and K. Hermann and Yori Zwols and Georg Ostrovski and Adam Cain and Helen King and C. Summerfield and P. Blunsom and K. Kavukcuoglu and D. Hassabis},
  journal={Nature},
  year={2016},
  volume={538},
  pages={471-476}
}

@article{Bosselut2018SimulatingAD,
  title={Simulating Action Dynamics with Neural Process Networks},
  author={Antoine Bosselut and Omer Levy and Ari Holtzman and C. Ennis and D. Fox and Yejin Choi},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{wolf2020transformers,
  title={{Transformers: State-of-the-Art Natural Language Processing}},
  author={Wolf, Thomas and Chaumond, Julien and Debut, Lysandre and Sanh, Victor and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and others},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={38--45},
  year={2020}
}

@article{mudigere2021high,
  title={{High-Performance, Distributed Training of Large-Scale Deep Learning Recommendation Models}},
  author={Mudigere, Dheevatsa and Hao, Yuchen and Huang, Jianyu and Tulloch, Andrew and Sridharan, Srinivas and Liu, Xing and Ozdal, Mustafa and Nie, Jade and Park, Jongsoo and Luo, Liang and others},
  journal={arXiv preprint arXiv:2104.05158},
  year={2021}
}

@inproceedings{mattson2020mlperf,
  title={{MLPerf Training Benchmark}},
  author={Mattson, Peter and Cheng, Christine and Coleman, Cody and Diamos, Greg and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and others},
  booktitle={Third Conference on Machine Learning and Systems},
  year={2020}
}

@inproceedings{jia2019taso,
  title={{TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions}},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}

@article{jia2019optimizing,
  title={{Optimizing DNN Computation with Relaxed Graph Substitutions}},
  author={Jia, Zhihao and Thomas, James and Warszawski, Tod and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
  journal={SysML 2019},
  year={2019}
}

@article{jia2018beyond,
  title={{Beyond Data and Model Parallelism for Deep Neural Networks}},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={SysML 2019},
  year={2019}
}

@article{jayakumar2021top,
  title={{Top-KAST: Top-K Always Sparse Training}},
  author={Jayakumar, Siddhant M and Pascanu, Razvan and Rae, Jack W and Osindero, Simon and Elsen, Erich},
  journal={arXiv preprint arXiv:2106.03517},
  year={2021}
}

@InProceedings{Elsen_2020_CVPR,
author = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
title = {{Fast Sparse ConvNets}},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{gale2020sparse,
  title={{Sparse GPU Kernels for Deep Learning}},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2020},
  organization={IEEE}
}

@article{polino2018model,
  title={{Model Compression via Distillation and Quantization}},
  author={Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
  journal={arXiv preprint arXiv:1802.05668},
  year={2018}
}

@inproceedings{zhou2018adaptive,
  title={{Adaptive Quantization for Deep Neural Networks}},
  author={Zhou, Yiren and Moosavi-Dezfooli, Seyed-Mohsen and Cheung, Ngai-Man and Frossard, Pascal},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@InProceedings{pmlr-v119-evci20a,
  title = 	 {{Rigging the Lottery: Making All Tickets Winners}},
  author =       {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2943--2952},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/evci20a/evci20a.pdf}
}


@misc{cfaa,
  title = {{Scraping public websites likely doesn’t violate the Computer Fraud and Abuse Act, court holds}},
  author={ Wajert, Lyndsey and Rottman, Gabe},
  howpublished = {\url{https://www.rcfp.org/scraping-not-violation-cfaa/}},
  year={2019}
}

@article{lemley2020fair,
  title={Fair Learning},
  author={Lemley, Mark A and Casey, Bryan},
  journal={Tex. L. Rev.},
  volume={99},
  pages={743},
  year={2020},
  publisher={HeinOnline}
}
@article{lemley2019remedies,
  title={Remedies for robots},
  author={Lemley, Mark A and Casey, Bryan},
  journal={The University of Chicago Law Review},
  volume={86},
  number={5},
  pages={1311--1396},
  year={2019},
  publisher={JSTOR}
}
@article{selbst2020negligence,
  title={Negligence and AI's human users},
  author={Selbst, Andrew D},
  journal={BUL Rev.},
  volume={100},
  pages={1315},
  year={2020},
  publisher={HeinOnline}
}
@article{massaro2016siri,
  title={SIRI-OUSLY 2.0: what artificial intelligence reveals about the first amendment},
  author={Massaro, Toni M and Norton, Helen and Kaminski, Margot E},
  journal={Minn. L. Rev.},
  volume={101},
  pages={2481},
  year={2016},
  publisher={HeinOnline}
}
@article{kajbaf2019first,
  title={The First Amendment and Modern Technology: The Free Speech Clause and Chatbot Speech},
  author={Kajbaf, Hilda},
  journal={Hastings Const. LQ},
  volume={47},
  pages={337},
  year={2019},
  publisher={HeinOnline}
}
@article{lamo2019regulating,
  title={Regulating bot speech},
  author={Lamo, Madeline and Calo, Ryan},
  journal={UCLA L. Rev.},
  volume={66},
  pages={988},
  year={2019},
  publisher={HeinOnline}
}
@article{grimmelmann2015there,
  title={There's No Such Thing as a Computer-Authored Work-And It's a Good Thing, Too},
  author={Grimmelmann, James},
  journal={Colum. JL \& Arts},
  volume={39},
  pages={403},
  year={2015},
  publisher={HeinOnline}
}
@article{ginsburg2019authors,
  title={Authors and machines},
  author={Ginsburg, Jane C and Budiardjo, Luke Ali},
  journal={Berkeley Tech. LJ},
  volume={34},
  pages={343},
  year={2019},
  publisher={HeinOnline}
}

@INPROCEEDINGS{dai2018dark,
  author={Dai, Dengxin and Gool, Luc Van},
  booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime}, 
  year={2018},
  volume={},
  number={},
  pages={3819-3824},
  doi={10.1109/ITSC.2018.8569387}}
  
@InProceedings{yu2020bdd100k,
    author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen,
              Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
    title = {BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}

@misc{sun2020scalability,
      title={Scalability in Perception for Autonomous Driving: Waymo Open Dataset}, 
      author={Pei Sun and Henrik Kretzschmar and Xerxes Dotiwalla and Aurelien Chouard and Vijaysai Patnaik and Paul Tsui and James Guo and Yin Zhou and Yuning Chai and Benjamin Caine and Vijay Vasudevan and Wei Han and Jiquan Ngiam and Hang Zhao and Aleksei Timofeev and Scott Ettinger and Maxim Krivokon and Amy Gao and Aditya Joshi and Sheng Zhao and Shuyang Cheng and Yu Zhang and Jonathon Shlens and Zhifeng Chen and Dragomir Anguelov},
      year={2020},
      eprint={1912.04838},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{ruder2018strong,
  author =	"Ruder, Sebastian
  	 and Plank, Barbara",
  title =    "Strong Baselines for Neural Semi-Supervised Learning under Domain Shift",
  booktitle = 	     "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year =    "2018",
  publisher =	"Association for Computational Linguistics",
  pages =   "1044--1054",
  location =	"Melbourne, Australia",
  url =    "http://aclweb.org/anthology/P18-1096"
}

@book{abney2007semisup,
author = {Abney, Steven},
title = {Semisupervised Learning for Computational Linguistics},
year = {2007},
isbn = {1584885599},
publisher = {Chapman \& Hall/CRC},
edition = {1st},
abstract = {The rapid advancement in the theoretical understanding of statistical and machine learning methods for semisupervised learning has made it difficult for nonspecialists to keep up to date in the field. Providing a broad, accessible treatment of the theory as well as linguistic applications, Semisupervised Learning for Computational Linguistics offers self-contained coverage of semisupervised methods that includes background material on supervised and unsupervised learning. The book presents a brief history of semisupervised learning and its place in the spectrum of learning methods before moving on to discuss well-known natural language processing methods, such as self-training and co-training. It then centers on machine learning techniques, including the boundary-oriented methods of perceptrons, boosting, support vector machines (SVMs), and the null-category noise model. In addition, the book covers clustering, the expectation-maximization (EM) algorithm, related generative methods, and agreement methods. It concludes with the graph-based method of label propagation as well as a detailed discussion of spectral methods. Taking an intuitive approach to the material, this lucid book facilitates the application of semisupervised learning methods to natural language processing and provides the framework and motivation for a more systematic study of machine learning.}
}

@misc{han2019unsupervised,
      title={Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling}, 
      author={Xiaochuang Han and Jacob Eisenstein},
      year={2019},
      eprint={1904.02817},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{kumar2020conservative,
  author       = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
  title        = {Conservative Q-Learning for Offline Reinforcement Learning},
  conference   = {arXiv Pre-print},
  url          = {https://arxiv.org/abs/2006.04779},
  year = {2020},
}

@article{yu2020mopo,
  title={MOPO: Model-based Offline Policy Optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={arXiv preprint arXiv:2005.13239},
  year={2020}
}

@article{liu2020multilingual,
    title = "Multilingual Denoising Pre-training for Neural Machine Translation",
    author = "Liu, Yinhan  and
      Gu, Jiatao  and
      Goyal, Naman  and
      Li, Xian  and
      Edunov, Sergey  and
      Ghazvininejad, Marjan  and
      Lewis, Mike  and
      Zettlemoyer, Luke",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://aclanthology.org/2020.tacl-1.47",
    doi = "10.1162/tacl_a_00343",
    pages = "726--742",
    abstract = "This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART{---}a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1",
}

@misc{sun2019unsupervised,
      title={Unsupervised Domain Adaptation through Self-Supervision}, 
      author={Yu Sun and Eric Tzeng and Trevor Darrell and Alexei A. Efros},
      year={2019},
      eprint={1909.11825},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xiao2021contrastive,
      title={What Should Not Be Contrastive in Contrastive Learning}, 
      author={Tete Xiao and Xiaolong Wang and Alexei A. Efros and Trevor Darrell},
      year={2021},
      eprint={2008.05659},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{cole2021contrastive,
      title={When Does Contrastive Visual Representation Learning Work?}, 
      author={Elijah Cole and Xuan Yang and Kimberly Wilber and Oisin Mac Aodha and Serge Belongie},
      year={2021},
      eprint={2105.05837},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{houlsby19adapter,
title = {Parameter-Efficient Transfer Learning for {NLP}}, 
author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {2790--2799}, 
year = {2019}, 
editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, 
series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf}, url = { http://proceedings.mlr.press/v97/houlsby19a.html }, 

}

@inproceedings{schick-schutze-2021-just,
    title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.185",
    doi = "10.18653/v1/2021.naacl-main.185",
    pages = "2339--2352",
}
@inproceedings{schick-schutze-2021-exploiting,
    title = "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.20",
    pages = "255--269",
}

@inproceedings{le-scao-rush-2021-many,
    title = "How many data points is a prompt worth?",
    author = "Le Scao, Teven  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.208",
    doi = "10.18653/v1/2021.naacl-main.208",
    pages = "2627--2636",
}
@article{aghajanyan2020intrinsic,
  author    = {Armen Aghajanyan and
               Luke Zettlemoyer and
               Sonal Gupta},
  title     = {Intrinsic Dimensionality Explains the Effectiveness of Language Model
               Fine-Tuning},
  journal   = {CoRR},
  volume    = {abs/2012.13255},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.13255},
  archivePrefix = {arXiv},
  eprint    = {2012.13255},
  timestamp = {Tue, 05 Jan 2021 16:02:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-13255.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hu2021lora,
  author    = {Edward J. Hu and
               Yelong Shen and
               Phillip Wallis and
               Zeyuan Allen{-}Zhu and
               Yuanzhi Li and
               Shean Wang and
               Weizhu Chen},
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal   = {CoRR},
  volume    = {abs/2106.09685},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.09685},
  archivePrefix = {arXiv},
  eprint    = {2106.09685},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zaken2021bitfit,
  author    = {Elad Ben Zaken and
               Shauli Ravfogel and
               Yoav Goldberg},
  title     = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based
               Masked Language-models},
  journal   = {CoRR},
  volume    = {abs/2106.10199},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.10199},
  archivePrefix = {arXiv},
  eprint    = {2106.10199},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-10199.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{lester2021power,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shao2021adversarial,
      title={On the Adversarial Robustness of Visual Transformers}, 
      author={Rulin Shao and Zhouxing Shi and Jinfeng Yi and Pin-Yu Chen and Cho-Jui Hsieh},
      year={2021},
      eprint={2103.15670},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{winkler2019derm,
    author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and Hofmann-Wellenhof, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
    title = "{Association Between Surgical Skin Markings in Dermoscopic Images and Diagnostic Performance of a Deep Learning Convolutional Neural Network for Melanoma Recognition}",
    journal = {JAMA Dermatology},
    volume = {155},
    number = {10},
    pages = {1135-1141},
    year = {2019},
    month = {10},
    abstract = "{Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest.To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market.A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas).The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion.Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN’s diagnostic classification in unmarked, marked, and cropped images.In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\\% (95\\% CI, 79\\%-99.2\\%) and a specificity of 84.1\\% (95\\% CI, 76.0\\%-89.8\\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\\% (95\\% CI, 85.7\\%-100\\%) and a significantly reduced specificity of 45.8\\% (95\\% CI, 36.7\\%-55.2\\%, P \\&lt; .001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\\% (95\\% CI, 85.7\\%-100\\%), specificity of 97.2\\% (95\\% CI, 92.1\\%-99.0\\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate.This study’s findings suggest that skin markings significantly interfered with the CNN’s correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN’s association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN.German Clinical Trial Register (DRKS) Identifier: DRKS00013570}",
    issn = {2168-6068},
    doi = {10.1001/jamadermatol.2019.1735},
    url = {https://doi.org/10.1001/jamadermatol.2019.1735},
    eprint = {https://jamanetwork.com/journals/jamadermatology/articlepdf/2740808/jamadermatology\_winkler\_2019\_oi\_190038.pdf},
}



%%%%%% healthcare + biomedicine %%%%%% 
@incollection{percha2012discovery,
  title={Discovery and explanation of drug-drug interactions via text mining},
  author={Percha, Bethany and Garten, Yael and Altman, Russ B},
  booktitle={Biocomputing 2012},
  pages={410--421},
  year={2012},
  publisher={World Scientific}
}
@inproceedings{mozannar2020consistent,
  title={Consistent estimators for learning to defer to an expert},
  author={Mozannar, Hussein and Sontag, David},
  booktitle={International Conference on Machine Learning},
  pages={7076--7087},
  year={2020},
  organization={PMLR}
}
@inproceedings{wang2021domain,
  title={Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature},
  author={Wang, Yu and Li, Jinchao and Naumann, Tristan and Xiong, Chenyan and Cheng, Hao and Tinn, Robert and Wong, Cliff and Usuyama, Naoto and Rogahn, Richard and Shen, Zhihong and others},
  booktitle={ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)},
  year={2021}
}
@article{kadurin2017drugan,
  title={druGAN: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico},
  author={Kadurin, Artur and Nikolenko, Sergey and Khrabrov, Kuzma and Aliper, Alex and Zhavoronkov, Alex},
  journal={Molecular pharmaceutics},
  volume={14},
  number={9},
  pages={3098--3104},
  year={2017},
  publisher={ACS Publications}
}
@article{lavertu2019redmed,
  title={RedMed: Extending drug lexicons for social media applications},
  author={Lavertu, Adam and Altman, Russ B},
  journal={Journal of biomedical informatics},
  volume={99},
  pages={103307},
  year={2019},
  publisher={Elsevier}
}
@article{li2019neural,
  title={A neural topic-attention model for medical term abbreviation disambiguation},
  author={Li, Irene and Yasunaga, Michihiro and Nuzumlal{\i}, Muhammed Yavuz and Caraballo, Cesar and Mahajan, Shiwani and Krumholz, Harlan and Radev, Dragomir},
  journal={Machine Learning for Health (ML4H)},
  year={2019}
}
@article{keehan2020national,
  title={National Health Expenditure Projections, 2019--28: Expected Rebound In Prices Drives Rising Spending Growth: National health expenditure projections for the period 2019--2028.},
  author={Keehan, Sean P and Cuckler, Gigi A and Poisal, John A and Sisko, Andrea M and Smith, Sheila D and Madison, Andrew J and Rennie, Kathryn E and Fiore, Jacqueline A and Hardesty, James C},
  journal={Health Affairs},
  volume={39},
  number={4},
  pages={704--714},
  year={2020}
}
@article{davenport2019potential,
  title={The potential for artificial intelligence in healthcare},
  author={Davenport, Thomas and Kalakota, Ravi},
  journal={Future healthcare journal},
  volume={6},
  number={2},
  pages={94},
  year={2019},
  publisher={Royal College of Physicians}
}
@article{shrank2019waste,
  title={Waste in the US health care system: estimated costs and potential for savings},
  author={Shrank, William H and Rogstad, Teresa L and Parekh, Natasha},
  journal={Jama},
  volume={322},
  number={15},
  pages={1501--1509},
  year={2019},
  publisher={American Medical Association}
}
@article{kocher2021reducing,
  title={Reducing administrative waste in the US health care system},
  author={Kocher, Robert P},
  journal={JAMA},
  volume={325},
  number={5},
  pages={427--428},
  year={2021},
  publisher={American Medical Association}
}
@article{kirch2017addressing,
  title={Addressing the physician shortage: the peril of ignoring demography},
  author={Kirch, Darrell G and Petelle, Kate},
  journal={Jama},
  volume={317},
  number={19},
  pages={1947--1948},
  year={2017},
  publisher={American Medical Association}
}

%% biomedicine
@article{harrer2019artificial,
  title={Artificial intelligence for clinical trial design},
  author={Harrer, Stefan and Shah, Pratik and Antony, Bhavna and Hu, Jianying},
  journal={Trends in pharmacological sciences},
  volume={40},
  number={8},
  pages={577--591},
  year={2019},
  publisher={Elsevier}
}
@article{ren2021combiner,
  title={Combiner: Full Attention Transformer with Sparse Computation Cost},
  author={Ren, Hongyu and Dai, Hanjun and Dai, Zihang and Yang, Mengjiao and Leskovec, Jure and Schuurmans, Dale and Dai, Bo},
  journal={arXiv preprint arXiv:2107.05768},
  year={2021}
}
@article{hughes2011principles,
  title={Principles of early drug discovery},
  author={Hughes, James P and Rees, Stephen and Kalindjian, S Barrett and Philpott, Karen L},
  journal={British journal of pharmacology},
  volume={162},
  number={6},
  pages={1239--1249},
  year={2011},
  publisher={Wiley Online Library}
}
@article{amann2020explainability,
  title={Explainability for artificial intelligence in healthcare: a multidisciplinary perspective},
  author={Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I},
  journal={BMC Medical Informatics and Decision Making},
  volume={20},
  number={1},
  pages={1--9},
  year={2020},
  publisher={Springer}
}
@article{act1996health,
  title={Health insurance portability and accountability act of 1996},
  author={Act, Accountability},
  journal={Public law},
  volume={104},
  pages={191},
  year={1996}
}
@article{whirl2012pharmacogenomics,
  title={Pharmacogenomics knowledge for personalized medicine},
  author={Whirl-Carrillo, Michelle and McDonagh, Ellen M and Hebert, JM and Gong, Li and Sangkuhl, K and Thorn, CF and Altman, Russ B and Klein, Teri E},
  journal={Clinical Pharmacology \& Therapeutics},
  volume={92},
  number={4},
  pages={414--417},
  year={2012},
  publisher={Wiley Online Library}
}
@article{hamburg2010path,
  title={The path to personalized medicine},
  author={Hamburg, Margaret A and Collins, Francis S},
  journal={New England Journal of Medicine},
  volume={363},
  number={4},
  pages={301--304},
  year={2010},
  publisher={Mass Medical Soc}
}
@article{wouters2020estimated,
  title={Estimated research and development investment needed to bring a new medicine to market, 2009-2018},
  author={Wouters, Olivier J and McKee, Martin and Luyten, Jeroen},
  journal={Jama},
  volume={323},
  number={9},
  pages={844--853},
  year={2020},
  publisher={American Medical Association}
}
@article{yu2018artificial,
  title={Artificial intelligence in healthcare},
  author={Yu, Kun-Hsing and Beam, Andrew L and Kohane, Isaac S},
  journal={Nature biomedical engineering},
  volume={2},
  number={10},
  pages={719--731},
  year={2018},
  publisher={Nature Publishing Group}
}
@article{kreps2020model,
  title={Model uncertainty, political contestation, and public trust in science: Evidence from the COVID-19 pandemic},
  author={Kreps, SE and Kriner, DL},
  journal={Science advances},
  volume={6},
  number={43},
  pages={eabd4563},
  year={2020},
  publisher={American Association for the Advancement of Science}
}
@article{hanney2015long,
  title={How long does biomedical research take? Studying the time taken between biomedical and health research and its translation into products, policy, and practice},
  author={Hanney, Stephen R and Castle-Clarke, Sophie and Grant, Jonathan and Guthrie, Susan and Henshall, Chris and Mestre-Ferrandiz, Jorge and Pistollato, Michele and Pollitt, Alexandra and Sussex, Jon and Wooding, Steven},
  journal={Health research policy and systems},
  volume={13},
  number={1},
  pages={1--18},
  year={2015},
  publisher={BioMed Central}
}
@inproceedings{bharti2020medbot,
  title={Medbot: Conversational artificial intelligence powered chatbot for delivering tele-health after covid-19},
  author={Bharti, Urmil and Bajaj, Deepali and Batra, Hunar and Lalit, Shreya and Lalit, Shweta and Gangwani, Aayushi},
  booktitle={2020 5th International Conference on Communication and Electronics Systems (ICCES)},
  pages={870--875},
  year={2020},
  organization={IEEE}
}
@article{lalmuanawma2020applications,
  title={Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: A review},
  author={Lalmuanawma, Samuel and Hussain, Jamal and Chhakchhuak, Lalrinfela},
  journal={Chaos, Solitons \& Fractals},
  pages={110059},
  year={2020},
  publisher={Elsevier}
}
@article{mckibbin2020economic,
  title={The economic impact of COVID-19},
  author={McKibbin, Warwick and Fernando, Roshen and others},
  journal={Economics in the Time of COVID-19},
  volume={45},
  number={10.1162},
  year={2020},
  publisher={Centre for Economic Policy Research London}
}
@article{ruiz2020identification,
  title={Identification of disease treatment mechanisms through the multiscale interactome},
  author={Ruiz, Camilo and Zitnik, Marinka and Leskovec, Jure},
  journal={Nature Communications},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{linardatos2021explainable,
  title={Explainable AI: A Review of Machine Learning Interpretability Methods},
  author={Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  journal={Entropy},
  volume={23},
  number={1},
  pages={18},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{holzinger2019causability,
  title={Causability and explainability of artificial intelligence in medicine},
  author={Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"u}ller, Heimo},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={9},
  number={4},
  pages={e1312},
  year={2019},
  publisher={Wiley Online Library}
}
@article{challen2019artificial,
  title={Artificial intelligence, bias and clinical safety},
  author={Challen, Robert and Denny, Joshua and Pitt, Martin and Gompels, Luke and Edwards, Tom and Tsaneva-Atanasova, Krasimira},
  journal={BMJ Quality \& Safety},
  volume={28},
  number={3},
  pages={231--237},
  year={2019},
  publisher={BMJ Publishing Group Ltd}
}
@article{huang2019clinicalbert,
  title={Clinicalbert: Modeling clinical notes and predicting hospital readmission},
  author={Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
  journal={arXiv preprint arXiv:1904.05342},
  year={2019}
}
@article{woo2019ai,
  title={An AI boost for clinical trials.},
  author={Woo, Marcus},
  journal={Nature},
  volume={573},
  number={7775},
  pages={S100--S100},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{ali2020virtual,
  title={Virtual clinical trials: Perspectives in dermatology},
  author={Ali, Zarqa and Zibert, John Robert and Thomsen, Simon Francis},
  journal={Dermatology},
  volume={236},
  number={4},
  pages={375--382},
  year={2020},
  publisher={Karger Publishers}
}
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}
@article{alipanahi2015predicting,
  title={Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning},
  author={Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey, Brendan J},
  journal={Nature biotechnology},
  volume={33},
  number={8},
  pages={831--838},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{yang2020improved,
  title={Improved protein structure prediction using predicted interresidue orientations},
  author={Yang, Jianyi and Anishchenko, Ivan and Park, Hahnbeom and Peng, Zhenling and Ovchinnikov, Sergey and Baker, David},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={3},
  pages={1496--1503},
  year={2020},
  publisher={National Acad Sciences}
}

@article{jumper2020high,
  title={High accuracy protein structure prediction using deep learning},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Tunyasuvunakool, Kathryn and Ronneberger, Olaf and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Bridgland, Alex and others},
  journal={Fourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book)},
  volume={22},
  pages={24},
  year={2020}
}
@article{ji2021dnabert,
  title={DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome.},
  author={Ji, Y and Zhou, Z and Liu, H and Davuluri, RV},
  journal={Bioinformatics},
  year={2021}
}
@article{chaitanya2020contrastive,
  title={Contrastive learning of global and local features for medical image segmentation with limited annotations},
  author={Chaitanya, Krishna and Erdil, Ertunc and Karani, Neerav and Konukoglu, Ender},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{wang2015kernel,
  title={Kernel methods for large-scale genomic data analysis},
  author={Wang, Xuefeng and Xing, Eric P and Schaid, Daniel J},
  journal={Briefings in bioinformatics},
  volume={16},
  number={2},
  pages={183--192},
  year={2015},
  publisher={Oxford University Press}
}
@article{wang2014drug,
  title={Drug repositioning by integrating target information through a heterogeneous network model},
  author={Wang, Wenhui and Yang, Sen and Zhang, Xiang and Li, Jing},
  journal={Bioinformatics},
  volume={30},
  number={20},
  pages={2923--2930},
  year={2014},
  publisher={Oxford University Press}
}
@article{kong2011integrative,
  title={Integrative, multimodal analysis of glioblastoma using TCGA molecular data, pathology images, and clinical outcomes},
  author={Kong, Jun and Cooper, Lee AD and Wang, Fusheng and Gutman, David A and Gao, Jingjing and Chisolm, Candace and Sharma, Ashish and Pan, Tony and Van Meir, Erwin G and Kurc, Tahsin M and others},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={58},
  number={12},
  pages={3469--3474},
  year={2011},
  publisher={IEEE}
}
%C
@article{ribeiro2012classification,
  title={Classification and staging of chronic liver disease from multimodal data},
  author={Ribeiro, Ricardo T and Marinho, Rui Tato and Sanches, J Miguel},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={60},
  number={5},
  pages={1336--1344},
  year={2012},
  publisher={IEEE}
}
@article{aerts2006gene,
  title={Gene prioritization through genomic data fusion},
  author={Aerts, Stein and Lambrechts, Diether and Maity, Sunit and Van Loo, Peter and Coessens, Bert and De Smet, Frederik and Tranchevent, Leon-Charles and De Moor, Bart and Marynen, Peter and Hassan, Bassem and others},
  journal={Nature Biotechnology},
  volume={24},
  number={5},
  pages={537--544},
  year={2006},
  publisher={Nature Publishing Group}
}
@article{lanckriet2004statistical,
  title={A statistical framework for genomic data fusion},
  author={Lanckriet, Gert RG and De Bie, Tijl and Cristianini, Nello and Jordan, Michael I and Noble, William Stafford},
  journal={Bioinformatics},
  volume={20},
  number={16},
  pages={2626--2635},
  year={2004},
  publisher={Oxford University Press}
}
@article{schenone2013target,
  title={Target identification and mechanism of action in chemical biology and drug discovery},
  author={Schenone, Monica and Dan{\v{c}}{\'\i}k, Vlado and Wagner, Bridget K and Clemons, Paul A},
  journal={Nature Chemical Biology},
  volume={9},
  number={4},
  pages={232--240},
  year={2013},
  publisher={Nature Publishing Group}
}
@article{schneider2018automating,
  title={Automating drug discovery},
  author={Schneider, Gisbert},
  journal={Nature Reviews Drug Discovery},
  volume={17},
  number={2},
  pages={97--113},
  year={2018},
  publisher={Nature Publishing Group}
}
@article{walters2020applications,
  title={Applications of deep learning in molecule generation and molecular property prediction},
  author={Walters, W Patrick and Barzilay, Regina},
  journal={Accounts of Chemical Research},
  volume={54},
  number={2},
  pages={263--270},
  year={2020},
  publisher={ACS Publications}
}
@article{stokes2020deep,
  title={A deep learning approach to antibiotic discovery},
  author={Stokes, Jonathan M and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M and MacNair, Craig R and French, Shawn and Carfrae, Lindsey A and Bloom-Ackermann, Zohar and others},
  journal={Cell},
  volume={180},
  number={4},
  pages={688--702},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{jin2018junction,
  title={Junction tree variational autoencoder for molecular graph generation},
  author={Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={International Conference on Machine Learning},
  pages={2323--2332},
  year={2018},
  organization={PMLR}
}
@inproceedings{you2018graph,
  author = {You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
  title = {Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation},
  year = {2018},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages = {6412–6422},
  numpages = {11},
}
@article{huang2021therapeutics,
  title={Therapeutics data commons: machine learning datasets and tasks for therapeutics},
  author={Huang, Kexin and Fu, Tianfan and Gao, Wenhao and Zhao, Yue and Roohani, Yusuf and Leskovec, Jure and Coley, Connor W and Xiao, Cao and Sun, Jimeng and Zitnik, Marinka},
  journal={arXiv preprint arXiv:2102.09548},
  year={2021}
}
@article{camacho2018next,
  title={Next-generation machine learning for biological networks},
  author={Camacho, Diogo M and Collins, Katherine M and Powers, Rani K and Costello, James C and Collins, James J},
  journal={Cell},
  volume={173},
  number={7},
  pages={1581--1592},
  year={2018},
  publisher={Elsevier}
}
@article{duran2020extending,
  title={Extending the small-molecule similarity principle to all levels of biology with the {Chemical Checker}},
  author={Duran-Frigola, Miquel and Pauls, Eduardo and Guitart-Pla, Oriol and Bertoni, Martino and Alcalde, V{\'\i}ctor and Amat, David and Juan-Blanco, Teresa and Aloy, Patrick},
  journal={Nature Biotechnology},
  volume={38},
  number={9},
  pages={1087--1096},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{ramsundar2015massively,
  author    = {Bharath Ramsundar and
               Steven M. Kearnes and
               Patrick Riley and
               Dale Webster and
               David E. Konerding and
               Vijay S. Pande},
  title     = {Massively Multitask Networks for Drug Discovery},
  journal   = {CoRR},
  volume    = {abs/1502.02072},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.02072},
  archivePrefix = {arXiv},
  eprint    = {1502.02072},
  timestamp = {Thu, 21 Nov 2019 15:29:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/RamsundarKRWKP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{ashley2016towards,
  title={Towards precision medicine},
  author={Ashley, Euan A},
  journal={Nature Reviews Genetics},
  volume={17},
  number={9},
  pages={507--522},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{collins2015new,
  title={A new initiative on precision medicine},
  author={Collins, Francis S and Varmus, Harold},
  journal={New England Journal of Medicine},
  volume={372},
  number={9},
  pages={793--795},
  year={2015},
  publisher={Mass Medical Soc}
}
@article{bera2019artificial,
  title={Artificial intelligence in digital pathology—new tools for diagnosis and precision oncology},
  author={Bera, Kaustav and Schalper, Kurt A and Rimm, David L and Velcheti, Vamsidhar and Madabhushi, Anant},
  journal={Nature Reviews Clinical Oncology},
  volume={16},
  number={11},
  pages={703--715},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{gerstung2017precision,
  title={Precision oncology for acute myeloid leukemia using a knowledge bank approach},
  author={Gerstung, Moritz and Papaemmanuil, Elli and Martincorena, Inigo and Bullinger, Lars and Gaidzik, Verena I and Paschka, Peter and Heuser, Michael and Thol, Felicitas and Bolli, Niccolo and Ganly, Peter and others},
  journal={Nature Genetics},
  volume={49},
  number={3},
  pages={332--340},
  year={2017},
  publisher={Nature Publishing Group}
}
@article{grinfeld2018classification,
  title={Classification and personalized prognosis in myeloproliferative neoplasms},
  author={Grinfeld, Jacob and Nangalia, Jyoti and Baxter, E Joanna and Wedge, David C and Angelopoulos, Nicos and Cantrill, Robert and Godfrey, Anna L and Papaemmanuil, Elli and Gundem, Gunes and MacLean, Cathy and others},
  journal={New England Journal of Medicine},
  volume={379},
  number={15},
  pages={1416--1430},
  year={2018},
  publisher={Mass Medical Soc}
}
@article{adam2020machine,
  title={Machine learning approaches to drug response prediction: challenges and recent progress},
  author={Adam, George and Ramp{\'a}{\v{s}}ek, Ladislav and Safikhani, Zhaleh and Smirnov, Petr and Haibe-Kains, Benjamin and Goldenberg, Anna},
  journal={NPJ Precision Oncology},
  volume={4},
  number={1},
  pages={1--10},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{rajkomar2018scalable,
  title={Scalable and accurate deep learning with electronic health records},
  author={Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M and Hajaj, Nissan and Hardt, Michaela and Liu, Peter J and Liu, Xiaobing and Marcus, Jake and Sun, Mimi and others},
  journal={NPJ Digital Medicine},
  volume={1},
  number={1},
  pages={1--10},
  year={2018},
  publisher={Nature Publishing Group}
}
@inproceedings{yasunaga2021break,
  title={Break-It-Fix-It: Unsupervised Learning for Program Repair},
  author={Yasunaga, Michihiro and Liang, Percy},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}
@article{gottlieb2011predict,
  title={{PREDICT}: a method for inferring novel drug indications with application to personalized medicine},
  author={Gottlieb, Assaf and Stein, Gideon Y and Ruppin, Eytan and Sharan, Roded},
  journal={Molecular Systems Biology},
  volume={7},
  number={1},
  pages={496},
  year={2011},
  publisher={John Wiley \& Sons, Ltd},
  address = {Chichester, UK}
}
@article{martinez2020ethical,
  title={Ethical issues in using ambient intelligence in health-care settings},
  author={Martinez-Martin, Nicole and Luo, Zelun and Kaushal, Amit and Adeli, Ehsan and Haque, Albert and Kelly, Sara S and Wieten, Sarah and Cho, Mildred K and Magnus, David and Fei-Fei, Li and others},
  journal={The Lancet Digital Health},
  year={2020},
  publisher={Elsevier}
}
@article{kaushal2020geographic,
  title={Geographic distribution of US cohorts used to train deep learning algorithms},
  author={Kaushal, Amit and Altman, Russ and Langlotz, Curt},
  journal={Jama},
  volume={324},
  number={12},
  pages={1212--1213},
  year={2020},
  publisher={American Medical Association}
}
@article{lu2021pretrained,
  title={Pretrained transformers as universal computation engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}
@article{steinberg2021language,
  title={Language models are an effective representation learning technique for electronic health record data},
  author={Steinberg, Ethan and Jung, Ken and Fries, Jason A and Corbin, Conor K and Pfohl, Stephen R and Shah, Nigam H},
  journal={Journal of Biomedical Informatics},
  volume={113},
  pages={103637},
  year={2021},
  publisher={Elsevier}
}
@article{krumholz2016data,
  title={Data acquisition, curation, and use for a continuously learning health system},
  author={Krumholz, Harlan M and Terry, Sharon F and Waldstreicher, Joanne},
  journal={Jama},
  volume={316},
  number={16},
  pages={1669--1670},
  year={2016},
  publisher={American Medical Association}
}
@article{percha2021modern,
  title={Modern Clinical Text Mining: A Guide and Review},
  author={Percha, Bethany},
  journal={Annual Review of Biomedical Data Science},
  volume={4},
  year={2021},
  publisher={Annual Reviews}
}
@article{benam2019exploring,
  title={Exploring new technologies in biomedical research},
  author={Benam, Kambez H and Gilchrist, Siobhan and Kleensang, Andre and Satz, Ani B and Willett, Catherine and Zhang, Qiang},
  journal={Drug discovery today},
  volume={24},
  number={6},
  pages={1242--1247},
  year={2019},
  publisher={Elsevier}
}
@article{jaroch2018cell,
  title={Cell cultures in drug discovery and development: The need of reliable in vitro-in vivo extrapolation for pharmacodynamics and pharmacokinetics assessment},
  author={Jaroch, Karol and Jaroch, Alina and Bojko, Barbara},
  journal={Journal of Pharmaceutical and Biomedical Analysis},
  volume={147},
  pages={297--312},
  year={2018},
  publisher={Elsevier}
}
@article{diana2015robotic,
  title={Robotic surgery},
  author={Diana, M and Marescaux, JJBJoS},
  journal={Journal of British Surgery},
  volume={102},
  number={2},
  pages={e15--e28},
  year={2015},
  publisher={Oxford University Press}
}
@inproceedings{agrigoroaie2016developing,
  title={Developing a healthcare robot with personalized behaviors and social skills for the elderly},
  author={Agrigoroaie, Roxana M and Tapus, Adriana},
  booktitle={2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={589--590},
  year={2016},
  organization={IEEE}
}
@article{yu2019reinforcement,
  title={Reinforcement learning in healthcare: A survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim},
  journal={arXiv preprint arXiv:1908.08796},
  year={2019}
}
@article{chamikara2021privacy,
  title={Privacy preserving distributed machine learning with federated learning},
  author={Chamikara, Mahawaga Arachchige Pathum and Bertok, Peter and Khalil, Ibrahim and Liu, Dongxi and Camtepe, Seyit},
  journal={Computer Communications},
  volume={171},
  pages={112--125},
  year={2021},
  publisher={Elsevier}
}
%% healthcare interface for patients
@article{bates2019health,
  title={Health care chatbots are here to help},
  author={Bates, Mary},
  journal={IEEE pulse},
  volume={10},
  number={3},
  pages={12--14},
  year={2019},
  publisher={IEEE}
}
@article{abdi2018scoping,
  title={Scoping review on the use of socially assistive robot technology in elderly care},
  author={Abdi, Jordan and Al-Hindawi, Ahmed and Ng, Tiffany and Vizcaychipi, Marcela P},
  journal={BMJ open},
  volume={8},
  number={2},
  pages={e018815},
  year={2018},
  publisher={British Medical Journal Publishing Group}
}
@inproceedings{jeong2015designing,
  title={Designing a socially assistive robot for pediatric care},
  author={Jeong, Sooyeon and Santos, Kristopher Dos and Graca, Suzanne and O'Connell, Brianna and Anderson, Laurel and Stenquist, Nicole and Fitzpatrick, Katie and Goodenough, Honey and Logan, Deirdre and Weinstock, Peter and others},
  booktitle={Proceedings of the 14th international conference on interaction design and children},
  pages={387--390},
  year={2015}
}
@article{herriman2020asked,
  title={Asked and answered: building a chatbot to address Covid-19-related concerns},
  author={Herriman, Maguire and Meer, Elana and Rosin, Roy and Lee, Vivian and Washington, Vindell and Volpp, Kevin G},
  journal={Nejm Catalyst Innovations in Care Delivery},
  year={2020},
  publisher={NEJM Group}
}
@article{chaix2019chatbots,
  title={When chatbots meet patients: one-year prospective study of conversations between patients with breast cancer and a chatbot},
  author={Chaix, Benjamin and Bibault, Jean-Emmanuel and Pienkowski, Arthur and Delamon, Guillaume and Guillemass{\'e}, Arthur and Nectoux, Pierre and Brouard, Beno{\^\i}t},
  journal={JMIR cancer},
  volume={5},
  number={1},
  pages={e12856},
  year={2019},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

%% interface for doctors
@article{shah2020surgical,
  title={Surgical data recording technology: a solution to address medical errors?},
  author={Shah, Neal A and Jue, Jessica and Mackey, Tim K},
  journal={Annals of surgery},
  volume={271},
  number={3},
  pages={431--433},
  year={2020},
  publisher={LWW}
}


%%Giray
@article{van_Hartskamp_2019,
   title={Artificial Intelligence in Clinical Health Care Applications: Viewpoint},
   volume={8},
   ISSN={1929-073X},
   url={http://dx.doi.org/10.2196/12100},
   DOI={10.2196/12100},
   number={2},
   journal={Interactive Journal of Medical Research},
   publisher={JMIR Publications Inc.},
   author={van Hartskamp, Michael and Consoli, Sergio and Verhaegh, Wim and Petkovic, Milan and van de Stolpe, Anja},
   year={2019},
   month={Apr},
   pages={e12100}
}
@article{korngiebel2021considering,
  title={Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery},
  author={Korngiebel, Diane M and Mooney, Sean D},
  journal={NPJ Digital Medicine},
  volume={4},
  number={1},
  pages={1--3},
  year={2021},
  publisher={Nature Publishing Group}
}
@article{ionescu2020deep,
  title={Deep learning algorithms and big health care data in clinical natural language processing},
  author={Ionescu, Daniela and others},
  journal={Linguistic and Philosophical Investigations},
  number={19},
  pages={86--92},
  year={2020},
  publisher={Addleton Academic Publishers}
}
@inproceedings{elbattah2021role,
  title={The Role of Text Analytics in Healthcare: A Review of Recent Developments and Applications.},
  author={Elbattah, Mahmoud and Arnaud, {\'E}milien and Gignon, Maxime and Dequen, Gilles},
  booktitle={HEALTHINF},
  pages={825--832},
  year={2021}
}
@article{guan2019artificial,
  title={Artificial intelligence in healthcare and medicine: promises, ethical challenges and governance},
  author={Guan, Jian},
  journal={Chinese Medical Sciences Journal},
  volume={34},
  number={2},
  pages={76--83},
  year={2019},
  publisher={Elsevier}
}
@article{xu2019translating,
  title={Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges and future perspectives},
  author={Xu, Jia and Yang, Pengwei and Xue, Shang and Sharma, Bhuvan and Sanchez-Martin, Marta and Wang, Fang and Beaty, Kirk A and Dehan, Elinor and Parikh, Baiju},
  journal={Human genetics},
  volume={138},
  number={2},
  pages={109--124},
  year={2019},
  publisher={Springer}
}

%% Jing: for the intro
@article{swensen2011controlling,
  title={Controlling healthcare costs by removing waste: what American doctors can do now},
  author={Swensen, Stephen J and Kaplan, Gary S and Meyer, Gregg S and Nelson, Eugene C and Hunt, Gordon C and Pryor, David B and Weissberg, Jed I and Daley, Jennifer and Yates, Gary R and Chassin, Mark R},
  journal={BMJ quality \& safety},
  volume={20},
  number={6},
  pages={534--537},
  year={2011},
  publisher={BMJ Publishing Group Ltd}
}
@incollection{soltanian2019multimodal,
  title={Multimodal Analysis in Biomedicine},
  author={Soltanian-Zadeh, Hamid},
  booktitle={Big Data in Multimodal Medical Imaging},
  pages={193--203},
  year={2019},
  publisher={Chapman and Hall/CRC}
}
@book{suresh2020deep,
  title={Deep neural networks for multimodal imaging and biomedical applications},
  author={Suresh, Annamalai and Udendhran, R and Vimal, S},
  year={2020},
  publisher={IGI Global}
}
@inproceedings{zhu2019hierarchical,
  title={A hierarchical attention retrieval model for healthcare question answering},
  author={Zhu, Ming and Ahuja, Aman and Wei, Wei and Reddy, Chandan K},
  booktitle={The World Wide Web Conference},
  pages={2472--2482},
  year={2019}
}
@inproceedings{daniel2019towards,
  title={Towards automating healthcare question answering in a noisy multilingual low-resource setting},
  author={Daniel, Jeanne E and Brink, Willie and Eloff, Ryan and Copley, Charles},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={948--953},
  year={2019}
}
@article{liu2020interpretable,
  title={Interpretable Multi-Step Reasoning with Knowledge Extraction on Complex Healthcare Question Answering},
  author={Liu, Ye and Chowdhury, Shaika and Zhang, Chenwei and Caragea, Cornelia and Yu, Philip S},
  journal={arXiv preprint arXiv:2008.02434},
  year={2020}
}
@article{ni2015automated,
  title={Automated clinical trial eligibility prescreening: increasing the efficiency of patient identification for clinical trials in the emergency department},
  author={Ni, Yizhao and Kennebeck, Stephanie and Dexheimer, Judith W and McAneney, Constance M and Tang, Huaxiu and Lingren, Todd and Li, Qi and Zhai, Haijun and Solti, Imre},
  journal={Journal of the American Medical Informatics Association},
  volume={22},
  number={1},
  pages={166--178},
  year={2015},
  publisher={Oxford University Press}
}
@article{beck2020artificial,
  title={Artificial intelligence tool for optimizing eligibility screening for clinical trials in a large community cancer center},
  author={Beck, J Thaddeus and Rammage, Melissa and Jackson, Gretchen P and Preininger, Anita M and Dankwa-Mullan, Irene and Roebuck, M Christopher and Torres, Adam and Holtzen, Helen and Coverdill, Sadie E and Williamson, M Paul and others},
  journal={JCO clinical cancer informatics},
  volume={4},
  pages={50--59},
  year={2020},
  publisher={American Society of Clinical Oncology}
}
@article{miura2020improving,
  title={Improving factual completeness and consistency of image-to-text radiology report generation},
  author={Miura, Yasuhide and Zhang, Yuhao and Tsai, Emily Bao and Langlotz, Curtis P and Jurafsky, Dan},
  journal={NAACL},
  year={2021}
}
@article{li2021neural,
  title={Neural Natural Language Processing for Unstructured Data in Electronic Health Records: a Review},
  author={Li, Irene and Pan, Jessica and Goldwasser, Jeremy and Verma, Neha and Wong, Wai Pan and Nuzumlal{\i}, Muhammed Yavuz and Rosand, Benjamin and Li, Yixin and Zhang, Matthew and Chang, David and others},
  journal={arXiv preprint arXiv:2107.02975},
  year={2021}
}
@inproceedings{fenglincompetence2021,
  title={Competence-based Multimodal Curriculum Learning for Medical Report Generation},
  author={Liu, Fenglin and Ge, Shen and Wu, Xian},
  booktitle={ACL},
  year={2021}
}

% biomedicine - yusuf
@article{hie2021learning,
  title={Learning the language of viral evolution and escape},
  author={Hie, Brian and Zhong, Ellen D and Berger, Bonnie and Bryson, Bryan},
  journal={Science},
  volume={371},
  number={6526},
  pages={284--288},
  year={2021},
  publisher={American Association for the Advancement of Science}
}
@article{bepler2021learning,
  title={Learning the protein language: Evolution, structure, and function},
  author={Bepler, Tristan and Berger, Bonnie},
  journal={Cell Systems},
  volume={12},
  number={6},
  pages={654--669},
  year={2021},
  publisher={Elsevier}
}
@article{tsaban2021harnessing,
  title={Harnessing protein folding neural networks for peptide-protein docking},
  author={Tsaban, Tomer and Varga, Julia K and Avraham, Orly and Aharon, Ziv Ben and Khramushin, Alisa and Schueler-Furman, Ora},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}
@article{wu2021protein,
  title={Protein sequence design with deep generative models},
  author={Wu, Zachary and Johnston, Kadina E and Arnold, Frances H and Yang, Kevin K},
  journal={Current Opinion in Chemical Biology},
  volume={65},
  pages={18--27},
  year={2021},
  publisher={Elsevier}
}
@article{xu2020building,
  title={Building a PubMed knowledge graph},
  author={Xu, Jian and Kim, Sunkyu and Song, Min and Jeong, Minbyul and Kim, Donghyeon and Kang, Jaewoo and Rousseau, Justin F and Li, Xin and Xu, Weijia and Torvik, Vetle I and others},
  journal={Scientific data},
  volume={7},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{jin2021biomedical,
  title={Biomedical question answering: A comprehensive review},
  author={Jin, Qiao and Yuan, Zheng and Xiong, Guangzhi and Yu, Qianlan and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Liu, Xiaozhong and Yu, Sheng},
  journal={arXiv preprint arXiv:2102.05281},
  year={2021}
}
@article{chen2020treating,
  title={Treating health disparities with artificial intelligence},
  author={Chen, Irene Y and Joshi, Shalmali and Ghassemi, Marzyeh},
  journal={Nature medicine},
  volume={26},
  number={1},
  pages={16--17},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{chen2019can,
  title={Can AI help reduce disparities in general medical and mental health care?},
  author={Chen, Irene Y and Szolovits, Peter and Ghassemi, Marzyeh},
  journal={AMA journal of ethics},
  volume={21},
  number={2},
  pages={167--179},
  year={2019},
  publisher={American Medical Association}
}
@article{wiens2019no,
  title={Do no harm: a roadmap for responsible machine learning for health care},
  author={Wiens, Jenna and Saria, Suchi and Sendak, Mark and Ghassemi, Marzyeh and Liu, Vincent X and Doshi-Velez, Finale and Jung, Kenneth and Heller, Katherine and Kale, David and Saeed, Mohammed and others},
  journal={Nature medicine},
  volume={25},
  number={9},
  pages={1337--1340},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard S},
  journal={arXiv preprint arXiv:1703.05175},
  year={2017}
}
@article{ma2021few,
  title={Few-shot learning creates predictive models of drug response that translate from high-throughput screens to individual patients},
  author={Ma, Jianzhu and Fong, Samson H and Luo, Yunan and Bakkenist, Christopher J and Shen, John Paul and Mourragui, Soufiane and Wessels, Lodewyk FA and Hafner, Marc and Sharan, Roded and Peng, Jian and others},
  journal={Nature Cancer},
  volume={2},
  number={2},
  pages={233--244},
  year={2021},
  publisher={Nature Publishing Group}
}
@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  year={2021},
  publisher={National Acad Sciences}
}
%%%%%% END healthcare %%%%%%



%%%%%%  BEGIN DATA

@inproceedings{alper2013enhancing,
  title={Enhancing and abstracting scientific workflow provenance for data publishing},
  author={Alper, Pinar and Belhajjame, Khalid and Goble, Carole A and Karagoz, Pinar},
  booktitle={Proceedings of the Joint EDBT/ICDT 2013 Workshops},
  pages={313--318},
  year={2013}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{jo2020lessons,
  title={Lessons from archives: Strategies for collecting sociocultural data in machine learning},
  author={Jo, Eun Seo and Gebru, Timnit},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={306--316},
  year={2020}
}

% @article{blodgett2020language,
%   title={Language (technology) is power: A critical survey of" bias" in nlp},
%   author={Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
%   journal={arXiv preprint arXiv:2005.14050},
%   year={2020}
% }

@article{chang2020adversarial,
  title={On adversarial bias and the robustness of fair machine learning},
  author={Chang, Hongyan and Nguyen, Ta Duy and Murakonda, Sasi Kumar and Kazemi, Ehsan and Shokri, Reza},
  journal={arXiv preprint arXiv:2006.08669},
  year={2020}
}

@article{bandy2021addressing,
  title={Addressing" Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for BookCorpus},
  author={Bandy, Jack and Vincent, Nicholas},
  journal={arXiv preprint arXiv:2105.05241},
  year={2021}
}

@article{evans2011much,
  title={Much ado about data ownership},
  author={Evans, Barbara J},
  journal={Harv. JL \& Tech.},
  volume={25},
  pages={69},
  year={2011},
  publisher={HeinOnline}
}

@misc{gdprrightsforgotten,  
	title={Art. 17 GDPR: Right to erasure (‘right to be forgotten’)},
	howpublished={https://gdpr-info.eu/art-17-gdpr/}}
}

@misc{ca_bill_713,
	title={California State, Legislature. Assembly Bill No. 713},
	howpublished={https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=201920200AB713}
}

@article{voigt2017eu,
  title={The eu general data protection regulation (gdpr)},
  author={Voigt, Paul and Von dem Bussche, Axel},
  journal={A Practical Guide, 1st Ed., Cham: Springer International Publishing},
  volume={10},
  pages={3152676},
  year={2017},
  publisher={Springer}
}

@inproceedings{breck2019data,
  title={Data Validation for Machine Learning.},
  author={Breck, Eric and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven and Zinkevich, Martin},
  booktitle={MLSys},
  year={2019}
}

@article{schelter2018automating,
  title={Automating large-scale data quality verification},
  author={Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={12},
  pages={1781--1794},
  year={2018},
  publisher={VLDB Endowment}
}

@inproceedings{sambasivan2021everyone,
  title={“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI},
  author={Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
  booktitle={proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2021}
}


@misc{hfdatasets,  
	title={HuggingFace Datasets},
	howpublished={\url{https://huggingface.co/docs/datasets/master/}},
}

@misc{tf_datasets,  
	title={TensorFlow Datasets},
	howpublished={\url{https://www.tensorflow.org/datasets}},
}

@misc{datagov,  
	title={data.gov},
	howpublished={\url{https://www.data.gov/}},
}

@misc{kafka,  
	title={Apache Kafka},
	howpublished={\url{https://kafka.apache.org/}},
}

@misc{tamr,  
	title={Tamr},
	howpublished={\url{https://www.tamr.com/}},
}

@misc{aws_data_lakes,  
	title={Analytics on AWS},
	howpublished={\url{https://aws.amazon.com/big-data/datalakes-and-analytics/}},
}

@misc{michelangelo,  
	title={Meet Michelangelo: Uber’s Machine Learning Platform},
	howpublished={\url{https://eng.uber.com/michelangelo-machine-learning-platform/}},
	author={Hermann, Jeremy and Del Balso, Mike}
}

@misc{dataworld,  
	title={data.world},
	howpublished={\url{https://data.world/}},
}

@misc{harvarddataverse,  
	title={Harvard Dataverse},
	howpublished={\url{https://dataverse.harvard.edu/dataverse/harvard}},
}

@misc{datacommons,  
	title={Data Commons},
	howpublished={\url{https://datacommons.org/}},
}

@misc{paxata,  
	title={Paxata},
	howpublished={\url{https://www.paxata.com/}},
}

@misc{privacy1974,  
	title={Privacy Act of 1974},
	howpublished={\url{https://www.justice.gov/opcl/privacy-act-1974}},
}

@inproceedings{rekatsinas2017slimfast,
  title={Slimfast: Guaranteed results for data fusion and source reliability},
  author={Rekatsinas, Theodoros and Joglekar, Manas and Garcia-Molina, Hector and Parameswaran, Aditya and R{\'e}, Christopher},
  booktitle={Proceedings of the 2017 ACM International Conference on Management of Data},
  pages={1399--1414},
  year={2017}
}

@misc{lettertogeorgia,
    title={Letter to Georgia Board of Regents re: Open Records Request},
    howpublished={\url{http://www2.ed.gov/policy/gen/guid/fpco/ferpa/library/georgialtr.html}},
}


@misc{zoderworth2019,
    title={How Much Is Your Data Worth?},
    howpublished={\url{https://www.forbes.com/sites/stephanzoder/2019/08/06/how-much-is-your-data-worth}},
    author={Zoder, Stephan},
    year={2019}
}

@misc{marr2017,
    title={Really Big Data At Walmart: Real-Time Insights From Their 40+ Petabyte Data Cloud},
    howpublished={\url{https://www.forbes.com/sites/bernardmarr/2017/01/23/really-big-data-at-walmart-real-time-insights-from-their-40-petabyte-data-cloud}},
    author={Marr, Bernard},
    year={2017}
}


@inproceedings{chen2021mandoline,
  title={Mandoline: Model Evaluation under Distribution Shift},
  author={Chen, Mayee and Goel, Karan and Sohoni, Nimit S and Poms, Fait and Fatahalian, Kayvon and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={1617--1629},
  year={2021},
  organization={PMLR}
}

@inproceedings{ppa,
  title={Resolving PP attachment ambiguities with memory-based learning},
  author={Zavrel, Jakub and Daelemans, Walter and Veenstra, Jorn},
  booktitle={CoNLL97: Computational Natural Language Learning},
  year={1997}
}

@article{gnn,
  title={The graph neural network model},
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE transactions on neural networks},
  volume={20},
  number={1},
  pages={61--80},
  year={2008},
  publisher={IEEE}
}

@inproceedings{kenter2015ad,
  title={Ad hoc monitoring of vocabulary shifts over time},
  author={Kenter, Tom and Wevers, Melvin and Huijnen, Pim and De Rijke, Maarten},
  booktitle={Proceedings of the 24th ACM international on conference on information and knowledge management},
  pages={1191--1200},
  year={2015}
}

@inproceedings{fetahu2015much,
  title={How much is Wikipedia lagging behind news?},
  author={Fetahu, Besnik and Anand, Abhijit and Anand, Avishek},
  booktitle={Proceedings of the ACM Web Science Conference},
  pages={1--9},
  year={2015}
}

@article{linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv e-prints},
  pages={arXiv--2006},
  year={2020}
}


@inproceedings{reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{performer,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@misc{fipps,
    title={The Fair Information Practice Principles},
    howpublished={\url{https://www.dhs.gov/sites/default/files/publications/privacy-policy-guidance-memorandum-2008-01.pdf}},
}

@misc{hf_model_hub,
    title={HuggingFace Model Hub},
    howpublished={\url{https://huggingface.co/models}},
}

@misc{ng_data_centric,
    title={Andrew Ng Launches A Campaign For Data-Centric AI},
    author={Press, Gil},
    howpublished={\url{https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=44865f6a74f5}},
    year={2021}
}

@misc{hazy_data_centric,
    title={The Road to Software 2.0 or Data-Centric AI},
    author={Ré, Christopher},
    howpublished={\url{https://hazyresearch.stanford.edu/data-centric-ai}},
    year={2021}
}

@article{lstms,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{ikeda2010panda,
  title={Panda: A system for provenance and data},
  author={Ikeda, Robert and Widom, Jennifer},
  year={2010},
  publisher={Stanford InfoLab}
}

@article{albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv e-prints},
  pages={arXiv--1909},
  year={2019}
}

@article{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv e-prints},
  pages={arXiv--1907},
  year={2019}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}


@inproceedings{polyzotis2017data,
  title={Data management challenges in production machine learning},
  author={Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  booktitle={Proceedings of the 2017 ACM International Conference on Management of Data},
  pages={1723--1726},
  year={2017}
}

@inproceedings{so2019evolved,
  title={The evolved transformer},
  author={So, David and Le, Quoc and Liang, Chen},
  booktitle={International Conference on Machine Learning},
  pages={5877--5886},
  year={2019},
  organization={PMLR}
}


@inproceedings{amershi2019software,
  title={Software engineering for machine learning: A case study},
  author={Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  pages={291--300},
  year={2019},
  organization={IEEE}
}

@article{grant2021reducing,
  title={Reducing CO2 emissions by targeting the world’s hyper-polluting power plants},
  author={Grant, Don and Zelinka, David and Mitova, Stefania},
  journal={Environmental Research Letters},
  year={2021},
  publisher={IOP Publishing}
}


@inproceedings{chung2019slice,
  title={Slice finder: Automated data slicing for model validation},
  author={Chung, Yeounoh and Kraska, Tim and Polyzotis, Neoklis and Tae, Ki Hyun and Whang, Steven Euijong},
  booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)},
  pages={1550--1553},
  year={2019},
  organization={IEEE}
}

@article{ribeiro2020beyond,
  title={Beyond accuracy: Behavioral testing of NLP models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  journal={arXiv preprint arXiv:2005.04118},
  year={2020}
}

@inproceedings{ghorbani2019data,
  title={Data shapley: Equitable valuation of data for machine learning},
  author={Ghorbani, Amirata and Zou, James},
  booktitle={International Conference on Machine Learning},
  pages={2242--2251},
  year={2019},
  organization={PMLR}
}

@article{agrawal2019cloudy,
  title={Cloudy with high chance of DBMS: A 10-year prediction for Enterprise-Grade ML},
  author={Agrawal, Ashvin and Chatterjee, Rony and Curino, Carlo and Floratou, Avrilia and Gowdal, Neha and Interlandi, Matteo and Jindal, Alekh and Karanasos, Kostantinos and Krishnan, Subru and Kroth, Brian and others},
  journal={arXiv preprint arXiv:1909.00084},
  year={2019}
}

@article{oakden2019hidden,
  title={Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging. arXiv e-prints, art},
  author={Oakden-Rayner, Luke and Dunnmon, Jared and Carneiro, Gustavo and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:1909.12475},
  year={2019}
}

@techreport{armbrust2009above,
  title={Above the clouds: A berkeley view of cloud computing},
  author={Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D and Katz, Randy H and Konwinski, Andrew and Lee, Gunho and Patterson, David A and Rabkin, Ariel and Stoica, Ion and others},
  year={2009},
  institution={Technical Report UCB/EECS-2009-28, EECS Department, University of California~…}
}

@article{ipcc2021,
title = {{IPCC, 2021: Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change}},
year={2021},
author = {Masson-Delmotte, V. and Zhai, P. and Pirani, A. and Connors, S. L. and P/`ean, C. and Berger, S. and Caud, N. and Chen, Y. and  Goldfarb, L. and Gomis, M. I. and Huang, M. and Leitzell, K. and  Lonnoy, E. and Matthews, J. B. R. and Maycock, T. K. and Waterfield, T. and Yelekci, O. and Yu, R. and Zhou (Eds.), B.}
journal={Cambridge University Press}
}


@book{hellerstein2005readings,
  title={Readings in database systems},
  author={Hellerstein, Joseph M and Stonebraker, Michael},
  year={2005},
  publisher={MIT press}
}

@inproceedings{zaharia2012resilient,
  title={Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauly, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  booktitle={9th $\{$USENIX$\}$ Symposium on Networked Systems Design and Implementation ($\{$NSDI$\}$ 12)},
  pages={15--28},
  year={2012}
}

@inproceedings{kemper2011hyper,
  title={HyPer: A hybrid OLTP\&OLAP main memory database system based on virtual memory snapshots},
  author={Kemper, Alfons and Neumann, Thomas},
  booktitle={2011 IEEE 27th International Conference on Data Engineering},
  pages={195--206},
  year={2011},
  organization={IEEE}
}

@article{stonebraker2013voltdb,
  title={The VoltDB Main Memory DBMS.},
  author={Stonebraker, Michael and Weisberg, Ariel},
  journal={IEEE Data Eng. Bull.},
  volume={36},
  number={2},
  pages={21--27},
  year={2013}
}

@article{stonebraker2018data,
  title={Data Integration: The Current Status and the Way Forward.},
  author={Stonebraker, Michael and Ilyas, Ihab F and others},
  journal={IEEE Data Eng. Bull.},
  volume={41},
  number={2},
  pages={3--9},
  year={2018}
}

@inproceedings{vartak2016modeldb,
  title={ModelDB: a system for machine learning model management},
  author={Vartak, Manasi and Subramanyam, Harihar and Lee, Wei-En and Viswanathan, Srinidhi and Husnoo, Saadiyah and Madden, Samuel and Zaharia, Matei},
  booktitle={Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
  pages={1--3},
  year={2016}
}

@article{cudre2009demonstration,
  title={A demonstration of SciDB: a science-oriented DBMS},
  author={Cudr{\'e}-Mauroux, Philippe and Kimura, Hideaki and Lim, K-T and Rogers, Jennie and Simakov, Roman and Soroush, Emad and Velikhov, Pavel and Wang, Daniel L and Balazinska, Magdalena and Becla, Jacek and others},
  journal={Proceedings of the VLDB Endowment},
  volume={2},
  number={2},
  pages={1534--1537},
  year={2009},
  publisher={VLDB Endowment}
}

@article{keskar2019ctrl,
  title={Ctrl: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@article{lynchlanguage,
  title={Language Conditioned Imitation Learning over Unstructured Data},
  author={Lynch, Corey and Sermanet, Pierre},
  booktitle={Robotics: Science and Systems},
  year={2021}
}

@misc{ma2019nlpaug,
  title={NLP Augmentation},
  author={Edward Ma},
  howpublished={https://github.com/makcedward/nlpaug},
  year={2019}
}

@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={Springer}
}

@inproceedings{hollenstein-etal-2020-zuco,
    title = "{Z}u{C}o 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation",
    author = "Hollenstein, Nora  and
      Troendle, Marius  and
      Zhang, Ce  and
      Langer, Nicolas",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    pages = "138--146",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{dong2020multi,
  title={Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web},
  author={Dong, Xin Luna and Hajishirzi, Hannaneh and Lockard, Colin and Shiralkar, Prashant},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3543--3544},
  year={2020}
}

@article{liu2021pay,
  title={Pay Attention to MLPs},
  author={Liu, Hanxiao and Dai, Zihang and So, David R and Le, Quoc V},
  journal={arXiv preprint arXiv:2105.08050},
  year={2021}
}


@misc{datascience2020,
    title={11 Data Science Careers Shaping Our Future},
    author={Kelsey Miller},
    howpublished={\url{https://www.northeastern.edu/graduate/blog/data-science-careers-shaping-our-future/}},
    year={2020}
}

@article{rekatsinas2017holoclean,
  title={Holoclean: Holistic data repairs with probabilistic inference},
  author={Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F. and Ré, Christopher},
  journal={Proceedings of the VLDB Endowment (PVLDB)},
  year={2017},
}

@inproceedings{orr2020bootleg,
 author    = {Laurel Orr, and Megan Leszczynski, and Simran Arora, and Sen Wu, and Neel Guha, and Xiao Ling, and Chris Ré},
 title     = {Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation},
 year      = {2020},
 booktitle     = {Arxiv},
}

@inproceedings{logeswaran2019entdesc,
  title={Zero-Shot Entity Linking by Reading Entity Descriptions},
  author={Lajanugen Logeswaran and Ming-Wei Chang and Kenton Lee and Kristina Toutanova and Jacob Devlin and Honglak Lee},
  booktitle={arXiv:1906.07348v1},
  year={2019}
}

@article{ratner2017snorkel,
  title={Snorkel: Rapid Training Data Creation with Weak Supervision},
  author={Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
  journal={Proceedings of the VLDB Endowment (PVLDB)},
  year={2017},
}

@article{kiela2021dynabench,
  title={Dynabench: Rethinking benchmarking in NLP},
  author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
  journal={arXiv preprint arXiv:2104.14337},
  year={2021}
}

@article{Stonebraker2018dataintegration,
  title={Data Integration: The Current Status and the Way Forward},
  author={Stonebraker, Michael and Ilyas, Ihab F.},
  journal={IEEE Computer Society Technical Committee on Data Engineering},
  year={2018},
}

@inproceedings{hohman2020understanding,
  title={Understanding and visualizing data iteration in machine learning},
  author={Hohman, Fred and Wongsuphasawat, Kanit and Kery, Mary Beth and Patel, Kayur},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2020}
}

@article{short2017s,
  title={What's your data worth?},
  author={Short, James and Todd, Steve},
  journal={MIT Sloan Management Review},
  volume={58},
  number={3},
  pages={17},
  year={2017},
  publisher={Massachusetts Institute of Technology, Cambridge, MA}
}

@article{masur2018datalicensing,
  title={Data Licensing—Tips and Tactics},
  author={Masur, Daniel},
  journal={Corporate Compliance Insights},
  year={2018}
}

@article{hohman2018visual,
  title={Visual analytics in deep learning: An interrogative survey for the next frontiers},
  author={Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={8},
  pages={2674--2693},
  year={2018},
  publisher={IEEE}
}

@article{re2019overton,
  title={Overton: A data system for monitoring and improving machine-learned products},
  author={R{\'e}, Christopher and Niu, Feng and Gudipati, Pallavi and Srisuwananukorn, Charles},
  journal={arXiv preprint arXiv:1909.05372},
  year={2019}
}


@article{abiteboul1997semistructureddata,
  title={Querying Semi-Structured Data},
  author={Abiteboul, Serge},
  journal={International Conference on Database Theory},
  year={1997},
}

@inproceedings{feldman2021tailmemorization,
  title={Does Learning Require Memorization? A Short Tale about a Long Tail},
  author={Vitaly Feldman},
  booktitle={arXiv:1906.05271v4},
  year={2021}
}

@inproceedings{zhu2014objecttail,
  title={Capturing long-tail distributions of object subcategories},
  author={Xiangxin Zhu and Dragomir Anguelov and Deva Ramanan},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{horn2017wildtail,
  title={The Devil is in the Tails: Fine-grained Classification in the Wild},
  author={Grant Van Horn and Pietro Perona},
  booktitle={arXiv:1709.01450v1 },
  year={2017}
}

@inproceedings{babbar2018scarcitytail,
  title={Data scarcity, robustness and extreme multi-label classification},
  author={Rohit Babbar and Bernhard Schölkopf},
  booktitle={Machine Learning},
  year={2019}
}

@article{goel2021robustnessgym,
    title={Robustness Gym: Unifying the NLP Evaluation Landscape},
    author={Karan Goel and Nazneen Rajani and Jesse Vig and Samson Tan and Jason Wu and Stephan Zheng and Caiming Xiong and Mohit Bansal and Christopher Ré},
    journal={arXiv preprint arXiv:2101.04840},
    year={2021}
}


% KNOWLEDGE LMS
@article{perner2020ebert,
  title={E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT},
  author={Nina Poerner and Ulli Waltinger and Hinrich Schutze},
  journal={arXiv:1911.03681v2},
  year={2020}
}

@article{wang2020kepler,
  title={KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation},
  author={Xiaozhi Wang and Tianyu Gao and Zhaocheng Zhu and Zhengyan Zhang and Zhiyuan Liu and Juanzi Li and Jian Tang},
  journal={arXiv:1911.06136v3},
  year={2020}
}

@article{yamada2020luke,
  title={LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention},
  author={Ikuya Yamada and Akari Asai and Hiroyuki Shindo and Hideaki Takeda and Yuji Matsumoto},
  journal={EMNLP 2020},
  year={2020}
}

@inproceedings{xiong20wklm,
  title={{Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model}},
  author={Wenhan Xiong and Jingfei Du and William Yang Wang and Veselin Stoyanov},
  booktitle={International Conference on Learning Representations (ICLR) 2020},
  year={2020}
}

@inproceedings{logan2019kglm,
  title={Barack's Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling},
  author={Robert L. Logan and Nelson F. Liu and Matthew E. Peters and Matt Gardner and Sameer Singh},
  booktitle={ACL 2019},
  year={2019}
}


@misc{tableau_public,
    title={Tableau Public},
    howpublished={\url{https://public.tableau.com/en-us/s/about}},
}

@inproceedings{chen2015access,
  title={An access control model for protecting provenance graphs},
  author={Chen, Liang and Edwards, Peter and Nelson, John D and Norman, Timothy J},
  booktitle={2015 13th Annual Conference on Privacy, Security and Trust (PST)},
  pages={125--132},
  year={2015},
  organization={IEEE}
}

@article{lazaridou2021pitfalls,
  title={Pitfalls of Static Language Modelling},
  author={Lazaridou, Angeliki and Kuncoro, Adhiguna and Gribovskaya, Elena and Agrawal, Devang and Liska, Adam and Terzi, Tayfun and Gimenez, Mai and d'Autume, Cyprien de Masson and Ruder, Sebastian and Yogatama, Dani and others},
  journal={arXiv preprint arXiv:2102.01951},
  year={2021}
}

@article{alt2020tacred,
  title={TACRED revisited: A thorough evaluation of the TACRED relation extraction task},
  author={Alt, Christoph and Gabryszak, Aleksandra and Hennig, Leonhard},
  journal={arXiv preprint arXiv:2004.14855},
  year={2020}
}

@inproceedings{fernandez2018aurum,
  title={Aurum: A data discovery system},
  author={Fernandez, Raul Castro and Abedjan, Ziawasch and Koko, Famien and Yuan, Gina and Madden, Samuel and Stonebraker, Michael},
  booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)},
  pages={1001--1012},
  year={2018},
  organization={IEEE}
}

@inproceedings{bailis2017macrobase,
  title={MacroBase: Prioritizing Attention in Fast Data},
  author={Bailis, Peter and Gan, Edward and Madden, Samuel and Narayanan, Deepak and Rong, Kexin and Suri, Sahaana},
  booktitle={SIGMOD},
  year={2017},
}

@inproceedings{moritz2017trust,
  title={Trust, but verify: Optimistic visualizations of approximate queries for exploring big data},
  author={Moritz, Dominik and Fisher, Danyel and Ding, Bolin and Wang, Chi},
  booktitle={Proceedings of the 2017 CHI conference on human factors in computing systems},
  pages={2904--2915},
  year={2017}
}

@article{carlini2021poisoning,
  title={Poisoning and Backdooring Contrastive Learning},
  author={Carlini, Nicholas and Terzis, Andreas},
  journal={arXiv preprint arXiv:2106.09667},
  year={2021}
}

@inproceedings{carlini2021poisoningssl,
  title={Poisoning the Unlabeled Dataset of Semi-Supervised Learning},
  author={Carlini, Nicholas},
  booktitle={USENIX Security Symposium},
  year={2021}
}

@article{goh2021multimodal,
  author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  title = {Multimodal Neurons in Artificial Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/multimodal-neurons},
  doi = {10.23915/distill.00030}
}

@misc{lee2021deduplicating,
      title={Deduplicating Training Data Makes Language Models Better}, 
      author={Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini},
      year={2021},
      eprint={2107.06499},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%%%%%% END DATA


%%%%%% BEGIN Adaptation %%%%%%%%%%%%%%%%%%

@inproceedings{santoro2016meta,
  author={Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy P. Lillicrap},
  title={Meta-Learning with Memory-Augmented Neural Networks},
  year={2016},
  cdate={1451606400000},
  pages={1842-1850},
  url={http://proceedings.mlr.press/v48/santoro16.html},
  booktitle={ICML},
}

@article{khattab2020colbert,
  title={ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT},
  author={O. Khattab and M. Zaharia},
  journal={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2020}
}

@inproceedings{zhao2020masking,
    title = "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models",
    author = {Zhao, Mengjie  and
      Lin, Tao  and
      Mi, Fei  and
      Jaggi, Martin  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.174",
    doi = "10.18653/v1/2020.emnlp-main.174",
    pages = "2226--2241",
}

@article{hombaiah2021dynamic,
  author    = {Spurthi Amba Hombaiah and
               Tao Chen and
               Mingyang Zhang and
               Michael Bendersky and
               Marc Najork},
  title     = {Dynamic Language Models for Continuously Evolving Content},
  journal   = {CoRR},
  volume    = {abs/2106.06297},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.06297},
  archivePrefix = {arXiv},
  eprint    = {2106.06297},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-06297.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gururangan2020dont,
      title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks}, 
      author={Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
      year={2020},
      eprint={2004.10964},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{reed2021selfsupervised,
      title={Self-Supervised Pretraining Improves Self-Supervised Pretraining}, 
      author={Colorado J. Reed and Xiangyu Yue and Ani Nrusimha and Sayna Ebrahimi and Vivek Vijaykumar and Richard Mao and Bo Li and Shanghang Zhang and Devin Guillory and Sean Metzger and Kurt Keutzer and Trevor Darrell},
      year={2021},
      eprint={2103.12718},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{surden2020ethics,
  title={The ethics of artificial intelligence in law: Basic questions},
  author={Surden, Harry},
  journal={Forthcoming chapter in Oxford Handbook of Ethics of AI},
  pages={19--29},
  year={2020}
}
@misc{chalkidis2020legalbert,
      title={LEGAL-BERT: The Muppets straight out of Law School}, 
      author={Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},
      year={2020},
      eprint={2010.02559},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cole2021does,
      title={When Does Contrastive Visual Representation Learning Work?}, 
      author={Elijah Cole and Xuan Yang and Kimberly Wilber and Oisin Mac Aodha and Serge Belongie},
      year={2021},
      eprint={2105.05837},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{cao2015towards,
  title={Towards making systems forget with machine unlearning},
  author={Cao, Yinzhi and Yang, Junfeng},
  booktitle={2015 IEEE Symposium on Security and Privacy},
  pages={463--480},
  year={2015},
  organization={IEEE}
}


@article{bourtoule2019machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  journal={arXiv preprint arXiv:1912.03817},
  year={2019}
}

@article{karnofsky2016potential,
  title={Potential risks from advanced artificial intelligence: the philanthropic opportunity},
  author={Karnofsky, Holden},
  journal={Open Philanthropy Project},
  volume={6},
  year={2016}
}

@article{keskar2019ctrl,
  title={Ctrl: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@article{solaiman2021process,
  title={Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets},
  author={Solaiman, Irene and Dennison, Christy},
  journal={arXiv preprint arXiv:2106.10328},
  year={2021}
}

@article{sheng2019woman,
  title={The woman worked as a babysitter: On biases in language generation},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  journal={arXiv preprint arXiv:1909.01326},
  year={2019}
}

@article{kurita2019measuring,
  title={Measuring bias in contextualized word representations},
  author={Kurita, Keita and Vyas, Nidhi and Pareek, Ayush and Black, Alan W and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:1906.07337},
  year={2019}
}


@article{basta2019evaluating,
  title={Evaluating the underlying gender bias in contextualized word embeddings},
  author={Basta, Christine and Costa-Juss{\`a}, Marta R and Casas, Noe},
  journal={arXiv preprint arXiv:1904.08783},
  year={2019}
}

@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@article{jaech2018low,
    title = "Low-Rank {RNN} Adaptation for Context-Aware Language Modeling",
    author = "Jaech, Aaron and Ostendorf, Mari",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    url = "https://www.aclweb.org/anthology/Q18-1035",
    doi = "10.1162/tacl_a_00035",
    pages = "497--510",
}

@misc{decao2021editing,
      title={Editing Factual Knowledge in Language Models}, 
      author={Nicola De Cao and Wilker Aziz and Ivan Titov},
      year={2021},
      eprint={2104.08164},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sinitsin2020editable,
    title={Editable Neural Networks},
    author={Anton Sinitsin and Vsevolod Plokhotnyuk and Dmitry Pyrkin and Sergei Popov and Artem Babenko},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=HJedXaEtvS}
}

@misc{zhu2020modifying,
      title={Modifying Memories in Transformer Models}, 
      author={Chen Zhu and Ankit Singh Rawat and Manzil Zaheer and Srinadh Bhojanapalli and Daliang Li and Felix Yu and Sanjiv Kumar},
      year={2020},
      eprint={2012.00363},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gehrmann-etal-2021-gem,
    title = "The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics",
    author = "Gehrmann, Sebastian  and
      Adewumi, Tosin  and
      Aggarwal, Karmanya  and
      Ammanamanchi, Pawan Sasanka  and
      Aremu, Anuoluwapo  and
      Bosselut, Antoine  and
      Chandu, Khyathi Raghavi  and
      Clinciu, Miruna-Adriana  and
      Das, Dipanjan  and
      Dhole, Kaustubh  and
      Du, Wanyu  and
      Durmus, Esin  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Emezue, Chris Chinenye  and
      Gangal, Varun  and
      Garbacea, Cristina  and
      Hashimoto, Tatsunori  and
      Hou, Yufang  and
      Jernite, Yacine  and
      Jhamtani, Harsh  and
      Ji, Yangfeng  and
      Jolly, Shailza  and
      Kale, Mihir  and
      Kumar, Dhruv  and
      Ladhak, Faisal  and
      Madaan, Aman  and
      Maddela, Mounica  and
      Mahajan, Khyati  and
      Mahamood, Saad  and
      Majumder, Bodhisattwa Prasad  and
      Martins, Pedro Henrique  and
      McMillan-Major, Angelina  and
      Mille, Simon  and
      van Miltenburg, Emiel  and
      Nadeem, Moin  and
      Narayan, Shashi  and
      Nikolaev, Vitaly  and
      Niyongabo Rubungo, Andre  and
      Osei, Salomey  and
      Parikh, Ankur  and
      Perez-Beltrachini, Laura  and
      Rao, Niranjan Ramesh  and
      Raunak, Vikas  and
      Rodriguez, Juan Diego  and
      Santhanam, Sashank  and
      Sedoc, Jo{\~a}o  and
      Sellam, Thibault  and
      Shaikh, Samira  and
      Shimorina, Anastasia  and
      Sobrevilla Cabezudo, Marco Antonio  and
      Strobelt, Hendrik  and
      Subramani, Nishant  and
      Xu, Wei  and
      Yang, Diyi  and
      Yerukola, Akhila  and
      Zhou, Jiawei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.10",
    doi = "10.18653/v1/2021.gem-1.10",
    pages = "96--120",
    abstract = "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
}

@misc{shoeybi2019megatronlm,
  added-at = {2020-01-20T15:54:47.000+0100},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  biburl = {https://www.bibsonomy.org/bibtex/2127d71e4f32cd099d2537d52d6934e3a/nosebrain},
  interhash = {cd4e6ad8c8e1319cbde05fe82576070b},
  intrahash = {127d71e4f32cd099d2537d52d6934e3a},
  note = {cite arxiv:1909.08053},
  timestamp = {2020-01-20T15:54:47.000+0100},
  title = {{Megatron-LM: Training Multi-Billion Parameter Language Models Using
  Model Parallelism}},
  url = {http://arxiv.org/abs/1909.08053},
  year = 2019
}

@inproceedings{mikolov2010recurrent,
author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukas and Cernocký, Jan and Khudanpur, Sanjeev},
year = {2010},
month = {01},
pages = {1045-1048},
title = {Recurrent neural network based language model},
volume = {2},
journal = {Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010}
}

@inproceedings{karpukhin2020dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
}

@article{gao2020fewshot,
  author    = {Tianyu Gao and
               Adam Fisch and
               Danqi Chen},
  title     = {Making Pre-trained Language Models Better Few-shot Learners},
  journal   = {CoRR},
  volume    = {abs/2012.15723},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.15723},
  archivePrefix = {arXiv},
  eprint    = {2012.15723},
  timestamp = {Fri, 08 Jan 2021 17:23:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-15723.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout


@inproceedings{lewis2020retrieval,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{guu2020realm,
  title={REALM: Retrieval-Augmented Language Model Pre-Training},
  author={Kelvin Guu and Kenton Lee and Z. Tung and Panupong Pasupat and Ming-Wei Chang},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.08909}
}

@incollection{mccloskey1989catastrophic,
title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {24},
pages = {109-165},
year = {1989},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
author = {Michael McCloskey and Neal J. Cohen},
}

@article{parisi2018continual,
title = {Continual lifelong learning with neural networks: A review},
journal = {Neural Networks},
volume = {113},
pages = {54-71},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300231},
author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
keywords = {Continual learning, Lifelong learning, Catastrophic forgetting, Developmental systems, Memory consolidation},
}

@article{Ratcliff1990ConnectionistMO,
  title={Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
  author={R. Ratcliff},
  journal={Psychological review},
  year={1990},
  volume={97 2},
  pages={
          285-308
        }
}

%%%%%% END Adaptation %%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%START OF REASONING%%%%%%%%%%%%%%%%%%
@book{LakoffNunez00,
  added-at = {2006-09-28T12:28:58.000+0200},
  address = {New York},
  author = {Lakoff, George and Núñez, Rafael},
  biburl = {https://www.bibsonomy.org/bibtex/2447107571f579d4107775ae38a9d36b4/yish},
  interhash = {f226838ff042bdea82df0084b1f229ab},
  intrahash = {447107571f579d4107775ae38a9d36b4},
  keywords = {cerme6 cognition conceptual embodied learning mathematics metaphor mythesis},
  publisher = {Basic Books},
  timestamp = {2008-10-04T15:24:08.000+0200},
  title = {Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being},
  url = {http://perso.unifr.ch/rafael.nunez/welcome.html},
  year = 2000
}

@book{DBLP:books/aw/RN2020,
	title        = {Artificial Intelligence: {A} Modern Approach (4th Edition)},
	author       = {Stuart J. Russell and Peter Norvig},
	year         = 2020,
	publisher    = {Pearson},
	isbn         = 9780134610993,
	url          = {http://aima.cs.berkeley.edu/},
	timestamp    = {Thu, 20 May 2021 08:27:41 +0200},
	biburl       = {https://dblp.org/rec/books/aw/RN2020.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{lime,
	title        = {{LIME:} Learning Inductive Bias for Primitives of Mathematical Reasoning},
	author       = {Yuhuai Wu and Markus N. Rabe and Wenda Li and Jimmy Ba and Roger B. Grosse and Christian Szegedy},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
}
@inproceedings{li2021isarstep,
	title        = {IsarStep: a Benchmark for High-level Mathematical Reasoning},
	author       = {Wenda Li and Lei Yu and Yuhuai Wu and Lawrence C. Paulson},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Pzj6fzU6wkj}
}

@article{pact,
	title        = {Proof Artifact Co-training for Theorem Proving with Language Models},
	author       = {Jesse Michael Han and Jason Rute and Yuhuai Wu and Edward W. Ayers and Stanislas Polu},
	year         = 2021,
	journal      = {The First Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021},
	url          = {https://mathai-iclr.github.io/papers/papers/MATHAI_23_paper.pdf},
}
@article{refactor,
	title        = {REFACTOR: Learning to Extract Theorems from Proofs},
	author       = {Jin Peng Zhou and Yuhuai Wu and Colin Li and Roger Grosse},
	year         = 2021,
	journal      = {The First Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021},
	url          = {https://mathai-iclr.github.io/papers/papers/MATHAI_22_paper.pdf},
}
@article{DBLP:journals/corr/polu2020generative,
	title        = {Generative Language Modeling for Automated Theorem Proving},
	author       = {Stanislas Polu and Ilya Sutskever},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2009.03393},
	url          = {https://arxiv.org/abs/2009.03393},
	archiveprefix = {arXiv},
	eprint       = {2009.03393},
	timestamp    = {Sat, 23 Jan 2021 01:12:25 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2009-03393.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2006-04757,
	title        = {Mathematical reasoning via self-supervised skip-tree training},
	author       = {Markus N. Rabe and Dennis Lee and Kshitij Bansal and Christian Szegedy},
	year         = 2021,
	journal      = {ICLR},
	url          = {https://openreview.net/forum?id=YmqAnY0CMEy},
}
@article{DBLP:journals/nature/SilverHMGSDSAPL16,
  author    = {David Silver and
               Aja Huang and
               Chris J. Maddison and
               Arthur Guez and
               Laurent Sifre and
               George van den Driessche and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Vedavyas Panneershelvam and
               Marc Lanctot and
               Sander Dieleman and
               Dominik Grewe and
               John Nham and
               Nal Kalchbrenner and
               Ilya Sutskever and
               Timothy P. Lillicrap and
               Madeleine Leach and
               Koray Kavukcuoglu and
               Thore Graepel and
               Demis Hassabis},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  journal   = {Nat.},
  volume    = {529},
  number    = {7587},
  pages     = {484--489},
  year      = {2016},
  url       = {https://doi.org/10.1038/nature16961},
  doi       = {10.1038/nature16961},
  timestamp = {Mon, 08 Jun 2020 22:21:28 +0200},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{silver2017alphagozero,
	title        = {Mastering the game of {Go} without human knowledge},
	author       = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	year         = 2017,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 550,
	number       = 7676,
	pages        = {354--359}
}
@article{DBLP:journals/corr/abs-2105-04297,
  author    = {Dinglan Peng and
               Shuxin Zheng and
               Yatao Li and
               Guolin Ke and
               Di He and
               Tie{-}Yan Liu},
  title     = {How could Neural Networks understand Programs?},
  journal   = {CoRR},
  volume    = {abs/2105.04297},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.04297},
  archivePrefix = {arXiv},
  eprint    = {2105.04297},
  timestamp = {Fri, 14 May 2021 12:13:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-04297.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2103-05247,
  author    = {Kevin Lu and
               Aditya Grover and
               Pieter Abbeel and
               Igor Mordatch},
  title     = {Pretrained Transformers as Universal Computation Engines},
  journal   = {CoRR},
  volume    = {abs/2103.05247},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.05247},
  archivePrefix = {arXiv},
  eprint    = {2103.05247},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-05247.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{KOEDINGER1990511,
	title        = {Abstract planning and perceptual chunks: Elements of expertise in geometry},
	author       = {Kenneth R. Koedinger and John R. Anderson},
	year         = 1990,
	journal      = {Cognitive Science},
	volume       = 14,
	number       = 4,
	pages        = {511--550},
	doi          = {https://doi.org/10.1016/0364-0213(90)90008-K},
	issn         = {0364-0213},
	url          = {http://www.sciencedirect.com/science/article/pii/036402139090008K},
	abstract     = {We present a new model of skilled performance in geometry proof problem solving called the Diagram Configuration model (DC). While previous models plan proofs in a step-by-step fashion, we observed that experts plan at a more abstract level: They focus on the key steps and skip the less important ones. DC models this abstract planning behavior by parsing geometry problem diagrams into perceptual chunks, called diagram configurations, which cue relevant schematic knowledge. We provide verbal protocol evidence that DC's schemas correspond with the step-skipping inferences experts make in their initial planning. We compare DC with other models of geometry expertise and then, in the final section, we discuss more general implications of our research. DC's reasoning has important similarities with Larkin's (1988) display-based reasoning approach and Johnson-Laird's (1983) mental model approach. DC's perceptually based schemas are a step towards a unified explanation of (1) experts' superior problem-solving effectiveness, (2) experts' superior problem-state memory, and (3) experts' ability, in certain domains, to solve relatively simple problems by pure forward inferencing. We also argue that the particular and efficient knowledge organization of DC challenges current theories of skill acquisition as it presents an end-state of learning that is difficult to explain within such theories. Finally, we discuss the implications of DC for geometry instruction.}
}
@article{Larkin1987Simon,
author = {Larkin, Jill H. and Simon, Herbert A.},
title = {Why a Diagram is (Sometimes) Worth Ten Thousand Words},
journal = {Cognitive Science},
volume = {11},
number = {1},
pages = {65-100},
doi = {https://doi.org/10.1111/j.1551-6708.1987.tb00863.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1551-6708.1987.tb00863.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6708.1987.tb00863.x},
abstract = {We distinguish diagrammatic from sentential paper-and-pencil representations of information by developing alternative models of information-processing systems that are informationally equivalent and that can be characterized as sentential or diagrammatic. Sentential representations are sequential, like the propositions in a text. Diagrammatic representations are indexed by location in a plane. Diagrammatic representations also typically display information that is only implicit in sentential representations and that therefore has to be computed, sometimes at great cost, to make it explicit for use. We then contrast the computational efficiency of these representations for solving several illustrative problems in mathematics and physics. When two representations are informationally equivalent, their computational efficiency depends on the information-processing operators that act on them. Two sets of operators may differ in their capabilities for recognizing patterns, in the inferences they can carry out directly, and in their control strategies (in particular, the control of search). Diagrammatic and sentential representations support operators that differ in all of these respects. Operators working on one representation may recognize features readily or make inferences directly that are difficult to realize in the other representation. Most important, however, are differences in the efficiency of search for information and in the explicitness of information. In the representations we call diagrammatic, information is organized by location, and often much of the information needed to make an inference is present and explicit at a single location. In addition, cues to the next logical step in the problem may be present at an adjacent location. Therefore problem solving can proceed through a smooth traversal of the diagram, and may require very little search or computation of elements that had been implicit.},
year = {1987}
}
@inproceedings{DBLP:conf/mkm/UrbanJ20,
  author    = {Josef Urban and
               Jan Jakubuv},
  editor    = {Christoph Benzm{\"{u}}ller and
               Bruce R. Miller},
  title     = {First Neural Conjecturing Datasets and Experiments},
  booktitle = {Intelligent Computer Mathematics - 13th International Conference,
               {CICM} 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {12236},
  pages     = {315--323},
  publisher = {Springer},
  year      = {2020},
  url       = {https://doi.org/10.1007/978-3-030-53518-6\_24},
  doi       = {10.1007/978-3-030-53518-6\_24},
  timestamp = {Fri, 24 Jul 2020 13:44:33 +0200},
  biburl    = {https://dblp.org/rec/conf/mkm/UrbanJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/nature/Senior0JKSGQZNB20,
  author    = {Andrew W. Senior and
               Richard Evans and
               John Jumper and
               James Kirkpatrick and
               Laurent Sifre and
               Tim Green and
               Chongli Qin and
               Augustin Z{\'{\i}}dek and
               Alexander W. R. Nelson and
               Alex Bridgland and
               Hugo Penedones and
               Stig Petersen and
               Karen Simonyan and
               Steve Crossan and
               Pushmeet Kohli and
               David T. Jones and
               David Silver and
               Koray Kavukcuoglu and
               Demis Hassabis},
  title     = {Improved protein structure prediction using potentials from deep learning},
  journal   = {Nat.},
  volume    = {577},
  number    = {7792},
  pages     = {706--710},
  year      = {2020},
  url       = {https://doi.org/10.1038/s41586-019-1923-7},
  doi       = {10.1038/s41586-019-1923-7},
  timestamp = {Thu, 13 Aug 2020 18:45:04 +0200},
  biburl    = {https://dblp.org/rec/journals/nature/Senior0JKSGQZNB20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2102-04664,
  author    = {Shuai Lu and
               Daya Guo and
               Shuo Ren and
               Junjie Huang and
               Alexey Svyatkovskiy and
               Ambrosio Blanco and
               Colin B. Clement and
               Dawn Drain and
               Daxin Jiang and
               Duyu Tang and
               Ge Li and
               Lidong Zhou and
               Linjun Shou and
               Long Zhou and
               Michele Tufano and
               Ming Gong and
               Ming Zhou and
               Nan Duan and
               Neel Sundaresan and
               Shao Kun Deng and
               Shengyu Fu and
               Shujie Liu},
  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
               and Generation},
  journal   = {CoRR},
  volume    = {abs/2102.04664},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.04664},
  archivePrefix = {arXiv},
  eprint    = {2102.04664},
  timestamp = {Tue, 11 May 2021 14:34:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-04664.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2102-07492,
  author    = {Baptiste Rozi{\`{e}}re and
               Marie{-}Anne Lachaux and
               Marc Szafraniec and
               Guillaume Lample},
  title     = {{DOBF:} {A} Deobfuscation Pre-Training Objective for Programming Languages},
  journal   = {CoRR},
  volume    = {abs/2102.07492},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.07492},
  archivePrefix = {arXiv},
  eprint    = {2102.07492},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-07492.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/icml/BansalLRSW19,
	title        = {{HOList}: An Environment for Machine Learning of Higher Order Logic Theorem Proving},
	author       = {Kshitij Bansal and Sarah M. Loos and Markus N. Rabe and Christian Szegedy and Stewart Wilcox},
	year         = 2019,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
	volume       = 97,
	pages        = {454--463},
	url          = {http://proceedings.mlr.press/v97/bansal19a.html},
	editor       = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
	timestamp    = {Tue, 11 Jun 2019 15:37:38 +0200},
	biburl       = {https://dblp.org/rec/conf/icml/BansalLRSW19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{vlad2021mathai,
	title        = {Training a First-Order Theorem Prover from Synthetic Data},
	author       = {Vlad Firoiu and Eser Aygün and Ankit Anand and Zafarali Ahmed and Xavier Glorot and Laurent Orseau and Doina Precup and Shibl Mourad},
	year         = 2021,
	journal      = {The First Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021},
	url          = {https://mathai-iclr.github.io/papers/papers/MATHAI_18_paper.pdf},
}
@article{DBLP:journals/corr/abs-1806-00608,
	title        = {GamePad: {A} Learning Environment for Theorem Proving},
	author       = {Daniel Huang and Prafulla Dhariwal and Dawn Song and Ilya Sutskever},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1806.00608},
	url          = {http://arxiv.org/abs/1806.00608},
	archiveprefix = {arXiv},
	eprint       = {1806.00608},
	timestamp    = {Sun, 12 May 2019 18:49:48 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1806-00608.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{wu2021int,
	title        = {{INT}: {An Inequality Benchmark for Evaluating Generalization in Theorem Proving}},
	author       = {Yuhuai Wu and Albert Jiang and Jimmy Ba and Roger Grosse},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=O6LPudowNQm}
}
@article{DBLP:journals/eor/BengioLP21,
	title        = {Machine learning for combinatorial optimization: {A} methodological tour d'horizon},
	author       = {Yoshua Bengio and Andrea Lodi and Antoine Prouvost},
	year         = 2021,
	journal      = {Eur. J. Oper. Res.},
	volume       = 290,
	number       = 2,
	pages        = {405--421},
	doi          = {10.1016/j.ejor.2020.07.063},
	url          = {https://doi.org/10.1016/j.ejor.2020.07.063},
	timestamp    = {Tue, 19 Jan 2021 08:13:08 +0100},
	biburl       = {https://dblp.org/rec/journals/eor/BengioLP21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/ftpl/GulwaniPS17,
  author    = {Sumit Gulwani and
               Oleksandr Polozov and
               Rishabh Singh},
  title     = {Program Synthesis},
  journal   = {Found. Trends Program. Lang.},
  volume    = {4},
  number    = {1-2},
  pages     = {1--119},
  year      = {2017},
  url       = {https://doi.org/10.1561/2500000010},
  doi       = {10.1561/2500000010},
  timestamp = {Wed, 20 May 2020 21:26:01 +0200},
  biburl    = {https://dblp.org/rec/journals/ftpl/GulwaniPS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Drews2000DrugDA,
  title={Drug discovery: a historical perspective.},
  author={J. Drews},
  journal={Science},
  year={2000},
  volume={287 5460},
  pages={1960-4}
}
@book{computer_aided_design,
author = {Haigh, Martin J.},
title = {An Introduction to Computer-Aided Design and Manufacture},
year = {1985},
isbn = {0632012420},
publisher = {Blackwell Scientific Publications, Ltd.},
address = {GBR}
}
@article{DBLP:journals/nature/SeglerPW18,
  author    = {Marwin H. S. Segler and
               Mike Preuss and
               Mark P. Waller},
  title     = {Planning chemical syntheses with deep neural networks and symbolic
               {AI}},
  journal   = {Nat.},
  volume    = {555},
  number    = {7698},
  pages     = {604--610},
  year      = {2018},
  url       = {https://doi.org/10.1038/nature25978},
  doi       = {10.1038/nature25978},
  timestamp = {Sat, 05 Sep 2020 17:48:36 +0200},
  biburl    = {https://dblp.org/rec/journals/nature/SeglerPW18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/PapadimitriouJ20,
  author    = {Isabel Papadimitriou and
               Dan Jurafsky},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {Learning Music Helps You Read: Using Transfer to Study Linguistic
               Structure in Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {6829--6839},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.554},
  doi       = {10.18653/v1/2020.emnlp-main.554},
  timestamp = {Fri, 08 Jan 2021 21:21:07 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/PapadimitriouJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%%%%%%%%%%%%%%END OF REASONING%%%%%%%%%%%%%%%%%%

@misc{wang_what_2020,
	title = {What the AI Community Can Learn From Sneezing Ferrets and a Mutant Virus Debate},
	howpublished = {\url{https://medium.com/partnership-on-ai/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e}},
	language = {en},
	author = {Wang, Jasmine},
	month = dec,
	year = {2020},
}

@inproceedings{kelley2009nutrition,
title={A" nutrition label" for privacy},
author={Kelley, Patrick Gage and Bresee, Joanna and Cranor, Lorrie Faith and Reeder, Robert W},
booktitle={Proceedings of the 5th Symposium on Usable Privacy and Security},
pages={1--12},
year={2009}
}

@misc{dodge2019work,
      title={Show Your Work: Improved Reporting of Experimental Results}, 
      author={Jesse Dodge and Suchin Gururangan and Dallas Card and Roy Schwartz and Noah A. Smith},
      year={2019},
      eprint={1909.03004},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Mitchell_2019,
   title={Model Cards for Model Reporting},
   ISBN={9781450361255},
   url={http://dx.doi.org/10.1145/3287560.3287596},
   DOI={10.1145/3287560.3287596},
   journal={Proceedings of the Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
   year={2019},
   month={Jan}
}

@inproceedings{shevlane_offense-defense_2020,
	title = {The {Offense}-{Defense} {Balance} of {Scientific} {Knowledge}: {Does} {Publishing} {AI} {Research} {Reduce} {Misuse}?},
	shorttitle = {The {Offense}-{Defense} {Balance} of {Scientific} {Knowledge}},
	url = {http://arxiv.org/abs/2001.00463},
	abstract = {There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.},
	booktitle = {Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, and Society (AIES
    ’20)},
	author = {Shevlane, Toby and Dafoe, Allan},
	month = {Feb},
	year = {2020},
}

@misc{wang_what_2021,
	title = {What {Responsible} {AI} {Can} {Learn} {From} the {Race} to {Fix} {Meltdown} and {Spectre}},
	howpublished = {\url{https://medium.com/partnership-on-ai/lessons-for-the-ai-community-from-meltdown-and-spectre-b358cc81eeff}},
	language = {en},
	author = {Wang, Jasmine},
	month = {Feb},
	year = {2021},
}

@techreport{solaiman_release_2019,
	title = {Release {Strategies} and the {Social} {Impacts} of {Language} {Models}},
	url = {http://arxiv.org/abs/1908.09203},
	abstract = {Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.},
	author = {Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and McCain, Miles and Newhouse, Alex and Blazakis, Jason and McGuffie, Kris and Wang, Jasmine},
	month = {Nov},
	year = {2019},
	institution = {OpenAI},
}
@article{awad2018moral,
  title={The moral machine experiment},
  author={Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-Fran{\c{c}}ois and Rahwan, Iyad},
  journal={Nature},
  volume={563},
  number={7729},
  pages={59--64},
  year={2018},
  publisher={Nature Publishing Group}
}
@inproceedings{whittlestone_role_nodate,
	title = {The {Role} and {Limits} of {Principles} in {AI} {Ethics}: {Towards} a {Focus} on {Tensions}},
	abstract = {The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.},
	language = {en},
	author = {Whittlestone, Jess and Nyrup, Rune and Alexandrova, Anna and Cave, Stephen},
	pages = {7},
}

@article{narayanan.2020,
author = {Narayanan, Arvind and Mathur, Arunesh and Chetty, Marshini and Kshirsagar, Mihir},
title = {Dark Patterns: Past, Present, and Future},
year = {2020},
volume = {63},
number = {9},
url = {https://doi.org/10.1145/3397884},
doi = {10.1145/3397884},
journal = {Commun. ACM},
month = aug,
pages = {42–47},
}

@article{obar.2020,
author = {Jonathan A. Obar and Anne Oeldorf-Hirsch},
title = {The biggest lie on the Internet: {I}gnoring the privacy policies and terms of service policies of social networking services},
journal = {Information, Communication \& Society},
volume = {23},
number = {1},
pages = {128-147},
year  = {2020},
doi = {10.1080/1369118X.2018.1486870},
URL = { https://doi.org/10.1080/1369118X.2018.1486870},
}



@article{artetxe_cross-lingual_2020,
	title = {On the {Cross}-lingual {Transferability} of {Monolingual} {Representations}},
	url = {http://arxiv.org/abs/1910.11856},
	journal = {arXiv:1910.11856 [cs]},
	author = {Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
	month = {May},
	year = {2020},
}

@article{wang_extending_2020,
	title = {Extending {Multilingual} {BERT} to {Low}-{Resource} {Languages}},
	url = {http://arxiv.org/abs/2004.13640},
	abstract = {Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success has focused only on the top 104 languages in Wikipedia that it was trained on. In this paper, we propose a simple but effective approach to extend M-BERT (E-BERT) so that it can benefit any new language, and show that our approach benefits languages that are already in M-BERT as well. We perform an extensive set of experiments with Named Entity Recognition (NER) on 27 languages, only 16 of which are in M-BERT, and show an average increase of about 6\% F1 on languages that are already in M-BERT and 23\% F1 increase on new languages.},
	journal = {arXiv:2004.13640 [cs]},
	author = {Wang, Zihan and K, Karthikeyan and Mayhew, Stephen and Roth, Dan},
	month = {Apr},
	year = {2020},
}

@misc{eluetherAI_whyrelease, 
    title = {Why Release a Large Language Model?},
    author = {Leahy, Connor},
    howpublished = {\url{https://blog.eleuther.ai/why-release-a-large-language-model/}}, 
    year = {2021}, 
    month = {June}, 
}

@techreport{pineau_improving_2020,
	title = {Improving {Reproducibility} in {Machine} {Learning} {Research} ({A} {Report} from the {NeurIPS} 2019 {Reproducibility} {Program})},
	url = {http://arxiv.org/abs/2003.12206},
	author = {Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv and Larivière, Vincent and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily and Larochelle, Hugo},
	month = {Dec},
	year = {2020},
    institution = {NeurIPS 2019 Reproducibility Program}, 
}

@misc{tamkin2021understanding,
      title={Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models}, 
      author={Alex Tamkin and Miles Brundage and Jack Clark and Deep Ganguli},
      year={2021},
      eprint={2102.02503},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{Anderson2013,
    title = {The Imperative of Integration},
    author = {Elizabeth Anderson},
    year = {2013},
    month = {04},
    day = {21},
    pagecount = {264},
    isbn = {9780691158112}
}

@article{Baier1986,
 ISSN = {00141704, 1539297X},
 URL = {http://www.jstor.org/stable/2381376},
 author = {Annette Baier},
 journal = {Ethics},
 number = {2},
 pages = {231--260},
 publisher = {University of Chicago Press},
 title = {Trust and Antitrust},
 volume = {96},
 year = {1986}
}

@misc{sheng2021revealing,
      title={Revealing Persona Biases in Dialogue Systems}, 
      author={Emily Sheng and Josh Arnold and Zhou Yu and Kai-Wei Chang and Nanyun Peng},
      year={2021},
      eprint={2104.08728},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{dinan-etal-2020-queens,
    title = {Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation},
    author = {Dinan, Emily  and
      Fan, Angela  and
      Williams, Adina  and
      Urbanek, Jack  and
      Kiela, Douwe  and
      Weston, Jason},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = {nov},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.656},
    doi = {10.18653/v1/2020.emnlp-main.656},
    pages = {8173--8188}
}

@article {levendowski2018copyright,
  title = {How copyright law can fix artificial intelligence's implicit bias problem},
  author = {Levendowski, Amanda},
  journal = {Wash. L. Rev.},
  volume = {93},
  pages = {579},
  year = {2018},
  publisher = {HeinOnline}
}
@article{villaronga2018humans,
  title={Humans forget, machines remember: Artificial intelligence and the right to be forgotten},
  author={Villaronga, Eduard Fosch and Kieseberg, Peter and Li, Tiffany},
  journal={Computer Law \& Security Review},
  volume={34},
  number={2},
  pages={304--313},
  year={2018},
  publisher={Elsevier}
}

@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{poliak2018hypothesis,
    title = "Hypothesis Only Baselines in Natural Language Inference",
    author = "Poliak, Adam  and
      Naradowsky, Jason  and
      Haldar, Aparajita  and
      Rudinger, Rachel  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S18-2023",
    doi = "10.18653/v1/S18-2023",
    pages = "180--191",
}

@InProceedings{GevaEtAl2019, title = {{Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets}}, author = {Geva, Mor and Goldberg, Yoav and Berant, Jonathan}, booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing}, note = {arXiv preprint arXiv:1908.07898}, year = {2019} }

@article{gillis2019big,
  title={Big data and discrimination},
  author={Gillis, Talia B and Spiess, Jann L},
  journal={The University of Chicago Law Review},
  volume={86},
  number={2},
  pages={459--488},
  year={2019},
  publisher={JSTOR}
}
@article{scherer2019applying,
  title={Applying Old Rules to New Tools: Employment Discrimination Law in the Age of Algorithms},
  author={Scherer, Matthew U and King, Allan G and Mrkonich, Marko J},
  journal={SCL Rev.},
  volume={71},
  pages={449},
  year={2019},
  publisher={HeinOnline}
}
@article{xiang2021reconciling,
  title={Reconciling legal and technical approaches to algorithmic bias},
  author={Xiang, Alice},
  journal={Tennessee Law Review},
  volume={88},
  number={3},
  year={2021}
}
@article{ho2020affirmative,
  title={Affirmative Algorithms: The Legal Grounds for Fairness as Awareness},
  author={Ho, Daniel E and Xiang, Alice},
  journal={U. Chi. L. Rev. Online},
  pages={134},
  year={2020},
  publisher={HeinOnline}
}
@misc{code_is_law,
    title={{Code Is Law}},
    author={Lawrence Lessig},
    howpublished={\url{https://www.harvardmagazine.com/2000/01/code-is-law-html}},
    publisher={Harvard Magazine},
    year={2000}
}

@inproceedings{raghu2019transfusion,
 author = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Transfusion: Understanding Transfer Learning for Medical Imaging},
 url = {https://proceedings.neurips.cc/paper/2019/file/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Rini2017-fakenews,
	journal = {Kennedy Institute of Ethics Journal},
	year = {2017},
	volume = {27},
	title = {Fake News and Partisan Epistemology},
	pages = {43--64},
	doi = {10.1353/ken.2017.0025},
	author = {Regina Rini},
	number = {S2}
}

@InProceedings{Fong_2017_ICCV,
author = {Fong, Ruth C. and Vedaldi, Andrea},
title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@inproceedings{lin-et-al-2021-naacl,
  aclid =       {2021.naacl-main.405},
  author =      {Chu-Cheng Lin and Aaron Jaech and Xin Li and Matt
                 Gormley and Jason Eisner},
  title =       {Limitations of Autoregressive Models and Their
                 Alternatives},
  booktitle =   {Proceedings of the 2021 Conference of the North
                 American Chapter of the Association for Computational
                 Linguistics: Human Language Technologies (NAACL-HLT)},
  pages =       {5147--5173},
  year =        {2021},
  month =       jun,
  address =     {Online},
  url =         {http://cs.jhu.edu/~jason/papers/#lin-et-al-2021-naacl}
}

@inproceedings{qin-eisner-2021,
  aclid =       {2021.naacl-main.410},
  author =      {Guanghui Qin and Jason Eisner},
  title =       {Learning How To Ask: Querying {LM}s with Mixtures of
                 Soft Prompts},
  booktitle =   {Proceedings of the 2021 Conference of the North
                 American Chapter of the Association for Computational
                 Linguistics: Human Language Technologies (NAACL-HLT)},
  pages =       {5203--5212},
  year =        {2021},
  month =       jun,
  address =     {Online},
  url =         {http://cs.jhu.edu/~jason/papers/#qin-eisner-2021}
}

@misc{dhingra2021time,
      title={Time-Aware Language Models as Temporal Knowledge Bases}, 
      author={Bhuwan Dhingra and Jeremy R. Cole and Julian Martin Eisenschlos and Daniel Gillick and Jacob Eisenstein and William W. Cohen},
      year={2021},
      eprint={2106.15110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@unpublished{Khattab-etal:2021:HAI,
    author = {Khattab, Omar  and  Potts, Christopher  and  Zaharia, Matei},
    note = {Stanford HAI Blog},
    url={https://hai.stanford.edu/news/moderate-proposal-radically-better-ai-powered-web-search},
    year = {2021},
    title = {A Moderate Proposal for Radically Better {AI}-Powered {W}eb Search}}

@unpublished{Khattab-etal:2021:Baleen,
    title={Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval},
    author={Khattab, Omar  and  Potts, Christopher  and  Zaharia, Matei},
    note={arXiv:2101.00436},
    url={https://arxiv.org/abs/2101.00436},
    year={2021}}

@unpublished{Khattab-etal:2020:OpenQA,
	author = {Khattab, Omar and Potts, Christopher and Zaharia, Matei},
	eprint = {https://arxiv.org/abs/2007.00814},
	title = {{Relevance-Guided Supervision for {OpenQA} with {ColBERT}}},
	year = {2020}}


@article{hwangbo2019learning,
  title={Learning agile and dynamic motor skills for legged robots},
  author={Hwangbo, Jemin and Lee, Joonho and Dosovitskiy, Alexey and Bellicoso, Dario and Tsounis, Vassilios and Koltun, Vladlen and Hutter, Marco},
  journal={Science Robotics},
  volume={4},
  number={26},
  year={2019},
  publisher={Science Robotics}
}

@article{kalashnikov2021mt,
  title={MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale},
  author={Kalashnikov, Dmitry and Varley, Jacob and Chebotar, Yevgen and Swanson, Benjamin and Jonschkowski, Rico and Finn, Chelsea and Levine, Sergey and Hausman, Karol},
  journal={arXiv preprint arXiv:2104.08212},
  year={2021}
}

%%%%%%%%%%%%%%%START OF ECONOMICS %%%%%%%%%%%%%%%%%%
@article{Bresnahan1995,
abstract = {Whole eras of technical progress and growth appear to be driven by a few 'General Purpose Technologies' (GPT's), such as the steam engine, the electric motor, and semiconductors. GPT's are characterized by pervasiveness, inherent potential for technical improvements, and 'innovational complementarities', giving rise to increasing returns-to-scale. However, a decentralized economy will have difficulty in fully exploiting the growth opportunities of GPT's: arms-length market transactions between the GPT and its users may result in 'too little, too late' innovation. Likewise, difficulties in forecasting the technological developments of the other side can lower the rate of technical advance of all sectors. {\textcopyright} 1995.},
author = {Bresnahan, Timothy F. and Trajtenberg, M.},
doi = {10.1016/0304-4076(94)01598-T},
file = {:Users/Zanele/Dropbox (MIT)/MendeleyPapers/Bresnahan, Trajtenberg_1995_General purpose technologies 'Engines of growth'.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Coordination,Growth,Social returns,Technical change},
mendeley-groups = {Part2Paper},
number = {1},
pages = {83--108},
title = {{General purpose technologies 'Engines of growth'?}},
volume = {65},
year = {1995}
}

@article{Brynjolfsson2019,
abstract = {Artificial intelligence (AI) is surpassing human performance in a growing number of domains. However, there is limited evidence of its economic effects. Using data from a digital platform, we study a key application of AI: machine translation. We find that the introduction of a new machine translation system has significantly increased international trade on this platform, increasing exports by 10.9%. Furthermore, heterogeneous treatment effects are consistent with a substantial reduction in translation costs. Our results provide causal evidence that language barriers significantly hinder trade and that AI has already begun to improve economic efficiency in at least one domain.},
author = {Brynjolfsson, Erik and Hui, Xiang and Liu, Meng},
doi = {10.1287/mnsc.2019.3388},
file = {:Users/Zanele/Dropbox (MIT)/MendeleyPapers/Brynjolfsson, Hui, Liu_2019_Does Machine Translation Affect International Trade Evidence from a Large Digital Platform.pdf:pdf},
isbn = {0000000255127},
issn = {0025-1909},
journal = {Management Science},
keywords = {Artificial intelligence,Digital platforms,International trade,Machine learning,Machine translation},
month = {dec},
number = {12},
pages = {5449--5460},
title = {{Does Machine Translation Affect International Trade? Evidence from a Large Digital Platform}},
url = {http://pubsonline.informs.org/doi/10.1287/mnsc.2019.3388},
volume = {65},
year = {2019}
}

@misc{chen2021evaluating,
    title={Evaluating Large Language Models Trained on Code},
    author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
    year={2021},
    eprint={2107.03374},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{BrynjolfssonErik2019Hswm,
author = {Brynjolfsson, Erik and Collis, Avinash},
issn = {0017-8012},
journal = {Harvard business review},
keywords = {Measurement ; Evaluation ; Economic indicators ; Economic aspects ; Models ; Online services ; Gross domestic product ; Quality of life},
number = {6},
pages = {140--},
publisher = {Harvard Business School Press},
title = {{How should we measure the digital economy? Focus on the value created, not just the prices paid}},
volume = {97},
year = {2019}
}
@article{Webb2019,
abstract = {The article examined the nature of the influence of artificial intelligence (AI) on behavior of people in the labor market in Russian regions. The group of authors analyzed the positive and negative impact of AI on behavior of people in the labor market by assessing the effectiveness of human potential use, human learning and competitiveness, increase (reduction) in unemployment, social inequality, impact on the human psyche and its safety. The study found that AI improves human efficiency and competitiveness. At the same time, there is an increase in competition and not everyone can find decent jobs in the labor market. This will boost unemployment. Social status is in the risk zone. Many people are confident that AI serves and will serve the purposes and desires of a small group of rich people, worsening the well-being of the majority of the population and increasing social inequality. Information security and the human psyche have found themselves in the negative sector. The article first considered the social aspect of the impact of artificial intelligence (AI) on behavior of people in the labor market. The main advantage of the article is a systematic approach to assessing the nature of AI's impact on behavior of people in the labor market. The received results can become a basis for the further researches about character of influence of AI on a society, and also decisions on finding of balance between the person and AI on the labor market.},
author = {Webb, Michael},
doi = {10.2139/ssrn.3482150},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {Artificial Intelligence,Behavior of People,Human Potential,Labor Market,Risks,Social Inequality},
number = {4 Special Issue},
pages = {526--532},
title = {{The Impact of Artificial Intelligence on the Labor Market}},
url = {https://www.ssrn.com/abstract=3482150},
volume = {12},
year = {2019}
}



%%%%%%%%%%%%%%%END OF ECONOMICS%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%START OF ROBOTICS %%%%%%%%%%%%%%%%%%

@article{shen2020igibson,
  title={iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes},
  author={Shen, Bokui and Xia, Fei and Li, Chengshu and Mart{\'\i}n-Mart{\'\i}n, Roberto and Fan, Linxi and Wang, Guanzhi and Buch, Shyamal and D'Arpino, Claudia and Srivastava, Sanjana and Tchapmi, Lyne P and others},
  journal={arXiv preprint arXiv:2012.02924},
  year={2020}
}

@inproceedings{schaul2015universal,
  title={Universal value function approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International conference on machine learning},
  pages={1312--1320},
  year={2015},
  organization={PMLR}
}

@book{nof1999handbook,
  title={Handbook of industrial robotics},
  author={Nof, Shimon Y},
  year={1999},
  publisher={John Wiley \& Sons}
}

@article{khoshnevis2004automated,
  title={Automated construction by contour crafting—related robotics and information technologies},
  author={Khoshnevis, Behrokh},
  journal={Automation in construction},
  volume={13},
  number={1},
  pages={5--19},
  year={2004},
  publisher={Elsevier}
}

@article{thorpe1988vision,
  title={Vision and navigation for the Carnegie-Mellon Navlab},
  author={Thorpe, Charles and Hebert, Martial H and Kanade, Takeo and Shafer, Steven A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={10},
  number={3},
  pages={362--373},
  year={1988},
  publisher={IEEE}
}

@misc{kalashnikov2018qtopt,
      title={QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation}, 
      author={Dmitry Kalashnikov and Alex Irpan and Peter Pastor and Julian Ibarz and Alexander Herzog and Eric Jang and Deirdre Quillen and Ethan Holly and Mrinal Kalakrishnan and Vincent Vanhoucke and Sergey Levine},
      year={2018},
      eprint={1806.10293},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{finn2016deep,
  title={Deep spatial autoencoders for visuomotor learning},
  author={Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={512--519},
  year={2016},
  organization={IEEE}
}

% === Image Citations ===
@misc{pandaRobotImage,
    title={Franka Emike Panda Image},
    author={TQ Robotics},
    url={https://www.tq-group.com/fileadmin/_processed_/e/0/csm_franka_panda_back_6525acd90b.png}
}

@misc{pepperRobotImage,
    title={Pepper Robot Image},
    author={SoftBank Robotics},
    url={https://us.softbankrobotics.com/hubfs/SBR_Pepper-Hero_FullRobot.png}
}

% === End Image Citations ===

@article{Levine2018LearningHC,
  title={Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  author={Sergey Levine and P. Pastor and A. Krizhevsky and Deirdre Quillen},
  journal={The International Journal of Robotics Research},
  year={2018},
  volume={37},
  pages={421 - 436}
}

@inproceedings{Gupta2018RobotLI,
  title={Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias},
  author={Abhinav Gupta and Adithyavairavan Murali and Dhiraj Gandhi and Lerrel Pinto},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{ng2000algorithms,
    title={Algorithms for Inverse Reinforcement Learning},
    author={Andrew Y. Ng and Stuart Russell},
    booktitle={International Conference on Machine Learning},
    year={2000}
}

@article{alayrac2020self,
  title={Self-Supervised MultiModal Versatile Networks.},
  author={Alayrac, Jean-Baptiste and Recasens, Adria and Schneider, Rosalia and Arandjelovic, Relja and Ramapuram, Jason and De Fauw, Jeffrey and Smaira, Lucas and Dieleman, Sander and Zisserman, Andrew},
  journal={NeurIPS},
  volume={2},
  number={6},
  pages={7},
  year={2020}
}

@article{kaiser2017one,
  title={One model to learn them all},
  author={Kaiser, Lukasz and Gomez, Aidan N and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1706.05137},
  year={2017}
}

@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Goyal2017TheS,
  title={The “Something Something” Video Database for Learning and Evaluating Visual Common Sense},
  author={R. Goyal and S. Kahou and Vincent Michalski and Joanna Materzynska and S. Westphal and Heuna Kim and Valentin Haenel and Ingo Fr{\"u}nd and P. Yianilos and Moritz Mueller-Freitag and F. Hoppe and Christian Thurau and I. Bax and R. Memisevic},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={5843-5851}
}

@inproceedings{caba2015activitynet,
  title={ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  author={Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={961--970},
  year={2015}
}

@article{Lee2012DiscoveringIP,
  title={Discovering important people and objects for egocentric video summarization},
  author={Yong Jae Lee and Joydeep Ghosh and K. Grauman},
  journal={2012 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2012},
  pages={1346-1353}
}
@article{tsimpoukelli2021multimodal,
  title={Multimodal Few-Shot Learning with Frozen Language Models},
  author={Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={arXiv preprint arXiv:2106.13884},
  year={2021}
}
@article{Smith2019AVIDLM,
  title={AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos},
  author={Laura Smith and Nikita Dhawan and Marvin Zhang and P. Abbeel and Sergey Levine},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.04443}
}

@article{Lynch2020GroundingLI,
  title={Grounding Language in Play},
  author={Corey Lynch and Pierre Sermanet},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.07648}
}

@misc{chen2021learning,
      title={Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos}, 
      author={Annie S. Chen and Suraj Nair and Chelsea Finn},
      year={2021},
      eprint={2103.16817},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@article{Sadeghi2017CAD2RLRS,
  title={{CAD}$^2${RL}: Real Single-Image Flight without a Single Real Image},
  author={Fereshteh Sadeghi and Sergey Levine},
  journal={ArXiv},
  year={2017},
  volume={abs/1611.04201}
}

@article{OpenAI2019SolvingRC,
  title={Solving Rubik's Cube with a Robot Hand},
  author={OpenAI and I. Akkaya and Marcin Andrychowicz and Maciek Chociej and Mateusz Litwin and Bob McGrew and Arthur Petron and Alex Paino and Matthias Plappert and Glenn Powell and Raphael Ribas and Jonas Schneider and N. Tezak and Jerry Tworek and P. Welinder and Lilian Weng and Qiming Yuan and Wojciech Zaremba and Lei Zhang},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.07113}
}

@article{Mahler2017DexNet2D,
  title={Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics},
  author={Jeffrey Mahler and Jacky Liang and Sherdil Niyaz and Michael Laskey and R. Doan and Xinyu Liu and J. A. Ojea and Ken Goldberg},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.09312}
}

@inproceedings{
	RoboImitationPeng20,
	author = {Peng, Xue Bin and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang-Wei Edward and Tan, Jie and Levine, Sergey},
	booktitle={Robotics: Science and Systems},
	year = {2020},
	month = {07},
	title = {Learning Agile Robotic Locomotion Skills by Imitating Animals},
	doi = {10.15607/RSS.2020.XVI.064}
}

@misc{szot2021habitat,
      title={Habitat 2.0: Training Home Assistants to Rearrange their Habitat}, 
      author={Andrew Szot and Alex Clegg and Eric Undersander and Erik Wijmans and Yili Zhao and John Turner and Noah Maestre and Mustafa Mukadam and Devendra Chaplot and Oleksandr Maksymets and Aaron Gokaslan and Vladimir Vondrus and Sameer Dharur and Franziska Meier and Wojciech Galuba and Angel Chang and Zsolt Kira and Vladlen Koltun and Jitendra Malik and Manolis Savva and Dhruv Batra},
      year={2021},
      eprint={2106.14405},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lee2020detect,
  title={Detect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors},
  author={Lee, Michelle A and Tan, Matthew and Zhu, Yuke and Bohg, Jeannette},
  journal={arXiv preprint arXiv:2012.00201},
  year={2020}
}

@inproceedings{lee2020multimodal,
  title={Multimodal Sensor Fusion with Differentiable Filters},
  author={Lee, Michelle A and Yi, Brent and Mart{\'\i}n-Mart{\'\i}n, Roberto and Savarese, Silvio and Bohg, Jeannette},
  booktitle={Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2020}
}

@inproceedings{li2019connecting,
  title={Connecting touch and vision via cross-modal prediction},
  author={Li, Yunzhu and Zhu, Jun-Yan and Tedrake, Russ and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10609--10618},
  year={2019}
}

@article{shenigibson,
  title={iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes},
  author={Shen, Bokui and Xia, Fei and Li, Chengshu and Mart{\i}n-Mart{\i}n, Roberto and Fan, Linxi and Wang, Guanzhi and Buch, Shyamal and D’Arpino, Claudia and Srivastava, Sanjana and Tchapmi, Lyne P and  Vainio, Kent and Fei-Fei, Li and Savarese, Silvio},
  journal={arXiv preprint},
  year={2020}
}

@article{Matterport3D,
  title={Matterport3D: Learning from RGB-D Data in Indoor Environments},
  author={Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Niessner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
  journal={International Conference on 3D Vision (3DV)},
  year={2017}
}

@article{Kolve2017AI2THORAI,
  title={AI2-THOR: An Interactive 3D Environment for Visual AI},
  author={Eric Kolve and R. Mottaghi and Winson Han and Eli VanderBilt and Luca Weihs and Alvaro Herrasti and Daniel Gordon and Yuke Zhu and Abhinav Gupta and Ali Farhadi},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.05474}
}

@report{sanneman2020state,
  title={The State of Industrial Robotics: Emerging Technologies, Challenges, and Key Research Directions},
  author={Lindsay Sanneman and Christopher Fourie and Julie Shah},
  year={2020},
  url={https://www.therobotreport.com/wp-content/uploads/2021/01/2020-Research-Brief-Sanneman-Fourie-Shah.pdf}
}

@article {bock2007construction,
  title = {Construction robotics},
  author = {Bock, Thomas},
  journal = {Autonomous Robots},
  volume = {22},
  number = {3},
  pages = {201--209},
  year = {2007},
  publisher = {Springer}
}

@article{dragan2013formalizing,
  title={Formalizing Assistive Teleoperation},
  author={Dragan, Anca D and Srinivasa, Siddhartha S},
  journal={Robotics: Science and Systems VIII},
  pages={73},
  year={2013},
  publisher={MIT Press}
}

@inproceedings{pinto2016supersizing,
  title={Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours},
  author={Pinto, Lerrel and Gupta, Abhinav},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3406--3413},
  year={2016},
  organization={IEEE}
}

@article{badue2020self,
  title={Self-driving cars: A survey},
  author={Badue, Claudine and Guidolini, R{\^a}nik and Carneiro, Raphael Vivacqua and Azevedo, Pedro and Cardoso, Vinicius Brito and Forechi, Avelino and Jesus, Luan and Berriel, Rodrigo and Paix{\~a}o, Thiago Meireles and Mutz, Filipe and others},
  journal={Expert Systems with Applications},
  pages={113816},
  year={2020},
  publisher={Elsevier}
}

%%%%%%%%%%%%%%%END OF ROBOTICS%%%%%%%%%%%%%%%%%%

@inproceedings{kaushik2019learning,
  title={Learning the difference that makes a difference with counterfactually-augmented data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inbook{Sambasivan2021,
author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
title = {“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445518},
abstract = { AI models are increasingly applied in high-stakes domains like health and conservation.
Data quality carries an elevated significance in high-stakes AI due to its heightened
downstream impact, impacting predictions like cancer detection, wildlife poaching,
and loan allocations. Paradoxically, data is the most under-valued and de-glamorised
aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews
with 53 AI practitioners in India, East and West African countries, and USA. We define,
identify, and present empirical evidence on Data Cascades—compounding events causing
negative, downstream effects from data issues—triggered by conventional AI/ML practices
that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible,
delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing
data excellence as a first-class citizen of AI, resulting in safer and more robust
systems for all.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {39},
numpages = {15}
}


@article{haochen2021spectral,
  author    = {Jeff Z. HaoChen and
               Colin Wei and
               Adrien Gaidon and
               Tengyu Ma},
  title     = {Provable Guarantees for Self-Supervised Deep Learning with Spectral
               Contrastive Loss},
  journal   = {CoRR},
  volume    = {abs/2106.04156},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.04156},
  archivePrefix = {arXiv},
  eprint    = {2106.04156},
  timestamp = {Fri, 11 Jun 2021 11:04:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-04156.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  year={2016},
booktitle={International Conference on Learning Representations},
}

@InProceedings{hewitt2019structural,
    author =      "Hewitt, John and Manning, Christopher D.",
    title =       "{A} Structural Probe for Finding Syntax in Word Representations",
    booktitle =   "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)",
    year =        "2019",
    publisher =   "Association for Computational Linguistics",
    location =    "Minneapolis, USA",
}

@inproceedings{
tenney2018what,
title={What do you learn from context? Probing for sentence structure in contextualized word representations},
author={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJzSgnRcKX},
}

@inproceedings{adi2017finegrained,
title = {Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks},
author={Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
booktitle = {International Conference on Learning Representations},
year = {2017},
}

@InProceedings{belinkov2017what,
  author = 	{Belinkov, Yonatan
		and Durrani, Nadir
		and Dalvi, Fahim
		and Sajjad, Hassan
		and Glass, James},
  title = 	{What do Neural Machine Translation Models Learn about Morphology?},
  booktitle = 	{Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    },
  year = 	{2017},
  publisher = 	{Association for Computational Linguistics},
  pages = 	{861--872},
  location = 	{Vancouver, Canada},
  doi = 	{10.18653/v1/P17-1080},
  url = 	{http://aclweb.org/anthology/P17-1080}
}

@InProceedings{conneau2018what,
  author = 	"Conneau, Alexis
		and Kruszewski, Germ{\'a}n
		and Lample, Guillaume
		and Barrault, Lo{\"i}c
		and Baroni, Marco",
  title = 	"What you can cram into a single {$\backslash$}{\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"2126--2136",
  location = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-1198"
}

@inproceedings{veldhoen2016diagnostic,
    title={Diagnostic Classifiers: Revealing how Neural Networks Process Hierarchical Structure},
    author={Veldhoen, Sara and Hupkes, Dieuwke and Zuidema, Willem},
    year={2016},
    booktitle={Pre-Proceedings of the Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches (CoCo @ NIPS 2016)}
}
@inproceedings{pimentel2020information,
    title = "Information-Theoretic Probing for Linguistic Structure",
    author = "Pimentel, Tiago  and
      Valvoda, Josef  and
      Hall Maudslay, Rowan  and
      Zmigrod, Ran  and
      Williams, Adina  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.420",
    pages = "4609--4622",
}

@misc{voita2020informationtheoretic,
    title={Information-Theoretic Probing with Minimum Description Length},
    author={Elena Voita and Ivan Titov},
    year={2020},
    eprint={2003.12298},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{hewitt2019control,
    title = "Designing and Interpreting Probes with Control Tasks",
    author = "Hewitt, John  and
      Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1275",
}


@InProceedings{chen2020imagegpt,
  title = 	 {Generative Pretraining From Pixels},
  author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1691--1703},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  url = 	 {http://proceedings.mlr.press/v119/chen20s.html},
  abstract = 	 {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.}
}
@misc{wei2021pretrained,
      title={Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning}, 
      author={Colin Wei and Sang Michael Xie and Tengyu Ma},
      year={2021},
      eprint={2106.09226},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{xie2021composed, 
title={Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization}, 
author={Sang Michael Xie and Tengyu Ma and Percy Liang},
journal={International Conference on Machine Learning (ICML)}, 
year={2021} 
}

@article{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}
@article{chen2020big,
  title={Big Self-Supervised Models are Strong Semi-Supervised Learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2006.10029},
  year={2020}
}

@article{tu2020empirical,
  title={An empirical study on robustness to spurious correlations using pre-trained language models},
  author={Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={621--633},
  year={2020},
  publisher={MIT Press}
}
@misc{miller2021accuracy,
      title={Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization}, 
      author={John Miller and Rohan Taori and Aditi Raghunathan and Shiori Sagawa and Pang Wei Koh and Vaishaal Shankar and Percy Liang and Yair Carmon and Ludwig Schmidt},
      year={2021},
      eprint={2107.04649},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@Article{	  SaMa20mat,
  title		= {A Mathematical Exploration of Why Language Models Help
		  Solve Downstream Tasks},
  author	= {N. Saunshi and S. Malladi and S. Arora},
  year		= {2020},
  journal	= {arXiv preprint arXiv:2010.03648}
}

@Article{	  LeLe20pre,
  title		= {Predicting What You Already Know Helps: Provable Self-Supervised Learning},
  author	= {J. D. Lee and Q. Lei and N. Saunshi and J. Zhuo},
  year		= {2020},
  journal	= {arxiv preprint arXiv:2008.01064}
}

@InProceedings{	  zhang20ont,
  author	= {T. Zhang and T. Hashimoto},
  booktitle	= {Association for Computational Linguistics (ACL)},
  title		= {On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies},
  year		= {2020}
}

@misc{ramesh2021zeroshot,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

 
@unpublished{barocas17,
author = {Solon Barocas and Kate Crawford and Aaron Shapiro and Hanna Wallach},
	title = {The Problem With Bias: Allocative Versus Representational Harms in Machine Learning},
note = {Talk at SIGCIS Conference},
year = {2017}
}

@unpublished{crawford17,
author = {Kate Crawford},
	title = {The Problem With Bias},
note = {Keynote at NeurIPS},
year = {2017}
}

@book{noble,
  title={Algorithms of Oppression},
  author={Noble, Safiya Umoja},
  year={2018},
  publisher={New York University Press}
}


@article{schiebinger14,
  title={Scientific research must take gender into account},
  author={Schiebinger, Londa},
  journal={Nature},
  volume={507},
  number={7490},
  pages={9},
  year={2014},
}

@unpublished{schiebinger13,
  author =   {Londa Schiebinger},
  title =    {Machine Translation: Analyzing Gender},
  year =     {2013},
 url = {http://genderedinnovations.stanford.edu/case-studies/nlp.html#tabs-2}
}

@inproceedings{gonen19,
  title={Lipstick on a pig: {D}ebiasing methods cover up systematic gender biases in word embeddings but do not remove them},
  author={Gonen, Hila and Goldberg, Yoav},
  booktitle="Proceedings of NAACL 2019",
  year={2019}
}

@unpublished{datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  note={arXiv preprint arXiv:1803.09010},
  year={2020}
}

@inproceedings{modelcard,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={ACM FAccT},
  pages={220--229},
  year={2019}
}
@article{bender18,
  title={Data statements for natural language processing: Toward mitigating system bias and enabling better science},
  author={Bender, Emily M. and Friedman, Batya},
  journal="TACL",
  volume={6},
  pages={587--604},
  year={2018},
}


@inproceedings{geva-etal-2019-modeling,
    title = "Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets",
    author = "Geva, Mor  and
      Goldberg, Yoav  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1107",
    doi = "10.18653/v1/D19-1107",
    pages = "1161--1166",
    abstract = "Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.",
}

@inproceedings{sap-etal-2019-risk,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1163",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
    abstract = "We investigate how annotators{'} insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet{'}s dialect they are significantly less likely to label the tweet as offensive.",
}

@inproceedings{stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  booktitle="Proceedings of ACL 2021",
  year={2021}
}
@inproceedings{henderson17ethical,
  title={Ethical Challenges in Data-Driven Dialogue Systems},
  author={Henderson, Peter and Sinha, Koustuv and Angelard-Gontier, Nicolas and Ke, Nan Rosemary and Fried, Genevieve and Lowe, Ryan and Pineau, Joelle},
  booktitle={AAAI/ACM AI Ethics and Society Conference},
  year={2017}
}

@inproceedings{zhou-etal-2021-challenges,
    title = "Challenges in Automated Debiasing for Toxic Language Detection",
    author = "Zhou, Xuhui  and
      Sap, Maarten  and
      Swayamdipta, Swabha  and
      Choi, Yejin  and
      Smith, Noah",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.274",
    pages = "3143--3155",
    abstract = "Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.",
}

@inproceedings{park-etal-2018-reducing,
    title = "Reducing Gender Bias in Abusive Language Detection",
    author = "Park, Ji Ho  and
      Shin, Jamin  and
      Fung, Pascale",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1302",
    doi = "10.18653/v1/D18-1302",
    pages = "2799--2804",
    abstract = "Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, {``}You are a good woman{''} was considered {``}sexist{''} when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98{\%} and can be extended to correct model bias in other scenarios.",
}

@inproceedings{xia-etal-2020-demoting,
    title = "Demoting Racial Bias in Hate Speech Detection",
    author = "Xia, Mengzhou  and
      Field, Anjalie  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.socialnlp-1.2",
    doi = "10.18653/v1/2020.socialnlp-1.2",
    pages = "7--14",
    abstract = "In the task of hate speech detection, there exists a high correlation between African American English (AAE) and annotators{'} perceptions of toxicity in current datasets. This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech (high false positive rate) by current hate speech classifiers. Here, we use adversarial training to mitigate this bias. Experimental results on one hate speech dataset and one AAE dataset suggest that our method is able to reduce the false positive rate for AAE text with only a minimal compromise on the performance of hate speech classification.",
}

@inproceedings{wang2019balanced,
  title={Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations},
  author={Wang, Tianlu and Zhao, Jieyu and Yatskar, Mark and Chang, Kai-Wei and Ordonez, Vicente},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5310--5319},
  year={2019}
}

@inproceedings{zhao-etal-2017-men,
    title = "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1323",
    doi = "10.18653/v1/D17-1323",
    pages = "2979--2989",
    abstract = "Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33{\%} more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68{\%} at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5{\%} and 40.5{\%} for multilabel classification and visual semantic role labeling, respectively。",
}

@inproceedings{hashimoto2018fairness,
  title={Fairness without demographics in repeated loss minimization},
  author={Hashimoto, Tatsunori and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={1929--1938},
  year={2018},
  organization={PMLR}
}

@inproceedings{jia-etal-2020-mitigating,
    title = "Mitigating Gender Bias Amplification in Distribution by Posterior Regularization",
    author = "Jia, Shengyu  and
      Meng, Tao  and
      Zhao, Jieyu  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.264",
    doi = "10.18653/v1/2020.acl-main.264",
    pages = "2936--2942",
    abstract = "Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models{'} top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.",
}

@article{koenecke2020racial,
  title={Racial disparities in automated speech recognition},
  author={Koenecke, Allison and Nam, Andrew and Lake, Emily and Nudell, Joe and Quartey, Minnie and Mengesha, Zion and Toups, Connor and Rickford, John R and Jurafsky, Dan and Goel, Sharad},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={14},
  pages={7684--7689},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{blodgett17,
  title={Racial Disparity in Natural Language Processing: A Case Study of Social Media {A}frican-{A}merican {E}nglish},
  author={Blodgett, Su Lin and O'Connor, Brendan},
  booktitle={Fairness, Accountability, and Transparency in Machine Learning ({FAT/ML}) Workshop, {KDD}},
  year={2017}
}

@unpublished{solaiman2021process,
  title={Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets},
  author={Solaiman, Irene and Dennison, Christy},
  note={arXiv preprint arXiv:2106.10328},
  year={2021}
}

@inproceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}

@inproceedings{xu-etal-2021-detoxifying,
    title = "Detoxifying Language Models Risks Marginalizing Minority Voices",
    author = "Xu, Albert  and
      Pathak, Eshaan  and
      Wallace, Eric  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Klein, Dan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.190",
    doi = "10.18653/v1/2021.naacl-main.190",
    pages = "2390--2397",
    abstract = "Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.",
}

@book{benjamin2019,
  title={Race after Technology},
  author={Benjamin, Ruha},
  publisher={Polity Press},
  year= {2019},
}

@book{crawford2021,
  title={Atlas of AI},
  author={Kate Crawford},
  publisher={Yale University Press},
  year={2021}
}

@unpublished{wilson19,
author = {Benjamin Wilson and Judy Hoffman and Jamie Morgenstern},
	title = {Predictive Inequity in Object Detection},
note = {https://arxiv.org/pdf/1902.11097.pdf},
year = {2019}
}

@book{datafeminism,
  title={Data Feminism},
  author={D'Ignazio, Catherine and Klein, Lauren F.},
  year={2020},
  publisher={MIT Press}
}

Endnote
@inproceedings{waseem-etal-2017-understanding,
    title = "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
    author = "Waseem, Zeerak  and
      Davidson, Thomas  and
      Warmsley, Dana  and
      Weber, Ingmar",
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3012",
    doi = "10.18653/v1/W17-3012",
    pages = "78--84",
    abstract = "As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.",
}

@inproceedings{breitfeller-etal-2019-finding,
    title = "Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts",
    author = "Breitfeller, Luke  and
      Ahn, Emily  and
      Jurgens, David  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1176",
    doi = "10.18653/v1/D19-1176",
    pages = "1664--1674",
    abstract = "Microaggressions are subtle, often veiled, manifestations of human biases. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of microaggressions in communication has made it difficult for researchers to analyze their exact nature, and to quantify and extract microaggressions automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of microaggressions based on a small subset of data that we have. We then create two datasets: one with examples of diverse types of microaggressions recollected by their targets, and another with gender-based microaggressions in public conversations on social media. We introduce a new, more objective, criterion for annotation and an active-learning based procedure that increases the likelihood of surfacing posts containing microaggressions. Finally, we analyze the trends that emerge from these new datasets.",
}

@inproceedings{voigt-etal-2018-rtgender,
    title = "{R}t{G}ender: A Corpus for Studying Differential Responses to Gender",
    author = "Voigt, Rob  and
      Jurgens, David  and
      Prabhakaran, Vinodkumar  and
      Jurafsky, Dan  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1445",
}


@article{Johnson2020,
  doi = {10.1007/s11229-020-02696-y},
  url = {https://doi.org/10.1007/s11229-020-02696-y},
  year = {2020},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  author = {Gabbrielle M. Johnson},
  title = {Algorithmic bias: on the implicit biases of social technology},
  journal = {Synthese}
}

@article{Hellman2020,
  title={Measuring algorithmic fairness},
  author={Hellman, Deborah},
  journal={Va. L. Rev.},
  volume={106},
  pages={811},
  year={2020},
  publisher={HeinOnline}
}

@article{Chouldechova2020,
  doi = {10.1145/3376898},
  url = {https://doi.org/10.1145/3376898},
  year = {2020},
  month = apr,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {63},
  number = {5},
  pages = {82--89},
  author = {Alexandra Chouldechova and Aaron Roth},
  title = {A snapshot of the frontiers of fairness in machine learning},
  journal = {Communications of the {ACM}}
}

@misc{CorbettDavies2018,
      title={The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning}, 
      author={Sam Corbett-Davies and Sharad Goel},
      year={2018},
      eprint={1808.00023},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@inproceedings{Abebe2020,
  title={Roles for computing in social change},
  author={Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G},
  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  pages={252--260},
  year={2020}
}

@inproceedings{Katell2020,
  doi = {10.1145/3351095.3372874},
  url = {https://doi.org/10.1145/3351095.3372874},
  year = {2020},
  month = jan,
  publisher = {{ACM}},
  author = {Michael Katell and Meg Young and Dharma Dailey and Bernease Herman and Vivian Guetler and Aaron Tam and Corinne Bintz and Daniella Raz and P. M. Krafft},
  title = {Toward situated interventions for algorithmic equity},
  booktitle = {Proceedings of the 2020 Conference on Fairness,  Accountability,  and Transparency}
}

@inproceedings{Hanna2020,
  title={Towards a critical race methodology in algorithmic fairness},
  author={Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={501--512},
  year={2020}
}

@incollection{Gebru2021,
title = {Race and Gender},
author = {Timnit Gebru},
booktitle = {The Oxford Handbook of Ethics of AI},
editor = {Markus Dirk Dubber and Frank Pasquale and Sunit Das},
year = {2021},
publisher = {Oxford},
isbn = {9780190067397}
}

@inproceedings{Strengers2020,
  doi = {10.1145/3313831.3376315},
  url = {https://doi.org/10.1145/3313831.3376315},
  year = {2020},
  month = apr,
  publisher = {{ACM}},
  author = {Yolande Strengers and Lizhen Qu and Qiongkai Xu and Jarrod Knibbe},
  title = {Adhering,  Steering,  and Queering: Treatment of Gender in Natural Language Generation},
  booktitle = {Proceedings of the 2020 {CHI} Conference on Human Factors in Computing Systems}
}

@article{Oliva2021,
  title={Fighting hate speech, silencing drag queens? Artificial intelligence in content moderation and risks to LGBTQ voices online},
  author={Oliva, Thiago Dias and Antonialli, Dennys Marcelo and Gomes, Alessandra},
  journal={Sexuality \& Culture},
  volume={25},
  number={2},
  pages={700--732},
  year={2021},
  publisher={Springer}
}

@article{Tomasev2021,
Author = {Nenad Tomasev and Kevin R. McKee and Jackie Kay and Shakir Mohamed},
Title = {Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities},
Year = {2021},
journal = {arXiv:2102.04257},
Doi = {10.1145/3461702.3462540},
}

@book{Longino1990,
author = {Longino, Helen},
year = {1990},
title = {Science as Social Knowledge: Values and Objectivity in Scientific Inquiry},
publisher = {Princeton University Press},
address = {Princeton}
}

@article{OConnor2019,
  title={Dynamics and diversity in epistemic communities},
  author={O’Connor, Cailin and Bruner, Justin},
  journal={Erkenntnis},
  volume={84},
  number={1},
  pages={101--119},
  year={2019},
  publisher={Springer}
}

@article{Hofstra2020,
  doi = {10.1073/pnas.1915378117},
  url = {https://doi.org/10.1073/pnas.1915378117},
  year = {2020},
  month = apr,
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {17},
  pages = {9284--9291},
  author = {Bas Hofstra and Vivek V. Kulkarni and Sebastian Munoz-Najar Galvez and Bryan He and Dan Jurafsky and Daniel A. McFarland},
  title = {The Diversity{\textendash}Innovation Paradox in Science},
  journal = {Proceedings of the National Academy of Sciences}
}

@article{Nielsen2017,
  doi = {10.1073/pnas.1700616114},
  url = {https://doi.org/10.1073/pnas.1700616114},
  year = {2017},
  month = feb,
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {8},
  pages = {1740--1742},
  author = {Mathias Wullum Nielsen and Sharla Alegria and Love B\"{o}rjeson and Henry Etzkowitz and Holly J. Falk-Krzesinski and Aparna Joshi and Erin Leahey and Laurel Smith-Doerr and Anita Williams Woolley and Londa Schiebinger},
  title = {Opinion: Gender diversity leads to better science},
  journal = {Proceedings of the National Academy of Sciences}
}

@book{Harding2015,
author = {Sandra Harding},
doi = {doi:10.7208/9780226241531},
url = {https://doi.org/10.7208/9780226241531},
title = {Objectivity and Diversity},
year = {2015},
publisher = {University of Chicago Press}
}

@inproceedings{Wilson2021,
  author = {Christo Wilson and Avijit Ghosh and Shan Jiang and Alan Mislove and Lewis Baker and Janelle Szary and Kelly Trindel and Frida Polli},
  title = {{Building and Auditing Fair Algorithms: A Case Study in Candidate Screening}},
  booktitle = {{Proceedings of the Conference on Fairness, Accountability, and Transparency (FAccT 2021)}},
  address = {Virtual Event, Canada},
  month = {March},
  year = {2021},
}

@article{Creel2020,
  doi = {10.1086/709729},
  url = {https://doi.org/10.1086/709729},
  year = {2020},
  month = oct,
  publisher = {University of Chicago Press},
  volume = {87},
  number = {4},
  pages = {568--589},
  author = {Kathleen A. Creel},
  title = {Transparency in Complex Computational Systems},
  journal = {Philosophy of Science}
}

@article{Burrell2016,
  doi = {10.1177/2053951715622512},
  url = {https://doi.org/10.1177/2053951715622512},
  year = {2016},
  month = jan,
  publisher = {{SAGE} Publications},
  volume = {3},
  number = {1},
  pages = {205395171562251},
  author = {Jenna Burrell},
  title = {How the machine `thinks': Understanding opacity in machine learning algorithms},
  journal = {Big Data {\&} Society}
}

@article{Lipton2018,
  doi = {10.1145/3233231},
  url = {https://doi.org/10.1145/3233231},
  year = {2018},
  month = sep,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {61},
  number = {10},
  pages = {36--43},
  author = {Zachary C. Lipton},
  title = {The mythos of model interpretability},
  journal = {Communications of the {ACM}}
}

@inproceedings{Raji2020,
  doi = {10.1145/3351095.3372873},
  url = {https://doi.org/10.1145/3351095.3372873},
  year = {2020},
  month = jan,
  publisher = {{ACM}},
  author = {Inioluwa Deborah Raji and Andrew Smart and Rebecca N. White and Margaret Mitchell and Timnit Gebru and Ben Hutchinson and Jamila Smith-Loud and Daniel Theron and Parker Barnes},
  title = {Closing the {AI} accountability gap},
  booktitle = {Proceedings of the 2020 Conference on Fairness,  Accountability,  and Transparency}
}

@article{ajunwa2019paradox,
  title={The paradox of automation as anti-bias intervention},
  author={Ajunwa, Ifeoma},
  journal={Cardozo L. Rev.},
  volume={41},
  pages={1671},
  year={2019},
  publisher={HeinOnline}
}
@article{Hellman2021Big,
  title={Big Data and Compounding Injustice},
  author={Hellman, Deborah},
  journal={Journal of Moral Philosophy, forthcoming, Virginia Public Law and Legal Theory Research Paper 2021-27},
  year={2021}
}

@article{Cao_2020,
   title={Toward Gender-Inclusive Coreference Resolution},
   url={http://dx.doi.org/10.18653/v1/2020.acl-main.418},
   DOI={10.18653/v1/2020.acl-main.418},
   journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   publisher={Association for Computational Linguistics},
   author={Cao, Yang Trista and Daumé III, Hal},
   year={2020}
}

@article{Keyes2018,
  doi = {10.1145/3274357},
  url = {https://doi.org/10.1145/3274357},
  year = {2018},
  month = nov,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {2},
  number = {{CSCW}},
  pages = {1--22},
  author = {Os Keyes},
  title = {The Misgendering Machines},
  journal = {Proceedings of the {ACM} on Human-Computer Interaction}
}

@book{Gandy2021,
    title = {The Panoptic Sort},
    author = {Gandy, Jr., Oscar H.},
    year = {2021},
    month = {07},
    day = {07},
    pagecount = {352},
    isbn = {9780197579428}
}

@inproceedings{Antoniak2021,
title = {Bad Seeds: Evaluating Lexical Methods for Bias Measurement},
author = {Maria Antoniak and David Mimno},
booktitle = {Proceedings of ACL 2021},
year = {2021}
}

@misc{Czarnowska2021,
      title={Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics}, 
      author={Paula Czarnowska and Yogarshi Vyas and Kashif Shah},
      year={2021},
      eprint={2106.14574},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{williams2020psychology,
  title={Psychology cannot afford to ignore the many harms caused by microaggressions},
  author={Williams, Monnica T.},
  journal={Perspectives on Psychological Science},
  volume={15},
  number={1},
  pages={38--43},
  year={2020},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{spencer2016stereotype,
  title={Stereotype threat},
  author={Spencer, Steven J. and Logel, Christine and Davies, Paul G.},
  journal={Annual Review of Psychology},
  volume={67},
  pages={415--437},
  year={2016},
  publisher={Annual Reviews}
}

@article{lum2016predict,
  title={To predict and serve?},
  author={Lum, Kristian and Isaac, William},
  journal={Significance},
  volume={13},
  number={5},
  pages={14--19},
  year={2016},
  publisher={Wiley Online Library}
}

@inproceedings{ensign2018runaway,
  title={Runaway feedback loops in predictive policing},
  author={Ensign, Danielle and Friedler, Sorelle A and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle={Conference on Fairness, Accountability and Transparency},
  pages={160--171},
  year={2018},
  organization={PMLR}
}

@article{dinan21,
  title={Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling
},
author={Emily Dinan and Gavin Abercrombie and A. Stevie Bergman and Shannon Spruit and Dirk Hovy and Y-Lan Boureau and Verena Rieser},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03451},
  url={https://arxiv.org/abs/2107.03451}
}

@inproceedings{nozza21,
    title = "{HONEST}: Measuring Hurtful Sentence Completion in Language Models",
    author = "Nozza, Debora  and
      Bianchi, Federico  and
      Hovy, Dirk",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.191",
    doi = "10.18653/v1/2021.naacl-main.191",
    pages = "2398--2406",
    abstract = "Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3{\%} of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9{\%} of the time, and in 4{\%} to homosexuality when the target is male. The results raise questions about the use of these models in production settings.",
}

@inproceedings{sheng-etal-2019-woman,
    title = "The Woman Worked as a Babysitter: On Biases in Language Generation",
    author = "Sheng, Emily  and
      Chang, Kai-Wei  and
      Natarajan, Premkumar  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1339",
    doi = "10.18653/v1/D19-1339",
    pages = "3407--3412",
    abstract = "We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.",
}

@inproceedings{jurgens-etal-2019-just,
    title = "A Just and Comprehensive Strategy for Using {NLP} to Address Online Abuse",
    author = "Jurgens, David  and
      Hemphill, Libby  and
      Chandrasekharan, Eshwar",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1357",
    doi = "10.18653/v1/P19-1357",
    pages = "3658--3666",
    abstract = "Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.",
}

@article{schick2021self,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2103.00453},
  year={2021}
}

@inproceedings{liu2021fly,
  title={On-the-Fly Controlled Text Generation with Experts and Anti-Experts},
  author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A. and Choi, Yejin},
  booktitle={Proceedings of ACL 2021},
  year={2021}
  }
  
  @article{xu2020recipes,
  title={Recipes for safety in open-domain chatbots},
  author={Xu, Jing and Ju, Da and Li, Margaret and Boureau, Y-Lan and Weston, Jason and Dinan, Emily},
  journal={arXiv preprint arXiv:2010.07079},
  year={2020}
}

@article{DeCamp2020,
  doi = {10.1093/jamia/ocaa094},
  url = {https://doi.org/10.1093/jamia/ocaa094},
  year = {2020},
  month = jun,
  publisher = {Oxford University Press ({OUP})},
  volume = {27},
  number = {12},
  pages = {2020--2023},
  author = {Matthew DeCamp and Charlotta Lindvall},
  title = {Latent bias and the implementation of artificial intelligence in medicine},
  journal = {Journal of the American Medical Informatics Association}
}
  
  @Book{JonesGalliers,
  editor =       "Sparck Jones, Karen and Julia R. Galliers",
  title =    "Evaluating Natural Language Processing Systems",
  publisher =    "Springer",
  year =         "1996",
}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTERACTION STARTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@inproceedings{QianYang2018,
 author = {Qian Yang and Jina Suh and Nan-Chen Chen and Gonzalo Ramos},
 booktitle = {DIS '18: Proceedings of the 2018 Designing Interactive Systems Conference},
 publisher = {ACM},
 title = {Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models},
 year = {2018}
}

@inproceedings{Dove2017uxdesign,
 author = {Graham Dove and Kim Halskov and Jodi Forlizzi and John Zimmerman},
 booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
 publisher = {ACM},
 title = {UX Design Innovation: Challenges for Working with Machine Learning as a Design Material},
 year = {2017}
}

@inproceedings{Cooper2014face,
  title={About face: the essentials of interaction design},
  author={Alan Cooper and Robert Reimann and David Cronin and Christopher Noessel},
  booktitle={John Wiley \& Sons},
  year={2014}
}

@inproceedings{Yang2016wireframing,
 author = {Qian Yang and John Zimmerman and Aaron Steinfeld and Anthony Tomasic},
 booktitle = {Proceedings of the 2016 ACM Conference on Designing Interactive Systems},
 publisher = {ACM},
 title = {Planning Adaptive Mobile Experiences When Wireframing},
 year = {2016}
}

@inproceedings{Fails2003design,
 author = {Jerry Alan Fails and Dan R. Olsen},
 booktitle = {Proceedings of the conference on Human factors in computing systems},
 publisher = {ACM},
 title = {A design tool for camera-based interaction},
 year = {2003}
}

@inproceedings{Amershi2012regroup,
 author = {Saleema Amershi and James Fogarty and Daniel Weld},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 publisher = {ACM},
 title = {Regroup: Interactive Machine Learning for On-demand Group Creation in Social Networks},
 year = {2012}
}

@inproceedings{Horvitz1999mixedinit,
 author = {Eric Horvitz},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 publisher = {ACM},
 title = {Principles of mixed-initiative user interfaces},
 year = {1999}
}

@inproceedings{Myers2000ui,
 author = {Brad A. Myers and Scott E. Hudson and Randy Pausch},
 booktitle = {ACM Transactions on Computer Human Interaction},
 publisher = {ACM},
 title = {Past, Present and Future of User Interface Software Tools},
 year = {2000}
}

@inproceedings{Reynolds2021prompt,
 author = {Laria Reynolds and Kyle McDonell},
 booktitle = {Extended Abstract, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 publisher = {ACM},
 title = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
 year = {2021}
}

@inproceedings{Hendrycks2020Measuring,
 author = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
 booktitle = {International Conference on Learning Representations (ICLR)},
 title = {Measuring massive multitask language understanding},
 year = {2021}
}

@inproceedings{Liu2021incontext,
 author = {Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
 booktitle = {ArXiv},
 title = {What Makes Good In-Context Examples for GPT-3?},
 year = {2021}
}

@inproceedings{Zhao2021calibrate,
 author = {Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
 booktitle = {ICML},
 title = {Calibrate Before Use: Improving Few-Shot Performance of Language Models},
 year = {2021}
}

@inproceedings{Engelbart1963augment,
 author = {Douglas C. Engelbart},
 booktitle = {Computer-supported cooperative work: a book of readings},
 title = {A Conceptual Framework for the Augmentation of Man's Intellect},
 year = {1963}
}

@inproceedings{Shneiderman1997direct,
 author = {Ben Shneiderman},
 booktitle = {Interactions},
publisher = {ACM},
 title = {Direct manipulation vs. interface agents},
 year = {1997}
}

@article{Lee2019,
  author =       "Min Kyung Lee and Daniel Kusbit and Anson Kahng and Ji Tae Kim and Xinran Yuan and Allissa Chan and Daniel See and Ritesh Noothigattu and Siheon Lee and Alexandros Psomas and Ariel D. Procaccia",
  year =         "2019",
  title =        "WeBuildAI: Participatory framework for algorithmic governance",
  journal =      "CSCW",
}

@inproceedings{smith2020keeping,
  title={Keeping Community in the Loop: Understanding Wikipedia Stakeholder Values for Machine Learning-Based Systems},
  author={Smith, C Estelle and Yu, Bowen and Srivastava, Anjali and Halfaker, Aaron and Terveen, Loren and Zhu, Haiyi},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2020}
}

@article{Chandrasekharan2018internet,
 author = {Chandrasekharan, Eshwar and Samory, Mattia and Jhaver, Shagun and Charvat, Hunter and Bruckman, Amy and Lampe, Cliff and Eisenstein, Jacob and Gilbert, Eric},
 title = {The Internet's Hidden Rules: An Empirical Study of Reddit Norm Violations at Micro, Meso, and Macro Scales},
 journal = {Proc. ACM Hum.-Comput. Interact.},
 issue_date = {November 2018},
 volume = {2},
 number = {CSCW},
 month = nov,
 year = {2018},
 issn = {2573-0142},
 pages = {32:1--32:25},
 articleno = {32},
 numpages = {25},
 url = {http://doi.acm.org/10.1145/3274301},
 doi = {10.1145/3274301},
 acmid = {3274301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {community norms, mixed methods, moderation, online communities},
}

@article{Hancock2020AI,
  author =       "Jeffrey T Hancock and Mor Naaman and Karen Levy",
  year =         "2020",
  title =        "AI-Mediated Communication: Definition, Research Agenda, and Ethical Considerations",
  journal =      "Journal of Computer-Mediated Communication",
}

@article{Weiner2018,
  author =       "Ken Weiner",
  year =         "2018",
  title =        "Can AI Create True Art?",
  journal =      "Scientific American",
}

@inproceedings{Buschek2021writing,
  title={The Impact of Multiple Parallel Phrase Suggestions on Email Input and Composition Behaviour of Native and Non-Native English Writers},
  author={Daniel Buschek and Martin Zurn and Malin Eiband},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  year={2021}
}

@inproceedings{yang2020aiexamine,
  title={Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design},
  author={Qian Yang and Aaron Steinfeld and Carolyn P Rose and and John Zimmerman},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  year={2020}
}

@inproceedings{Karamcheti2021latent,
  title={Learning Visually Guided Latent Actions for Assistive Teleoperation},
  author={Siddharth Karamcheti and Albert J. Zhai and Dylan P. Losey and Dorsa Sadigh},
  booktitle={Learning for Dynamics and Control},
  year={2021}
}

@article{Shannon1948AMT,
  title={A mathematical theory of communication},
  author={C. Shannon},
  journal={Bell Syst. Tech. J.},
  year={1948},
  volume={27},
  pages={379-423}
}

@article{Clark2020ELECTRAPT,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.10555}
}

@article{Dosovitskiy2021AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={A. Dosovitskiy and L. Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and M. Dehghani and Matthias Minderer and G. Heigold and S. Gelly and Jakob Uszkoreit and N. Houlsby},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.11929}
}

@inproceedings{Oord2017NeuralDR,
  title={Neural Discrete Representation Learning},
  author={A{\"a}ron van den Oord and Oriol Vinyals and K. Kavukcuoglu},
  booktitle={NIPS},
  year={2017}
}

@article{Oord2018RepresentationLW,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}

@inproceedings{Oord2016WaveNetAG,
  title={WaveNet: A Generative Model for Raw Audio},
  author={A{\"a}ron van den Oord and S. Dieleman and H. Zen and K. Simonyan and Oriol Vinyals and A. Graves and Nal Kalchbrenner and A. Senior and K. Kavukcuoglu},
  booktitle={SSW},
  year={2016}
}

@article{Sennrich2016NeuralMT,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Rico Sennrich and B. Haddow and Alexandra Birch},
  journal={ArXiv},
  year={2016},
  volume={abs/1508.07909}
}

@article{Schuster2012JapaneseAK,
  title={Japanese and Korean voice search},
  author={M. Schuster and Kaisuke Nakajima},
  journal={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2012},
  pages={5149-5152}
}
@inproceedings{Kudo2018SentencePieceAS,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Taku Kudo and John Richardson},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Zhilin Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}

@article{Raffel2020ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and W. Li and Peter J. Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/1910.10683}
}

@article{Janner2021ReinforcementLA,
  title={Reinforcement Learning as One Big Sequence Modeling Problem},
  author={Michael Janner and Qiyang Li and Sergey Levine},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.02039}
}

@article{SohlDickstein2015DeepUL,
  title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author={J. Sohl-Dickstein and Eric A. Weiss and Niru Maheswaranathan and S. Ganguli},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.03585}
}

@article{Ho2020DenoisingDP,
  title={Denoising Diffusion Probabilistic Models},
  author={Jonathan Ho and Ajay Jain and P. Abbeel},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.11239}
}

@article{Song2019GenerativeMB,
  title={Generative Modeling by Estimating Gradients of the Data Distribution},
  author={Yang Song and S. Ermon},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.05600}
}

@article{Kingma2014AutoEncodingVB,
  title={Auto-Encoding Variational Bayes},
  author={Diederik P. Kingma and M. Welling},
  journal={CoRR},
  year={2014},
  volume={abs/1312.6114}
}

@inproceedings{Kingma2018GlowGF,
  title={Glow: Generative Flow with Invertible 1x1 Convolutions},
  author={Diederik P. Kingma and Prafulla Dhariwal},
  booktitle={NeurIPS},
  year={2018}
}

@misc{dinh2015nice,
      title={NICE: Non-linear Independent Components Estimation}, 
      author={Laurent Dinh and David Krueger and Yoshua Bengio},
      year={2015},
      eprint={1410.8516},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Wu2018UnsupervisedFL,
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination},
  author={Zhirong Wu and Yuanjun Xiong and Stella X. Yu and Dahua Lin},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3733-3742}
}

@article{He2020MomentumCF,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9726-9735}
}

@article{Grill2020BootstrapYO,
  title={Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning},
  author={Jean-Bastien Grill and Florian Strub and Florent Altch'e and C. Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and B. A. Pires and Z. Guo and M. G. Azar and Bilal Piot and K. Kavukcuoglu and R. Munos and Michal Valko},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.07733}
}

@article{Caron2021EmergingPI,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Mathilde Caron and Hugo Touvron and Ishan Misra and Herv'e J'egou and J. Mairal and Piotr Bojanowski and Armand Joulin},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.14294}
}

@article{Zhang2020ContrastiveLO,
  title={Contrastive Learning of Medical Visual Representations from Paired Images and Text},
  author={Yuhao Zhang and Hang Jiang and Y. Miura and Christopher D. Manning and C. Langlotz},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.00747}
}

@inproceedings{Iida2021TABBIEPR,
  title={TABBIE: Pretrained Representations of Tabular Data},
  author={H. Iida and Dung Thai and Varun Manjunatha and Mohit Iyyer},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{Lu2019ViLBERTPT,
  title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author={Jiasen Lu and Dhruv Batra and Devi Parikh and Stefan Lee},
  booktitle={NeurIPS},
  year={2019}
}

@article{Tamkin2021DABS,
  title={DABS: a Domain-Agnostic Benchmark for Self-Supervised Learning},
  author={Alex Tamkin and Vincent Liu and Rongfei Lu and Daniel Fein and Colin Schultz and Noah Goodman},
  year={2021},
}

@article{Tamkin2021ViewmakerNL,
  title={Viewmaker Networks: Learning Views for Unsupervised Representation Learning},
  author={A. Tamkin and Mike Wu and Noah D. Goodman},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.07432}
}

@article{Stiennon2020LearningTS,
  title={Learning to summarize from human feedback},
  author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan J. Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.01325}
}

@article{Schwarzer2021PretrainingRF,
  title={Pretraining Representations for Data-Efficient Reinforcement Learning},
  author={Max Schwarzer and Nitarshan Rajkumar and Michael Noukhovitch and Ankesh Anand and Laurent Charlin and Devon Hjelm and Philip Bachman and Aaron C. Courville},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04799}
}

@inproceedings{Haber2018LearningTP,
  title={Learning to Play with Intrinsically-Motivated Self-Aware Agents},
  author={N. Haber and Damian Mrowca and Li Fei-Fei and Daniel Yamins},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{Campos2020ExploreDA,
  title={Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills},
  author={V{\'i}ctor Campos and Alexander Trott and Caiming Xiong and R. Socher and Xavier Giro-i-Nieto and Jordi Torres},
  booktitle={ICML},
  year={2020}
}

@inproceedings{Mohamed2015VariationalIM,
  title={Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
  author={Shakir Mohamed, Danilo Jimenez Rezend},
  booktitle={NIPS},
  year={2015}
}

@techreport{singh2005intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Singh, Satinder and Barto, Andrew G and Chentanez, Nuttapong},
  year={2005},
  institution={MASSACHUSETTS UNIV AMHERST DEPT OF COMPUTER SCIENCE}
}

@article{Klyubin2005EmpowermentAU,
  title={Empowerment: a universal agent-centric measure of control},
  author={A. S. Klyubin and D. Polani and Chrystopher L. Nehaniv},
  journal={2005 IEEE Congress on Evolutionary Computation},
  year={2005},
  volume={1},
  pages={128-135 Vol.1}
}

@article{Salge2013EmpowermentA,
  title={Empowerment - an Introduction},
  author={Christoph Salge and C. Glackin and D. Polani},
  journal={ArXiv},
  year={2013},
  volume={abs/1310.1863}
}

@article{Liu2021BehaviorFT,
  title={Behavior From the Void: Unsupervised Active Pre-Training},
  author={Hao Liu and P. Abbeel},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.04551}
}

@article{Schmidhuber2019ReinforcementLU,
  title={Reinforcement Learning Upside Down: Don't Predict Rewards - Just Map Them to Actions},
  author={J. Schmidhuber},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.02875}
}

@article{Srivastava2019TrainingAU,
  title={Training Agents using Upside-Down Reinforcement Learning},
  author={R. Srivastava and Pranav Shyam and Filipe Wall Mutz and Wojciech Jaśkowski and J. Schmidhuber},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.02877}
}

@article{Chen2021DecisionTR,
  title={Decision Transformer: Reinforcement Learning via Sequence Modeling},
  author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and M. Laskin and P. Abbeel and A. Srinivas and Igor Mordatch},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.01345}
}

@article{Baker2020EmergentTU,
  title={Emergent Tool Use From Multi-Agent Autocurricula},
  author={Bowen Baker and I. Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.07528}
}

@book{lauer.2017,
 ISBN = {9780231168083},
 URL = {http://www.jstor.org/stable/10.7312/laue16808},
 author = {Josh Lauer},
 publisher = {Columbia University Press},
 title = {Creditworthy: {A} History of Consumer Surveillance and Financial Identity in {A}merica},
 year = {2017}
}

@book{zuboff.2018,
author = {Zuboff, Shoshana},
title = {The Age of Surveillance Capitalism: {T}he Fight for a Human Future at the New Frontier of Power},
year = {2018},
isbn = {1610395697},
}

@article{mac.2021,
  title = {Surveillance Nation},
  author = {Ryan Mac and Caroline Haskins and Brianna Sacks and Logan McDonald},
  journal = {Buzzfeed News},
  year = {2021},
  month = {9 April},
  date = {2021-04-09},
  url = {https://www.buzzfeednews.com/article/ryanmac/clearview-ai-local-police-facial-recognition},
  note = {Accessed 2021-07-18}
}

@article{kalluri.2020,
  title = {Don’t ask if artificial intelligence is good or fair, ask how it shifts power},
  author = {Pratyusha Kalluri},
  journal = {Nature},
  volume = 583,
  number = 169,
  year = 2020,
  url = {https://www.nature.com/articles/d41586-020-02003-2},
  doi = {https://doi.org/10.1038/d41586-020-02003-2}
}

@unpublished{albright.2019,
  author = {Alex Albright},
  title = {If You Give a Judge a Risk Score: {E}vidence from {K}entucky Bail Decisions},
  url = {https://thelittledataset.com/about_files/albright_judge_score.pdf},
  note = {Accessed 2021-07-18},
  year = 2019
}

@book{ferguson.2017,
 ISBN = {9781479892822},
 URL = {http://www.jstor.org/stable/j.ctt1pwtb27},
 author = {Andrew Guthrie Ferguson},
 publisher = {NYU Press},
 title = {The Rise of Big Data Policing: {S}urveillance, Race, and the Future of Law Enforcement},
 year = {2017}
}

@article {salganik.2020,
	author = {Salganik, Matthew J. and Lundberg, Ian and Kindel, Alexander T. and Ahearn, Caitlin E. and Al-Ghoneim, Khaled and Almaatouq, Abdullah and Altschul, Drew M. and Brand, Jennie E. and Carnegie, Nicole Bohme and Compton, Ryan James and Datta, Debanjan and Davidson, Thomas and Filippova, Anna and Gilroy, Connor and Goode, Brian J. and Jahani, Eaman and Kashyap, Ridhi and Kirchner, Antje and McKay, Stephen and Morgan, Allison C. and Pentland, Alex and Polimis, Kivan and Raes, Louis and Rigobon, Daniel E. and Roberts, Claudia V. and Stanescu, Diana M. and Suhara, Yoshihiko and Usmani, Adaner and Wang, Erik H. and Adem, Muna and Alhajri, Abdulla and AlShebli, Bedoor and Amin, Redwane and Amos, Ryan B. and Argyle, Lisa P. and Baer-Bositis, Livia and B{\"u}chi, Moritz and Chung, Bo-Ryehn and Eggert, William and Faletto, Gregory and Fan, Zhilin and Freese, Jeremy and Gadgil, Tejomay and Gagn{\'e}, Josh and Gao, Yue and Halpern-Manners, Andrew and Hashim, Sonia P. and Hausen, Sonia and He, Guanhua and Higuera, Kimberly and Hogan, Bernie and Horwitz, Ilana M. and Hummel, Lisa M. and Jain, Naman and Jin, Kun and Jurgens, David and Kaminski, Patrick and Karapetyan, Areg and Kim, E. H. and Leizman, Ben and Liu, Naijia and M{\"o}ser, Malte and Mack, Andrew E. and Mahajan, Mayank and Mandell, Noah and Marahrens, Helge and Mercado-Garcia, Diana and Mocz, Viola and Mueller-Gastell, Katariina and Musse, Ahmed and Niu, Qiankun and Nowak, William and Omidvar, Hamidreza and Or, Andrew and Ouyang, Karen and Pinto, Katy M. and Porter, Ethan and Porter, Kristin E. and Qian, Crystal and Rauf, Tamkinat and Sargsyan, Anahit and Schaffner, Thomas and Schnabel, Landon and Schonfeld, Bryan and Sender, Ben and Tang, Jonathan D. and Tsurkov, Emma and van Loon, Austin and Varol, Onur and Wang, Xiafei and Wang, Zhi and Wang, Julia and Wang, Flora and Weissman, Samantha and Whitaker, Kirstie and Wolters, Maria K. and Woon, Wei Lee and Wu, James and Wu, Catherine and Yang, Kengran and Yin, Jingwen and Zhao, Bingyu and Zhu, Chenyun and Brooks-Gunn, Jeanne and Engelhardt, Barbara E. and Hardt, Moritz and Knox, Dean and Levy, Karen and Narayanan, Arvind and Stewart, Brandon M. and Watts, Duncan J. and McLanahan, Sara},
	title = {Measuring the predictability of life outcomes with a scientific mass collaboration},
	volume = {117},
	number = {15},
	pages = {8398--8403},
	year = {2020},
	doi = {10.1073/pnas.1915006117},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/117/15/8398},
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{zhang2020hurtful,
  title={Hurtful words: quantifying biases in clinical contextual word embeddings},
  author={Zhang, Haoran and Lu, Amy X and Abdalla, Mohamed and McDermott, Matthew and Ghassemi, Marzyeh},
  booktitle={proceedings of the ACM Conference on Health, Inference, and Learning},
  pages={110--120},
  year={2020}
}

@article{citron.2008,
  title = {Technological Due Process},
  author = {Danielle Keats Citron},
  year = 2008,
  journal = {Wash. U. L. Rev.},
  volume = 1249,
  url = {https://openscholarship.wustl.edu/law_lawreview/vol85/iss6/2/}
}

@book{anderson2017private,
 ISBN = {9780691192246},
 URL = {http://www.jstor.org/stable/j.ctvc775n0},
 author = {Elizabeth Anderson},
 publisher = {Princeton University Press},
 title = {Private Government: How Employers Rule Our Lives (and Why We Don't Talk about It)},
 year = {2017}
}

@misc{christiano2016prosaic,
    title={Prosaic AI Alignment},
    url={https://ai-alignment.com/prosaic-ai-control-b959644d79c2},
    author={Paul Christiano},
    year={2016},
    month={Nov}
}

@misc{cotra2021narrow,
    title={The case for aligning narrowly superhuman models},
    url={https://ai-alignment.com/prosaic-ai-control-b959644d79c2},
    author={Ajeya Cotra},
    year={2021},
}

@article{Kenton2021AlignmentOL,
  title={Alignment of Language Agents},
  author={Zachary Kenton and Tom Everitt and Laura Weidinger and Iason Gabriel and Vladimir Mikulik and Geoffrey Irving},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.14659}
}

@article{McGuffie2020TheRR,
  title={The Radicalization Risks of GPT-3 and Advanced Neural Language Models},
  author={Kris McGuffie and Alex Newhouse},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.06807}
}

@article{branwen2020gpt,
  title={GPT-3 creative fiction},
  author={Branwen, Gwern},
  year={2020}
}

@misc{brockman2020math,
    title={Math - GPT\_Prompts},
    url={http://gptprompts.wikidot.com/logic:math#toc5},
    author={Matt Brockman},
    year={2020},
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@article{Florensa2017StochasticNN,
  title={Stochastic Neural Networks for Hierarchical Reinforcement Learning},
  author={Carlos Florensa and Yan Duan and P. Abbeel},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.03012}
}

@article{Pathak2017CuriosityDrivenEB,
  title={Curiosity-Driven Exploration by Self-Supervised Prediction},
  author={Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2017},
  pages={488-489}
}

@inproceedings{Precup2000EligibilityTF,
  title={Eligibility Traces for Off-Policy Policy Evaluation},
  author={Doina Precup and R. Sutton and Satinder Singh},
  booktitle={ICML},
  year={2000}
}

@inproceedings{liu2018imitation,
  title={Imitation from observation: Learning to imitate behaviors from raw video via context translation},
  author={Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1118--1125},
  year={2018},
  organization={IEEE}
}

@article{co2018guiding,
  title={Guiding policies with language via meta-learning},
  author={Co-Reyes, John D and Gupta, Abhishek and Sanjeev, Suvansh and Altieri, Nick and Andreas, Jacob and DeNero, John and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1811.07882},
  year={2018}
}

@article{misra2017mapping,
  title={Mapping instructions and visual observations to actions with reinforcement learning},
  author={Misra, Dipendra and Langford, John and Artzi, Yoav},
  journal={arXiv preprint arXiv:1704.08795},
  year={2017}
}

@article{Singh2021ParrotDB,
  title={Parrot: Data-Driven Behavioral Priors for Reinforcement Learning},
  author={Avi Singh and Huihan Liu and Gaoyue Zhou and Albert Yu and Nicholas Rhinehart and Sergey Levine},
  journal={ArXiv},
  year={2021},
  volume={abs/2011.10024}
}

@article{Ajay2021OPALOP,
  title={OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning},
  author={Anurag Ajay and Aviral Kumar and Pulkit Agrawal and Sergey Levine and Ofir Nachum},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.13611}
}

@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@inproceedings{Yang2021RepresentationMO,
  title={Representation Matters: Offline Pretraining for Sequential Decision Making},
  author={Mengjiao Yang and Ofir Nachum},
  booktitle={ICML},
  year={2021}
}

@article{milano2020recommender,
  title={Recommender systems and their ethical challenges},
  author={Milano, Silvia and Taddeo, Mariarosaria and Floridi, Luciano},
  journal={AI \& SOCIETY},
  volume={35},
  number={4},
  pages={957--967},
  year={2020},
  publisher={Springer}
}

@article{burr2018analysis,
  title={An analysis of the interaction between intelligent software agents and human users},
  author={Burr, Christopher and Cristianini, Nello and Ladyman, James},
  journal={Minds and machines},
  volume={28},
  number={4},
  pages={735--774},
  year={2018},
  publisher={Springer}
}

@incollection{goodhart1984problems,
  title={Problems of monetary management: the UK experience},
  author={Goodhart, Charles AE},
  booktitle={Monetary theory and practice},
  pages={91--121},
  year={1984},
  publisher={Springer}
}

@article{yudkowsky2008artificial,
  title={Artificial intelligence as a positive and negative factor in global risk},
  author={Yudkowsky, Eliezer and others},
  journal={Global catastrophic risks},
  volume={1},
  number={303},
  pages={184},
  year={2008}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{Julian_2019,
   title={Guaranteeing Safety for Neural Network-Based Aircraft Collision Avoidance Systems},
   ISBN={9781728106496},
   url={http://dx.doi.org/10.1109/DASC43569.2019.9081748},
   DOI={10.1109/dasc43569.2019.9081748},
   journal={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)},
   publisher={IEEE},
   author={Julian, Kyle D. and Kochenderfer, Mykel J.},
   year={2019},
   month={Sep}
}

@inproceedings{soares2015corrigibility,
  title={Corrigibility},
  author={Soares, Nate and Fallenstein, Benja and Armstrong, Stuart and Yudkowsky, Eliezer},
  booktitle={Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

@misc{bostrom2014superintelligence,
  title={Superintelligence: Paths, strategies, dangers},
  author={Bostrom, Nick},
  year={2014},
  publisher={Oxford: OUP}
}

@book{bostrom2011global,
  title={Global catastrophic risks},
  author={Bostrom, Nick and Cirkovic, Milan M},
  year={2011},
  publisher={Oxford University Press}
}

@article{everitt2018agi,
  title={AGI safety literature review},
  author={Everitt, Tom and Lea, Gary and Hutter, Marcus},
  journal={arXiv preprint arXiv:1805.01109},
  year={2018}
}
@misc{ginart2019making,
      title={Making AI Forget You: Data Deletion in Machine Learning}, 
      author={Antonio Ginart and Melody Y. Guan and Gregory Valiant and James Zou},
      year={2019},
      eprint={1907.05012},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTERACTION ENDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VISION BEGINS

@incollection{park2019gaugan,
  title={GauGAN: semantic image synthesis with spatially adaptive normalization},
  author={Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
  booktitle={ACM SIGGRAPH 2019 Real-Time Live!},
  pages={1--1},
  year={2019}
}

@article{hudson2021ganformer,
  title={Generative Adversarial Transformers},
  author={Hudson, Drew A and Zitnick, C. Lawrence},
  journal={Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021},
  year={2021}
}


@inproceedings{zamir2018taskonomy,
 title={Taskonomy: Disentangling Task Transfer Learning},
 author={Amir R. Zamir and Alexander Sax and William B. Shen and Leonidas J. Guibas and Jitendra Malik and Silvio Savarese},
 booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year={2018},
 organization={IEEE},
}
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}



@article{feifei2007we,
  title={{What do We Perceive in a Glance of a Real-World Scene?}},
  author={Fei-Fei, Li and Iyer, Asha and Koch, Christof and Perona, Pietro},
  journal={{J}ournal of {V}ision},
  volume={7},
  number={1},
  pages={10--10},
  year={2007},
  publisher={The Association for Research in Vision and Ophthalmology}
}

@inproceedings{haque2017towards,
  title={Towards vision-based smart hospitals: a system for tracking and monitoring hand hygiene compliance},
  author={Haque, Albert and Guo, Michelle and Alahi, Alexandre and Yeung, Serena and Luo, Zelun and Rege, Alisha and Jopling, Jeffrey and Downing, Lance and Beninati, William and Singh, Amit and others},
  booktitle={Machine Learning for Healthcare Conference},
  pages={75--87},
  year={2017},
  organization={PMLR}
}

@article{lyytinen2002ubiquitous,
  title={Ubiquitous computing},
  author={Lyytinen, Kalle and Yoo, Youngjin},
  journal={Communications of the ACM},
  volume={45},
  number={12},
  pages={63--96},
  year={2002},
  publisher={Citeseer}
}

@inproceedings{hong2004architecture,
  title={An architecture for privacy-sensitive ubiquitous computing},
  author={Hong, Jason I and Landay, James A},
  booktitle={Proceedings of the 2nd international conference on Mobile systems, applications, and services},
  pages={177--189},
  year={2004}
}

@article{delbracio2021mobile,
  title={Mobile Computational Photography: A Tour},
  author={Delbracio, Mauricio and Kelly, Damien and Brown, Michael S and Milanfar, Peyman},
  journal={arXiv preprint arXiv:2102.09000},
  year={2021}
}

@inproceedings{sermanet2018time,
  title={Time-contrastive networks: Self-supervised learning from video},
  author={Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={1134--1141},
  year={2018},
  organization={IEEE}
}

@inproceedings{raji2020saving,
  title={Saving face: Investigating the ethical concerns of facial recognition auditing},
  author={Raji, Inioluwa Deborah and Gebru, Timnit and Mitchell, Margaret and Buolamwini, Joy and Lee, Joonseok and Denton, Emily},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={145--151},
  year={2020}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  year={2017},
  publisher={Cambridge University Press}
}

@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@article{alyamkin2019low,
  title={Low-power computer vision: Status, challenges, and opportunities},
  author={Alyamkin, Sergei and Ardi, Matthew and Berg, Alexander C and Brighton, Achille and Chen, Bo and Chen, Yiran and Cheng, Hsin-Pai and Fan, Zichen and Feng, Chen and Fu, Bo and others},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={2},
  pages={411--421},
  year={2019},
  publisher={IEEE}
}

@inproceedings{goel2020survey,
  title={A survey of methods for low-power deep learning and computer vision},
  author={Goel, Abhinav and Tung, Caleb and Lu, Yung-Hsiang and Thiruvathukal, George K},
  booktitle={2020 IEEE 6th World Forum on Internet of Things (WF-IoT)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{feng2019computer,
  title={Computer vision algorithms and hardware implementations: A survey},
  author={Feng, Xin and Jiang, Youni and Yang, Xuejiao and Du, Ming and Li, Xin},
  journal={Integration},
  volume={69},
  pages={309--320},
  year={2019},
  publisher={Elsevier}
}

@article{zhai2021scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.04560},
  year={2021}
}

@inproceedings{kolesnikov2020big,
  title={Big transfer (bit): General visual representation learning},
  author={Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part V 16},
  pages={491--507},
  year={2020},
  organization={Springer}
}

@article{szlam2019build,
  title={Why Build an Assistant in Minecraft?},
  author={Szlam, Arthur and Gray, Jonathan and Srinet, Kavya and Jernite, Yacine and Joulin, Armand and Synnaeve, Gabriel and Kiela, Douwe and Yu, Haonan and Chen, Zhuoyuan and Goyal, Siddharth and others},
  journal={arXiv preprint arXiv:1907.09273},
  year={2019}
}

@article{haque2020illuminating,
  title={Illuminating the dark spaces of healthcare with ambient intelligence},
  author={Haque, Albert and Milstein, Arnold and Fei-Fei, Li},
  journal={Nature},
  volume={585},
  number={7824},
  pages={193--202},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{mcclelland1981interactive,
  title={An interactive activation model of context effects in letter perception: I. An account of basic findings.},
  author={McClelland, James L and Rumelhart, David E},
  journal={Psychological review},
  volume={88},
  number={5},
  pages={375},
  year={1981},
  publisher={American Psychological Association}
}

@article{lowe1992robust,
  title={Robust model-based motion tracking through the integration of search and estimation},
  author={Lowe, David G},
  journal={International Journal of Computer Vision},
  volume={8},
  number={2},
  pages={113--122},
  year={1992},
  publisher={Springer}
}

@article{lowe2004distinctive,
  title={Distinctive image features from scale-invariant keypoints},
  author={Lowe, David G},
  journal={International journal of computer vision},
  volume={60},
  number={2},
  pages={91--110},
  year={2004},
  publisher={Springer}
}

@inproceedings{bay2006surf,
  title={Surf: Speeded up robust features},
  author={Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
  booktitle={European conference on computer vision},
  pages={404--417},
  year={2006},
  organization={Springer}
}

@inproceedings{rosten2006machine,
  title={Machine learning for high-speed corner detection},
  author={Rosten, Edward and Drummond, Tom},
  booktitle={European conference on computer vision},
  pages={430--443},
  year={2006},
  organization={Springer}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@article{li2020behrt,
  title={BEHRT: transformer for electronic health records},
  author={Li, Yikuan and Rao, Shishir and Solares, Jose Roberto Ayala and Hassaine, Abdelaali and Ramakrishnan, Rema and Canoy, Dexter and Zhu, Yajie and Rahimi, Kazem and Salimi-Khorshidi, Gholamreza},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{rasmy2021med,
  title={Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction},
  author={Rasmy, Laila and Xiang, Yang and Xie, Ziqian and Tao, Cui and Zhi, Degui},
  journal={NPJ digital medicine},
  volume={4},
  number={1},
  pages={1--13},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{simonyan2014two,
  title={Two-stream convolutional networks for action recognition in videos},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1406.2199},
  year={2014}
}
@article{krishna2020generating,
  title={Generating soap notes from doctor-patient conversations},
  author={Krishna, Kundan and Khosla, Sopan and Bigham, Jeffrey P and Lipton, Zachary C},
  journal={arXiv preprint arXiv:2005.01795},
  year={2020}
}
@article{demner2020consumer,
  title={Consumer health information and question answering: helping consumers find answers to their health-related information needs},
  author={Demner-Fushman, Dina and Mrabet, Yassine and Ben Abacha, Asma},
  journal={Journal of the American Medical Informatics Association},
  volume={27},
  number={2},
  pages={194--201},
  year={2020},
  publisher={Oxford University Press}
}
@article{krishna2016visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={arXiv preprint arXiv:1602.07332},
  year={2016}
}
@inproceedings{laina2016deeper,
  title={Deeper depth prediction with fully convolutional residual networks},
  author={Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
  booktitle={2016 Fourth international conference on 3D vision (3DV)},
  pages={239--248},
  year={2016},
  organization={IEEE}
}
@article{agarwal2011building,
  title={Building rome in a day},
  author={Agarwal, Sameer and Furukawa, Yasutaka and Snavely, Noah and Simon, Ian and Curless, Brian and Seitz, Steven M and Szeliski, Richard},
  journal={Communications of the ACM},
  volume={54},
  number={10},
  pages={105--112},
  year={2011},
  publisher={ACM New York, NY, USA}
}
@inproceedings{wang2015designing,
  title={Designing deep networks for surface normal estimation},
  author={Wang, Xiaolong and Fouhey, David and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={539--547},
  year={2015}
}

@inproceedings{huang2021seeing,
  title={Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12976--12985},
  year={2021}
}
@article{luo2020univl,
  title={Univl: A unified video and language pre-training model for multimodal understanding and generation},
  author={Luo, Huaishao and Ji, Lei and Shi, Botian and Huang, Haoyang and Duan, Nan and Li, Tianrui and Li, Jason and Bharti, Taroon and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.06353},
  year={2020}
}
@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Linagzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={arXiv preprint arXiv:2104.11178},
  year={2021}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

inproceedings{anderson2018vision,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3674--3683},
  year={2018}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{xu2016ask,
  title={Ask, attend and answer: Exploring question-guided spatial attention for visual question answering},
  author={Xu, Huijuan and Saenko, Kate},
  booktitle={European Conference on Computer Vision},
  pages={451--466},
  year={2016},
  organization={Springer}
}

@book{gibson1979ecological,
  title={The ecological approach to visual perception},
  author={Gibson, James J},
  year={1979},
  publisher={Psychology Press}
}

@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{tsirikoglou2020survey,
  title={A survey of image synthesis methods for visual machine learning},
  author={Tsirikoglou, Apostolia and Eilertsen, Gabriel and Unger, Jonas},
  booktitle={Computer Graphics Forum},
  volume={39},
  number={6},
  pages={426--451},
  year={2020},
  organization={Wiley Online Library}
}

@article{galatolo2021generating,
  title={Generating images from caption and vice versa via CLIP-Guided Generative Latent Space Search},
  author={Galatolo, Federico A and Cimino, Mario GCA and Vaglini, Gigliola},
  journal={arXiv preprint arXiv:2102.01645},
  year={2021}
}

@book{moravec1988mind,
  title={Mind children: The future of robot and human intelligence},
  author={Moravec, Hans},
  year={1988},
  publisher={Harvard University Press}
}

@article{mitchell1989model,
  title={A model-based computer vision system for recognizing handwritten ZIP codes},
  author={Mitchell, Brian T and Gillies, Andrew M},
  journal={Machine Vision and Applications},
  volume={2},
  number={4},
  pages={231--243},
  year={1989},
  publisher={Springer}
}

@inproceedings{wan2014deep,
  title={Deep learning for content-based image retrieval: A comprehensive study},
  author={Wan, Ji and Wang, Dayong and Hoi, Steven Chu Hong and Wu, Pengcheng and Zhu, Jianke and Zhang, Yongdong and Li, Jintao},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={157--166},
  year={2014}
}

@inproceedings{gordo2016deep,
  title={Deep image retrieval: Learning global representations for image search},
  author={Gordo, Albert and Almaz{\'a}n, Jon and Revaud, Jerome and Larlus, Diane},
  booktitle={European conference on computer vision},
  pages={241--257},
  year={2016},
  organization={Springer}
}

@inproceedings{podlesnaya2016deep,
  title={Deep learning based semantic video indexing and retrieval},
  author={Podlesnaya, Anna and Podlesnyy, Sergey},
  booktitle={Proceedings of SAI intelligent systems conference},
  pages={359--372},
  year={2016},
  organization={Springer}
}

@article{jean2016combining,
  title={Combining satellite imagery and machine learning to predict poverty},
  author={Jean, Neal and Burke, Marshall and Xie, Michael and Davis, W Matthew and Lobell, David B and Ermon, Stefano},
  journal={Science},
  volume={353},
  number={6301},
  pages={790--794},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@article{yi2019clevrer,
  title={Clevrer: Collision events for video representation and reasoning},
  author={Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:1910.01442},
  year={2019}
}

@article{bear2021physion,
  title={Physion: Evaluating Physical Prediction from Vision in Humans and Machines},
  author={Bear, Daniel M and Wang, Elias and Mrowca, Damian and Binder, Felix J and Tung, Hsiau-Yu Fish and Pramod, RT and Holdaway, Cameron and Tao, Sirui and Smith, Kevin and Fei-Fei, Li and others},
  journal={arXiv preprint arXiv:2106.08261},
  year={2021}
}

@article{bakhtin2019phyre,
  title={Phyre: A new benchmark for physical reasoning},
  author={Bakhtin, Anton and van der Maaten, Laurens and Johnson, Justin and Gustafson, Laura and Girshick, Ross},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={5082--5093},
  year={2019}
}

@inproceedings{mildenhall2020nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  booktitle={European conference on computer vision},
  pages={405--421},
  year={2020},
  organization={Springer}
}

@article{sitzmann2019scene,
  title={Scene representation networks: Continuous 3d-structure-aware neural scene representations},
  author={Sitzmann, Vincent and Zollh{\"o}fer, Michael and Wetzstein, Gordon},
  journal={arXiv preprint arXiv:1906.01618},
  year={2019}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{interactive,
  title={Craftassist: A framework for dialogue-enabled interactive agents},
  author={Gray, Jonathan and Srinet, Kavya and Jernite, Yacine and Yu, Haonan and Chen, Zhuoyuan and Guo, Demi and Goyal, Siddharth and Zitnick, C Lawrence and Szlam, Arthur},
  journal={arXiv preprint arXiv:1907.08584},
  year={2019}
}

@inproceedings{embodied,
    title={Habitat: A Platform for Embodied AI Research},
    author={Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and others},
    booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
    pages={9338--9346},
    year={2019},
    organization={IEEE Computer Society}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017}
}

@article{ullman1979interpretation,
  title={The interpretation of structure from motion},
  author={Ullman, Shimon},
  journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume={203},
  number={1153},
  pages={405--426},
  year={1979},
  publisher={The Royal Society London}
}

@article{gat,
  title={Graph Attention Networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  journal={arXiv e-prints},
  pages={arXiv--1710},
  year={2017}
}


@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}

@inproceedings{Omohundro2008TheBA,
  title={The Basic AI Drives},
  author={S. Omohundro},
  booktitle={AGI},
  year={2008}
}

@article{cammarata2020thread,
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  title = {Thread: Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits},
  doi = {10.23915/distill.00024}
}

@article{Hubinger2019RisksFL,
  title={Risks from Learned Optimization in Advanced Machine Learning Systems},
  author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.01820}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@inproceedings{zellers2019vcr,
  author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  title = {From Recognition to Cognition: Visual Commonsense Reasoning},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2019}
}

@article{martin2021jrdb,
title={ {JRDB}: A Dataset and Benchmark of Egocentric Robot Visual Perception of Humans in Built Environments},
author={Martin-Martin*, Roberto and Patel, Mihir and Rezatofighi*, Hamid and Shenoi, Abhijeet and Gwak, JunYoung and Frankel, Eric and Sadeghian, Amir and Savarese, Silvio},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence ({TPAMI})},
year={2021}
}

@inproceedings{changpinyo2021cc12m,
  title = {{Conceptual 12M}: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle = {CVPR},
  year = {2021},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VISION ENDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


%%%%%% EDUCATION BEGINS %%%%%% 

@article{connor2019using,
  title={Using technology and assessment to personalize instruction: Preventing reading problems},
  author={Connor, Carol McDonald},
  journal={Prevention Science},
  volume={20},
  number={1},
  pages={89--99},
  year={2019},
  publisher={Springer}
}

@inproceedings{jensen2020toward,
  title={Toward automated feedback on teacher discourse to enhance teacher learning},
  author={Jensen, Emily and Dale, Meghan and Donnelly, Patrick J and Stone, Cathlyn and Kelly, Sean and Godley, Amanda and D'Mello, Sidney K},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2020}
}
@article{demszky2021measuring,
  title={Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions},
  author={Demszky, Dorottya and Liu, Jing and Mancenido, Zid and Cohen, Julie and Hill, Heather and Jurafsky, Dan and Hashimoto, Tatsunori},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2021}
}

@article{pardo2017provision,
  title={Provision of data-driven student feedback in la \& EDM},
  author={Pardo, Abelardo and Poquet, Oleksandra and Mart{\'\i}nez-Maldonado, Roberto and Dawson, Shane},
  journal={Handbook of learning analytics},
  pages={163--174},
  year={2017},
  publisher={Society for Learning Analytics Research (SoLAR)}
}

@inproceedings{malik2021generative,
  title={Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems},
  author={Malik, Ali and Wu, Mike and Vasavada, Vrinda and Song, Jinpeng and Coots, Madison and Mitchell, John and Goodman, Noah and Piech, Chris},
  booktitle={Proceedings of the 14th International Conference on Educational Data Mining},
  year={2021}
}

@misc{unsdg2015,
title = {Transforming Our World: The 2030 Agenda for Sustainable Development}, 
author = {{United Nations General Assembly}},
year={2015},
url={https://www.refworld.org/docid/57b6e3e44.html}
}

@article{woolf2013aied, title={AI Grand Challenges for Education}, volume={34}, url={https://ojs.aaai.org/index.php/aimagazine/article/view/2490}, DOI={10.1609/aimag.v34i4.2490}, number={4}, journal={AI Magazine}, author={Woolf, Beverly Park and Lane, H. Chad and Chaudhri, Vinay K. and Kolodner, Janet L.}, year={2013}, month={Dec.}, pages={66-84} }

@article{truax2018edlang,
author = {Megan L. Truax},
title = {The Impact of Teacher Language and Growth Mindset Feedback on Writing Motivation},
journal = {Literacy Research and Instruction},
volume = {57},
number = {2},
pages = {135-157},
year  = {2018},
publisher = {Routledge},
doi = {10.1080/19388071.2017.1340529},
URL = { https://doi.org/10.1080/19388071.2017.1340529},
eprint = { https://doi.org/10.1080/19388071.2017.1340529}
}

@inproceedings{dixon2018bias,
author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
title = {Measuring and Mitigating Unintended Bias in Text Classification},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278729},
doi = {10.1145/3278721.3278729},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {67–73},
numpages = {7},
keywords = {natural language processing, machine learning, fairness, algorithmic bias, text classification},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{nasrPrivacy2018,
author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
title = {Machine Learning with Membership Privacy Using Adversarial Regularization},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243855},
doi = {10.1145/3243734.3243855},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {634–646},
numpages = {13},
keywords = {membership privacy, min-max game, inference attacks, data privacy, indistinguishability, machine learning, adversarial process},
location = {Toronto, Canada},
series = {CCS '18}
}

@inproceedings{songRemember2017,
author = {Song, Congzheng and Ristenpart, Thomas and Shmatikov, Vitaly},
title = {Machine Learning Models That Remember Too Much},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134077},
doi = {10.1145/3133956.3134077},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {587–601},
numpages = {15},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{chandrasekaran2019reply,
  title={When to reply? context sensitive models to predict instructor interventions in mooc forums},
  author={Chandrasekaran, Muthu Kumar and Kan, Min-Yen},
  journal={arXiv preprint arXiv:1905.10851},
  year={2019}
}

@inproceedings{alrajhi2021urgency,
  title={Urgency Analysis of Learners' Comments: An Automated Intervention Priority Model for MOOC},
  author={Alrajhi, Laila and Alamri, Ahmed and Pereira, Filipe Dwan and Cristea, Alexandra I},
  booktitle={International Conference on Intelligent Tutoring Systems},
  pages={148--160},
  year={2021},
  organization={Springer}
}


@InProceedings{gulwani2013automated,
author = {Gulwani, Sumit and Singh, Rishabh},
title = {Automated Feedback Generation for Introductory Programming Assignments},
booktitle = {ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2013)},
year = {2013},
month = {July},
url = {https://www.microsoft.com/en-us/research/publication/automated-feedback-generation-for-introductory-programming-assignments/},
pages = {15-26},
edition = {ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2013)},
}

@article{Liu2020MockingjayUS,
  title={Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders},
  author={Andy T. Liu and Shuwen Yang and Po-Han Chi and Po-Chun Hsu and Hung-yi Lee},
  journal={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2020},
  pages={6419-6423}
}

@inproceedings{srivastava2021question, 
author = {Srivastava, Megha and Goodman, Noah}, 
booktitle = {Association for Computational Linguistics (ACL)}, 
title = {Question Generation for Adaptive Education}, 
year = {2021}, 
website = {https://arxiv.org/abs/2106.04262} }

@article{mckenzie2003pedagogy,
  title={Pedagogy Does Matter!},
  author={McKenzie, Jamie},
  journal={The Educational Technology Journal},
  volume={13},
  number={1},
  year={2003}
}

@article{wu2021prototransf,
  title={ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback},
  author={Wu, Mike and Piech, Chris and Goodman, Noah and Finn, Chelsea},
  journal={arxiv},
  year={2021}
}

@inproceedings{condor2021sbert,
  title={Automatic short answer grading with SBERT on out-of-sample questions},
  author={Condor, Aubrey and Litster, Max and Pardos, Zachary},
  booktitle={Proceedings of the 14th International Conference on Educational Data Mining},
  year={2021}
}

@inproceedings{mandel2014rleducgames,
author = {Mandel, Travis and Liu, Yun-En and Levine, Sergey and Brunskill, Emma and Popovic, Zoran},
title = {Offline Policy Evaluation across Representations with Applications to Educational Games},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1077–1084},
numpages = {8},
keywords = {importance sampling, policy evaluation, pomdps, offline learning, educational games, reinforcement learning},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{kolchinski2018adanlfeedback,
author = {Kolchinski, Y. Alex and Ruan, Sherry and Schwartz, Dan and Brunskill, Emma},
title = {Adaptive Natural-Language Targeting for Student Feedback},
year = {2018},
isbn = {9781450358866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231644.3231684},
doi = {10.1145/3231644.3231684},
booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
articleno = {26},
numpages = {4},
keywords = {natural language processing, intelligent tutoring systems, adaptive feedback},
location = {London, United Kingdom},
series = {L@S '18}
}

@inproceedings{doroudi2017robusevalmatrix,
author = {Doroudi, Shayan and Aleven, Vincent and Brunskill, Emma},
title = {Robust Evaluation Matrix: Towards a More Principled Offline Exploration of Instructional Policies},
year = {2017},
isbn = {9781450344500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3051457.3051463},
doi = {10.1145/3051457.3051463},
booktitle = {Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale},
pages = {3–12},
numpages = {10},
keywords = {policy estimation, reinforcement learning, instructional policies, off-policy, policy selection},
location = {Cambridge, Massachusetts, USA},
series = {L@S '17}
}

Key Phrase Extraction for Generating Educational Question-Answer Pairs [pdf] 
Angelica Willis, Glenn Davis, Lakshmi Manoharan, Sherry Ruan, James Landay, and Emma Brunskill 
Learning at Scale (L@S) 2019,

@inproceedings{willis2019keyphrase,
author = {Willis, Angelica and Davis, Glenn and Ruan, Sherry and Manoharan, Lakshmi and Landay, James and Brunskill, Emma},
title = {Key Phrase Extraction for Generating Educational Question-Answer Pairs},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333636},
doi = {10.1145/3330430.3333636},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {20},
numpages = {10},
keywords = {Automatic answer extraction, Educational question generation, Educational content generation, Recurrent neural networks},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@inproceedings{Guo2016questimator,
author = {Guo, Qi and Kulkarni, Chinmay and Kittur, Aniket and Bigham, Jeffrey P. and Brunskill, Emma},
title = {Questimator: Generating Knowledge Assessments for Arbitrary Topics},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {3726–3732},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{woods2017formative,
  title={Formative essay feedback using predictive scoring models},
  author={Woods, Bronwyn and Adamson, David and Miel, Shayne and Mayfield, Elijah},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={2071--2080},
  year={2017}
}

@inproceedings{woods2017formative,
  title={Formative essay feedback using predictive scoring models},
  author={Woods, Bronwyn and Adamson, David and Miel, Shayne and Mayfield, Elijah},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={2071--2080},
  year={2017}
}


@inproceedings{clark2018creative,
  title={Creative writing with a machine in the loop: Case studies on slogans and stories},
  author={Clark, Elizabeth and Ross, Anne Spencer and Tan, Chenhao and Ji, Yangfeng and Smith, Noah A},
  booktitle={23rd International Conference on Intelligent User Interfaces},
  pages={329--340},
  year={2018}
}

@inproceedings{nilforoshan2018leveraging,
  title={Leveraging quality prediction models for automatic writing feedback},
  author={Nilforoshan, Hamed and Wu, Eugene},
  booktitle={Twelfth International AAAI Conference on Web and Social Media},
  year={2018}
}


%%%%%% EDUCATION ENDS %%%%%% 

%%%%%% LANGUAGE  %%%%%% 

@inproceedings{wav2vec2,
	author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
	pages = {12449--12460},
	publisher = {Curran Associates, Inc.},
	title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
	url = {https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fb a3227870bb6d7f07-Paper.pdf},
	volume = {33},
	year = {2020}}


@Incollection{lu2020,
author="Lu, Kaiji
and Mardziel, Piotr
and Wu, Fangjing
and Amancharla, Preetam
and Datta, Anupam",
editor="Nigam, Vivek
and Ban Kirigin, Tajana
and Talcott, Carolyn
and Guttman, Joshua
and Kuznetsov, Stepan
and Thau Loo, Boon
and Okada, Mitsuhiro",
title="Gender Bias in Neural Natural Language Processing",
bookTitle="Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="189--202",
abstract="We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark data sets finds significant gender bias in how models view occupations. We then mitigate bias with counterfactual data augmentation (CDA): a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.",
isbn="978-3-030-62077-6",
doi="10.1007/978-3-030-62077-6_14",
url="https://doi.org/10.1007/978-3-030-62077-6_14"
}

@article{Hestness2017DeepLS,
  title={Deep Learning Scaling is Predictable, Empirically},
  author={J. Hestness and Sharan Narang and Newsha Ardalani and G. Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Y. Yang and Yanqi Zhou},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.00409}
}

@inproceedings{zhao2018,
    title = "Learning Gender-Neutral Word Embeddings",
    author = "Zhao, Jieyu  and
      Zhou, Yichao  and
      Li, Zeyu  and
      Wang, Wei  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1521",
    doi = "10.18653/v1/D18-1521",
    pages = "4847--4853",
    abstract = "Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",
}

@inproceedings{zhao2019,
    title = "Gender Bias in Contextualized Word Embeddings",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Cotterell, Ryan  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1064",
    doi = "10.18653/v1/N19-1064",
    pages = "629--634",
    abstract = "In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo{'}s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",
}

@inproceedings{park2018,
    title = "Reducing Gender Bias in Abusive Language Detection",
    author = "Park, Ji Ho  and
      Shin, Jamin  and
      Fung, Pascale",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1302",
    doi = "10.18653/v1/D18-1302",
    pages = "2799--2804",
    abstract = "Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, {``}You are a good woman{''} was considered {``}sexist{''} when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98{\%} and can be extended to correct model bias in other scenarios.",
}

@inproceedings{saha2020,
author = {Saha, Debjani and Schumann, Candice and McElfresh, Duncan C. and Dickerson, John P. and Mazurek, Michelle L. and Tschantz, Michael Carl},
title = {Human Comprehension of Fairness in Machine Learning},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375819},
doi = {10.1145/3375627.3375819},
abstract = {Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications.Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {152},
numpages = {1},
keywords = {algorithmic bias, human-computer interaction, fair machine learning, empirical study},
location = {New York, NY, USA},
series = {AIES '20}
}

@misc{CorbettDavies2018,
      title={The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning},
      author={Sam Corbett-Davies and Sharad Goel},
      year={2018},
      eprint={1808.00023},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{zhao2020training,
  title={Training confounder-free deep learning models for medical applications},
  author={Zhao, Qingyu and Adeli, Ehsan and Pohl, Kilian M},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--9},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{hong2020panda,
  title={A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference},
  author={Hong, Sanghyun and Kaya, Yi{\u{g}}itcan and Modoranu, Ionu{\c{t}}-Vlad and Dumitra{\c{s}}, Tudor},
  journal={arXiv preprint arXiv:2010.02432},
  year={2020}
}

@article{carril.2020,
title = {The impact of industry consolidation on government procurement: {E}vidence from {D}epartment of {D}efense contracting},
journal = {Journal of Public Economics},
volume = {184},
pages = {104141},
year = {2020},
doi = {https://doi.org/10.1016/j.jpubeco.2020.104141},
url = {https://www.sciencedirect.com/science/article/pii/S0047272720300050},
author = {Rodrigo Carril and Mark Duggan},
}

@article{stevenson.2021,
title = {Algorithmic Risk Assessment in the Hands of Humans},
author = {Megan T. Stevenson and Jennifer L. Doleac},
journal={SSRN},
year = 2021,
doi = {http://dx.doi.org/10.2139/ssrn.3489440 },
url = {https://ssrn.com/abstract=3489440},
}

@book{schull.2014,
author = {Sch{\"u}ll, Natasha Dow},
title = {Addiction by Design: {M}achine Gambling in {L}as {V}egas},
year = {2014},
isbn = {0691160880},
publisher = {Princeton University Press},
}

@book{thaler.2008,
  address = {New Haven, CT and London},
  author = {Thaler, Richard H. and Sunstein, Cass R.},
  isbn = {978-0-300-12223-7},
  publisher = {Yale University Press},
  year = 2008
}

@Inbook{calvo.2020,
author={Calvo, Rafael A.
and Peters, Dorian
and Vold, Karina
and Ryan, Richard M.",
editor="Burr, Christopher
and Floridi, Luciano},
title={Supporting Human Autonomy in {AI} Systems: {A} Framework for Ethical Enquiry},
bookTitle={Ethics of Digital Well-Being: {A} Multidisciplinary Approach},
year=2020,
publisher={Springer International Publishing},
pages={31--54},
isbn="978-3-030-50585-1",
doi="10.1007/978-3-030-50585-1_2",
url="https://doi.org/10.1007/978-3-030-50585-1_2"
}

@article{lucherinin.2021,
  author    = {Eli Lucherini and
               Matthew Sun and
               Amy A. Winecoff and
               Arvind Narayanan},
  title     = {{T-RECS:} {A} Simulation Tool to Study the Societal Impact of Recommender
               Systems},
  journal   = {CoRR},
  volume    = {abs/2107.08959},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.08959},
  archivePrefix = {arXiv},
  eprint    = {2107.08959},
}

@article{
}

@book{rosenblat.2018,
author = {Rosenblat, Alex},
title = {Uberland: {H}ow Algorithms Are Rewriting the Rules of Work},
year = {2018},
isbn = {0520298578},
publisher = {University of California Press},
}

@article{scheiber.2017,
  title = {How {U}ber Uses Psychological Tricks to Push Its Drivers’ Buttons},
  author = {Noam Scheiber},
  year = 2017,
  journal = {New York Times},
  month = {April},
  url = {https://www.nytimes.com/interactive/2017/04/02/technology/uber-drivers-psychological-tricks.html},
}

@article{allcott.2017,
Author = {Allcott, Hunt and Gentzkow, Matthew},
Title = {Social Media and Fake News in the 2016 Election},
Journal = {Journal of Economic Perspectives},
Volume = {31},
Number = {2},
Year = {2017},
Month = {May},
Pages = {211-36},
DOI = {10.1257/jep.31.2.211},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.211}}

@article{acquisti.2017,
author = {Acquisti, Alessandro and Adjerid, Idris and Balebako, Rebecca and Brandimarte, Laura and Cranor, Lorrie Faith and Komanduri, Saranga and Leon, Pedro Giovanni and Sadeh, Norman and Schaub, Florian and Sleeper, Manya and Wang, Yang and Wilson, Shomir},
title = {Nudges for Privacy and Security: Understanding and Assisting Users’ Choices Online},
year = {2017},
volume = {50},
number = {3},
url = {https://doi.org/10.1145/3054926},
doi = {10.1145/3054926},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {44},
}

@article{corbyn.2012,
  author = {Zoe Corbyn},
  title = {Facebook experiment boosts US voter turnout},
  year = 2012,
  doi = {https://doi.org/10.1038/nature.2012.11401},
}

@article{stevenson.2018.facebook,
  title={Facebook admits it was used to incite violence in {M}yanmar},
  author={Stevenson, Alexandra},
  journal={The New York Times},
  volume={6},
  year={2018}
}


@article{parker2003blink,
  title={In the blink of an eye: how vision sparked the big bang of evolution},
  author={Parker, Andrew},
  year={2003}
}

@article{rothchild2021c5t5,
title={C5T5: Controllable Generation of Organic Molecules with Transformers},
author={Daniel Rothchild and Alex Tamkin and Julie Yu and Ujval Misra and Joseph E. Gonzalez},
journal={arXiv preprint},
year={2021}
}

@article{Rives2019BiologicalSA,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={A. Rives and Siddharth Goyal and J. Meier and Demi Guo and Myle Ott and C. L. Zitnick and Jerry Ma and R. Fergus},
  journal={bioRxiv},
  year={2019}
}

@inproceedings{Saeed2021ContrastiveLO,
  title={Contrastive Learning of General-Purpose Audio Representations},
  author={Aaqib Saeed and David Grangier and Neil Zeghidour},
  booktitle={ICASSP},
  year={2021}
}

@inproceedings{Yin2020TaBERTPF,
  title={TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data},
  author={Pengcheng Yin and Graham Neubig and Wen-tau Yih and Sebastian Riedel},
  booktitle={ACL},
  year={2020}
}


@article{johnson2019billion,
  title={Billion-scale similarity search with gpus},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  year={2019},
  publisher={IEEE}
}

@misc{aharoni2020unsupervised,
      title={Unsupervised Domain Clusters in Pretrained Language Models}, 
      author={Roee Aharoni and Yoav Goldberg},
      year={2020},
      eprint={2004.02105},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Amodei2016ConcretePI,
  title={Concrete Problems in AI Safety},
  author={Dario Amodei and Christopher Olah and J. Steinhardt and P. F. Christiano and J. Schulman and Dandelion Man{\'e}},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.06565}
}
@article{hendryckssafety2021,
title={Unsolved Problems in ML Safety},
author={Dan Hendrycks and Nicholas Carlini and John Schulman and Tom Dietterich and Jacob Steinhardt},
journal={arXiv preprint},
year={2021}
}

@inproceedings{tsuchiya2018performance,
    title = "Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment",
    author = "Tsuchiya, Masatoshi",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1239",
}

@article{winner_artifacts_1980,
	title = {Do Artifacts Have Politics?},
	volume = {109},
	issn = {0011-5266},
	url = {http://www.jstor.org/stable/20024652},
	number = {1},
	urldate = {2021-08-10},
	journal = {Daedalus},
	author = {Winner, Langdon},
	year = {1980},
	note = {Publisher: The MIT Press},
	pages = {121-136},
}

@misc{rogaway_moral_nodate,
	title = {The Moral Character of Cryptographic Work},
	author = {Rogaway, Phillip},
	pages = {48},
	year = {2016},
}

@inproceedings{blodgett_language_2020,
	title = {Language (Technology) is Power: A Critical Survey of “Bias” in NLP},
	url = {https://aclanthology.org/2020.acl-main.485},
	doi = {10.18653/v1/2020.acl-main.485},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Barocas, Solon and Daumé III, Hal and Wallach, Hanna},
	month = jul,
	year = {2020},
	pages = {5454-5476},
}

@misc{askell_role_2019,
	title = {The Role of Cooperation in Responsible AI Development},
	url = {http://arxiv.org/abs/1907.04534},
	abstract = {In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.},
	author = {Askell, Amanda and Brundage, Miles and Hadfield, Gillian},
	month = jul,
	year = {2019},
}

@misc{mohammad_ethics_2021,
	title = {Ethics Sheets for AI Tasks},
	url = {http://arxiv.org/abs/2107.01183},
	author = {Mohammad, Saif M.},
	month = jul,
	year = {2021},
}

@misc{bernstein_esr_2021,
	title = {ESR: Ethics and Society Review of Artificial Intelligence Research},
	url = {http://arxiv.org/abs/2106.11521},
	author = {Bernstein, Michael S. and Levi, Margaret and Magnus, David and Rajala, Betsy and Satz, Debra and Waeiss, Charla},
	month = jul,
	year = {2021},
}

@article{zimmermann_stop_2021,
	title = {Stop Building Bad AI},
	url = {https://bostonreview.net/science-nature/annette-zimmermann-stop-building-bad-ai},
	journal = {Boston Review},
	author = {Zimmermann, Annette},
	month = jul,
	year = {2021},
}

@article{benjamin_ruha_bioethics, 
    title = {Informed Refusal: Toward a Justicebased Bioethics},
    author = {Benjamin, Ruha}, 
    journal = {Science, Technology, \& Human Values},
    year = {2016},
    month = {June}, 
    pages = {967-990}, 
}

@article{audra_simpson, 
    title = {On Ethnographic Refusal: Indigeneity, 'Voice' Colonial Citizenship},
    author = {Simpson, Audra}, 
    journal = {Junctures}, 
    year = {2007}, 
    month = dec,
}

@inproceedings {when_not_to_design, 
    title = {When the implication is not to design (technology)},
    author = {Baumer, Eric P.S and Silberman, M. Six},
    year = {2011},
    booktitle = {CHI '11: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems}, 
}



@article{ding.2021,
  author    = {Ming Ding and
               Zhuoyi Yang and
               Wenyi Hong and
               Wendi Zheng and
               Chang Zhou and
               Da Yin and
               Junyang Lin and
               Xu Zou and
               Zhou Shao and
               Hongxia Yang and
               Jie Tang},
  title     = {Cog{V}iew: {M}astering Text-to-Image Generation via Transformers},
  journal   = {CoRR},
  volume    = {abs/2105.13290},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.13290},
  archivePrefix = {arXiv},
  eprint    = {2105.13290},
  timestamp = {Tue, 01 Jun 2021 18:07:59 +0200},
}

@article{dodge.2021.documenting,
  author    = {Jesse Dodge and
               Maarten Sap and
               Ana Marasovic and
               William Agnew and
               Gabriel Ilharco and
               Dirk Groeneveld and
               Matt Gardner},
  title     = {Documenting the {E}nglish {C}olossal {C}lean {C}rawled {C}orpus},
  journal   = {CoRR},
  volume    = {abs/2104.08758},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08758},
  archivePrefix = {arXiv},
  eprint    = {2104.08758},
}

@article{MARGE2022101255,
    title = {Spoken language interaction with robots: Recommendations for future research},
    journal = {Computer Speech \& Language},
    volume = {71},
    pages = {101255},
    year = {2022},
    issn = {0885-2308},
    doi = {https://doi.org/10.1016/j.csl.2021.101255},
    url = {https://www.sciencedirect.com/science/article/pii/S0885230821000620},
    author = {Matthew Marge and Carol Espy-Wilson and Nigel G. Ward and Abeer Alwan and Yoav Artzi and Mohit Bansal and Gil Blankenship and Joyce Chai and Hal Daumé and Debadeepta Dey and Mary Harper and Thomas Howard and Casey Kennington and Ivana Kruijff-Korbayová and Dinesh Manocha and Cynthia Matuszek and Ross Mead and Raymond Mooney and Roger K. Moore and Mari Ostendorf and Heather Pon-Barry and Alexander I. Rudnicky and Matthias Scheutz and Robert St. Amant and Tong Sun and Stefanie Tellex and David Traum and Zhou Yu},
}

@book{friedman.2019,
  author = {Friedman, Batya and Hendry, David G.},
  title = {Value Sensitive Design: {S}haping Technology with Moral Imagination},
  year = {2019},
  isbn = {0262039532},
  publisher = {The MIT Press},
}

@article{prabhakaran.2020,
  author = {Vinodkumar Prabhakaran and Donald Martin, Jr.},
  title = {Participatory Machine Learning Using Community-Based System Dynamics},
  journal = {Health Hum Rights},
  year = 2020
  volume=22,
  number=2,
  month=December,
  pages={71--73},
}

@book{oneil.2016,
author = {O'Neil, Cathy},
title = {Weapons of Math Destruction: {H}ow Big Data Increases Inequality and Threatens Democracy},
year = {2016},
isbn = {0553418815},
publisher = {Crown Publishing Group},
address = {USA},
}

@article{calo.2021,
  title = {The Automated Administrative State: {A} Crisis of Legitimacy},
  author = {Ryan Calo and Danielle K. Citron},
  journal = {Emory Law Journal},
  volume = 40,
  issue = 4,
  year = 2021,
  url = {https://scholarlycommons.law.emory.edu/elj/vol70/iss4/1 }
}

@inproceedings{selbst.2018,
  title = {Fairness and Abstraction in Sociotechnical Systems},
  author = {Andrew D. Selbst and Danah Boyd and Sorelle Friedler and Suresh Venkatasubramanian and Janet Vertesi},
  booktitle = {Proceeedings of the Conference on Fairness, Accountability, and Transparency},
  year = 2018,
}

@misc{holland2018dataset,
      title={The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards}, 
      author={Sarah Holland and Ahmed Hosny and Sarah Newman and Joshua Joseph and Kasia Chmielinski},
      year={2018},
      eprint={1805.03677},
      archivePrefix={arXiv},
      primaryClass={cs.DB}
}

@misc{fazelpour2021diversity,
      title={Diversity in Sociotechnical Machine Learning Systems}, 
      author={Sina Fazelpour and Maria De-Arteaga},
      year={2021},
      eprint={2107.09163},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{vredenburgh2021freedom,
	title = {Freedom at Work},
	author = {Kate Vredenburgh},
	year = {2021},
	archivePrefix={PhilPapers}
}

@article{fazelpour2021bias,
  doi = {10.1111/phc3.12760},
  url = {https://doi.org/10.1111/phc3.12760},
  year = {2021},
  month = jun,
  publisher = {Wiley},
  volume = {16},
  number = {8},
  author = {Sina Fazelpour and David Danks},
  title = {Algorithmic bias: Senses,  sources,  solutions},
  journal = {Philosophy Compass}
}

@inproceedings{Danks2019,
  doi = {10.1145/3306618.3314228},
  url = {https://doi.org/10.1145/3306618.3314228},
  year = {2019},
  month = jan,
  publisher = {{ACM}},
  author = {David Danks},
  title = {The Value of Trustworthy {AI}},
  booktitle = {Proceedings of the 2019 {AAAI}/{ACM} Conference on {AI},  Ethics,  and Society}
}

@inproceedings{ke2021chextransfer,
  title={CheXtransfer: performance and parameter efficiency of ImageNet models for chest X-Ray interpretation},
  author={Ke, Alexander and Ellsworth, William and Banerjee, Oishi and Ng, Andrew Y and Rajpurkar, Pranav},
  booktitle={Proceedings of the Conference on Health, Inference, and Learning},
  pages={116--124},
  year={2021}
}

@inproceedings{verma2021towards,
  title={Towards domain-agnostic contrastive learning},
  author={Verma, Vikas and Luong, Thang and Kawaguchi, Kenji and Pham, Hieu and Le, Quoc},
  booktitle={International Conference on Machine Learning},
  pages={10530--10541},
  year={2021},
  organization={PMLR}
}

@inproceedings{
    anonymous2021cliport,
    title={{CLIP}ort: What and Where Pathways for Robotic Manipulation},
    author={Anonymous},
    booktitle={Submitted to 5th Annual Conference on Robot Learning },
    year={2021},
    url={https://openreview.net/forum?id=9uFiX_HRsIL},
    note={under review}
}

@misc{hendrycks2021natural,
      title={Natural Adversarial Examples}, 
      author={Dan Hendrycks and Kevin Zhao and Steven Basart and Jacob Steinhardt and Dawn Song},
      year={2021},
      eprint={1907.07174},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{belmont_report, 
    title = {The Belmont Report}, 
    author = { Department of Health, Education, and Welfare}, 
    year = {1979}
    month = {April}

}

@misc{shevlane2020offensedefense,
      title={The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?}, 
      author={Toby Shevlane and Allan Dafoe},
      year={2020},
      eprint={2001.00463},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{brundage2018malicious,
      title={The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation}, 
      author={Miles Brundage and Shahar Avin and Jack Clark and Helen Toner and Peter Eckersley and Ben Garfinkel and Allan Dafoe and Paul Scharre and Thomas Zeitzoff and Bobby Filar and Hyrum Anderson and Heather Roff and Gregory C. Allen and Jacob Steinhardt and Carrick Flynn and Seán Ó hÉigeartaigh and Simon Beard and Haydn Belfield and Sebastian Farquhar and Clare Lyle and Rebecca Crootof and Owain Evans and Michael Page and Joanna Bryson and Roman Yampolskiy and Dario Amodei},
      year={2018},
      eprint={1802.07228},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{aghajanyan2021HTLM,
  author    = {Armen Aghajanyan and
               Dmytro Okhonko and
               Mike Lewis and
               Mandar Joshi and
               Hu Xu and
               Gargi Ghosh and
               Luke Zettlemoyer},
  title     = {{HTLM:} Hyper-Text Pre-Training and Prompting of Language Models},
  journal   = {CoRR},
  volume    = {abs/2107.06955},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.06955},
  archivePrefix = {arXiv},
  eprint    = {2107.06955},
  timestamp = {Wed, 21 Jul 2021 15:55:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-06955.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Grady2015,
  doi = {10.1378/chest.15-0706},
  url = {https://doi.org/10.1378/chest.15-0706},
  year = {2015},
  month = nov,
  publisher = {Elsevier {BV}},
  volume = {148},
  number = {5},
  pages = {1148--1155},
  author = {Christine Grady},
  title = {Institutional Review Boards},
  journal = {Chest}
}

@book{stark_IRBs, 
    author = {Stark, Laura},
    title = {Behind Closed Doors: IRBs and the Making of Medical Research},
    publisher = {University of Chicago Press},
    year = {2012},
}

@misc{lawfare_norms, 
    author = {Crootof, Rebecca}, 
    title = {Artificial Intelligence Research Needs Responsible Publication Norms},
    year = {2019},
    howpublished = {Lawfare},
}

@inbook{changingtherules_ohm, 
    author = {Ohm, Paul}, 
    title = {Changing the Rules: General Principles for Data Use and Analysis}, 
    booktitle = {Privacy, Big Data, and the Public Good: Frameworks for Engagement},
    year = {2014}, 
    pages = {96 - 111}, 
    publisher = {Cambridge University Press}, 
}

@inbook{Taylor1994,
title = {The Politics of Recognition},
author = {Taylor, Charles},
year = {1994},
booktitle = {Multiculturalism: Examining the Politics of Recognition},
editor = {A. Gutmann},
publisher = {Princeton University Press},
address = {Princeton},
pages = {25–73}
}

@inproceedings{Dai2021,
  doi = {10.1145/3461702.3462521},
  url = {https://doi.org/10.1145/3461702.3462521},
  year = {2021},
  month = may,
  publisher = {{ACM}},
  author = {Jessica Dai and Sina Fazelpour and Zachary Lipton},
  title = {Fair Machine Learning Under Partial Compliance},
  booktitle = {Proceedings of the 2021 {AAAI}/{ACM} Conference on {AI},  Ethics,  and Society}
}

@inproceedings{Passi2019,
  doi = {10.1145/3287560.3287567},
  url = {https://doi.org/10.1145/3287560.3287567},
  year = {2019},
  month = jan,
  publisher = {{ACM}},
  author = {Samir Passi and Solon Barocas},
  title = {Problem Formulation and Fairness},
  booktitle = {Proceedings of the Conference on Fairness,  Accountability,  and Transparency}
}

@article{Floridi2018,
  doi = {10.1007/s11023-018-9482-5},
  url = {https://doi.org/10.1007/s11023-018-9482-5},
  year = {2018},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {28},
  number = {4},
  pages = {689--707},
  author = {Luciano Floridi and Josh Cowls and Monica Beltrametti and Raja Chatila and Patrice Chazerand and Virginia Dignum and Christoph Luetge and Robert Madelin and Ugo Pagallo and Francesca Rossi and Burkhard Schafer and Peggy Valcke and Effy Vayena},
  title = {{AI}4People{\textemdash}An Ethical Framework for a Good {AI} Society: Opportunities,  Risks,  Principles,  and Recommendations},
  journal = {Minds and Machines}
}

@misc{COMPAS_propublica, 
    author = {Julia Angwin and Jeff Larson and Surya Mattu and Lauren Kirchner}, 
    title = {Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks.},
    howpublished = {ProPublica},
    year = {2016}, 
    month = {May},
}

@article{Cowburn2005,
  doi = {10.1079/phn2005666},
  url = {https://doi.org/10.1079/phn2005666},
  year = {2005},
  month = feb,
  publisher = {Cambridge University Press ({CUP})},
  volume = {8},
  number = {1},
  pages = {21--28},
  author = {Gill Cowburn and Lynn Stockley},
  title = {Consumer understanding and use of nutrition labelling: a systematic review},
  journal = {Public Health Nutrition}
}

@techreport{jurassic1,
title = {Jurassic-1: Technical Details and Evaluation},
author = {Opher Lieber and Or Sharir and Barak Lenz and Yoav Shoham},
type = {White Paper},
year = {2021},
institution = {AI21 Labs}
}

@Book{Korsgaard2009,
 author = {Korsgaard, Christine},
 title = {Self-constitution : agency, identity, and integrity},
 publisher = {Oxford University Press},
 year = {2009},
 address = {Oxford New York},
 isbn = {9780199552795}
 }
 
 @incollection{Williams1973,
 author = {Bernard Williams},  booktitle = {Utilitarianism: For and Against},
 editor = {Smart, J.C. and Williams, Bernard},
publisher = {Cambridge University Press},
year = {1973}, 
pages = {82-118}
}

@article{Dillmann2004TeachingAL,
  title={Teaching and learning of robot tasks via observation of human performance},
  author={R. Dillmann},
  journal={Robotics Auton. Syst.},
  year={2004},
  volume={47},
  pages={109-116}
}

@article{goodrich2007hri,
  title={Human-Robot Interaction: A Survey},
  author={M. A. Goodrich and A. Schultz},
  journal={Found. Trends Hum. Comput. Interact.},
  year={2007},
  volume={1},
  pages={203-275}
}

@inproceedings{nair2018rig,
  title={Visual Reinforcement Learning with Imagined Goals},
  author={Ashvin Nair and Vitchyr H. Pong and Murtaza Dalal and Shikhar Bahl and Steven Lin and Sergey Levine},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{anonymous2021offlinelang,
title={Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation},
author={Anonymous},
booktitle={Submitted to 5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=tfLu5W6SW5J},
note={under review}
}

@article{Schmeckpeper2020ReinforcementLW,
  title={Reinforcement Learning with Videos: Combining Offline Observations with Interaction},
  author={K. Schmeckpeper and Oleh Rybkin and Kostas Daniilidis and Sergey Levine and Chelsea Finn},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.06507}
}

@article{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv:2102.12092},
  year={2021}
}

@article{thrun1995lifelong,
  title={Lifelong robot learning},
  author={S. Thrun and Tom Michael Mitchell},
  journal={Robotics Auton. Syst.},
  year={1995},
  volume={15},
  pages={25-46}
}

@inproceedings{brooks2002flesh,
  title={Flesh and Machines: How Robots Will Change Us},
  author={R. Brooks},
  year={2002}
}

@inproceedings{gupta2018homes,
  title={Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias},
  author={A. Gupta and Adithyavairavan Murali and Dhiraj Gandhi and Lerrel Pinto},
  booktitle={NeurIPS},
  year={2018}
}

% ===========================================================
% Broken/Missing Cites -- Contact Sidd if questions!

%%% Fixed/Broken VISION Cites
@article{henaff2021efficient,
  title={Efficient visual pretraining with contrastive detection},
  author={H{\'e}naff, Olivier J and Koppula, Skanda and Alayrac, Jean-Baptiste and Oord, Aaron van den and Vinyals, Oriol and Carreira, Jo{\~a}o},
  journal={ICCV},
  year={2021}
}

@article{he2019moco,
  author  = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
  title   = {Momentum Contrast for Unsupervised Visual Representation Learning},
  journal = {arXiv preprint arXiv:1911.05722},
  year    = {2019},
}

@inproceedings{selvaraju2021casting,
  title={Casting your model: Learning to localize improves self-supervised representations},
  author={Selvaraju*, Ramprasaath R and Desai*, Karan and Johnson, Justin and Naik, Nikhil},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11058--11067},
  year={2021}
}

@article{chen2020mocov2,
  author  = {Xinlei Chen and Haoqi Fan and Ross Girshick and Kaiming He},
  title   = {Improved Baselines with Momentum Contrastive Learning},
  journal = {arXiv preprint arXiv:2003.04297},
  year    = {2020},
}

@inproceedings{chen2021geosim,
  title={GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving},
  author={Chen, Yun and Rong, Frieda and Duggal, Shivam and Wang, Shenlong and Yan, Xinchen and Manivasagam, Sivabalan and Xue, Shangjie and Yumer, Ersin and Urtasun, Raquel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7230--7240},
  year={2021}
}

@inproceedings{li2020visual,
    Title={Visual Grounding of Learned Physical Models},
    Author={Li, Yunzhu and Lin, Toru and Yi, Kexin and Bear, Daniel and Yamins, Daniel L.K. and Wu, Jiajun and Tenenbaum, Joshua B. and Torralba, Antonio},
    Booktitle={ICML},
    Year={2020}
}

@article{zhang2017shape,
  title={Shape and material from sound},
  author={Zhang, Zhoutong and Li, Qiujia and Huang, Zhengjia and Wu, Jiajun and Tenenbaum, Joshua B and Freeman, William T},
  year={2017},
  publisher={Neural Information Processing Systems}
}

@inproceedings{gao2020visualechoes,
  title = {VisualEchoes: Spatial Image Representation Learning through Echolocation},
  author = {Gao, Ruohan and Chen, Changan and Al-Halab, Ziad and Schissler, Carl and Grauman, Kristen},
  booktitle = {ECCV},
  year = {2020}
}

@article{jaegle2021perceiverio,
  title={Perceiver IO: A General Architecture for Structured Inputs \& Outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}

@article{zacks2001perceiving,
  title={Perceiving, remembering, and communicating structure in events.},
  author={Zacks, Jeffrey M and Tversky, Barbara and Iyer, Gowri},
  journal={Journal of experimental psychology: General},
  volume={130},
  number={1},
  pages={29},
  year={2001},
  publisher={American Psychological Association}
}

@article{tversky2013event,
  title={Event perception},
  author={Tversky, Barbara and Zacks, Jeffrey M},
  journal={Oxford handbook of cognitive psychology},
  pages={83--94},
  year={2013},
  publisher={Oxford University Press Oxford}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7464--7473},
  year={2019}
}

@article{kolve2017ai2thor,
  author = {Eric Kolve and Roozbeh Mottaghi and Daniel Gordon and Yuke Zhu and Abhinav Gupta and Ali Farhadi},
  journal = {arXiv preprint arXiv:1712.05474},
  title = {AI2-THOR: An Interactive 3D Environment for Visual {AI}},
  year = {2017},
}

@inproceedings{habitat19iccv,
  title     =     {Habitat: {A} {P}latform for {E}mbodied {AI} {R}esearch},
  author    =     {{Manolis Savva*} and {Abhishek Kadian*} and {Oleksandr Maksymets*} and Yili Zhao and Erik Wijmans and Bhavana Jain and Julian Straub and Jia Liu and Vladlen Koltun and Jitendra Malik and Devi Parikh and Dhruv Batra},
  booktitle =     {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      =     {2019}
}

@article{shen2021igibson,
  title={iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes},
  author={Shen, Bokui and Xia, Fei and Li, Chengshu and Mart{\i}n-Mart{\i}n, Roberto and Fan, Linxi and Wang, Guanzhi and D'Arpino, Claudia and Buch, Shyamal and Srivastava, Sanjana and Tchapmi, Lyne P and  Vainio, Kent and Fei-Fei, Li and Savarese, Silvio},
  journal={International Conference on Intelligent Robots and Systems (IROS)},
  year={2021}
}

@article{srivastava2021behavior,
  title={BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments},
  author={Srivastava, Sanjana and Li, Chengshu and Lingelbach, Michael and Mart{\'\i}n-Mart{\'\i}n, Roberto and Xia, Fei and Vainio, Kent and Lian, Zheng and Gokmen, Cem and Buch, Shyamal and Liu, C Karen and others},
  journal={arXiv preprint arXiv:2108.03332},
  year={2021}
}


@article{zhou2019hype,
  title={Hype: A benchmark for human eye perceptual evaluation of generative models},
  author={Zhou*, Sharon and Gordon*, Mitchell L and Krishna, Ranjay and Narcomey, Austin and Fei-Fei, Li and Bernstein, Michael S},
  journal={NeurIPS},
  year={2019}
}

@article{khashabi2021genie,
  title={Genie: A leaderboard for human-in-the-loop evaluation of text generation},
  author={Khashabi, Daniel and Stanovsky, Gabriel and Bragg, Jonathan and Lourie, Nicholas and Kasai, Jungo and Choi, Yejin and Smith, Noah A and Weld, Daniel S},
  journal={arXiv preprint arXiv:2101.06561},
  year={2021}
}

@article{conser2019revisiting,
  title={Revisiting Visual Grounding},
  author={Conser, Erik and Hahn, Kennedy and Watson, Chandler M and Mitchell, Melanie},
  journal={arXiv preprint arXiv:1904.02225},
  year={2019}
}

@article{agarwal2021evaluating,
  title={Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications},
  author={Agarwal, Sandhini and Krueger, Gretchen and Clark, Jack and Radford, Alec and Kim, Jong Wook and Brundage, Miles},
  journal={arXiv preprint arXiv:2108.02818},
  year={2021}
}

@article{dolhansky2020deepfake,
  title={The deepfake detection challenge dataset},
  author={Dolhansky, Brian and Bitton, Joanna and Pflaum, Ben and Lu, Jikuo and Howes, Russ and Wang, Menglin and Canton Ferrer, Cristian},
  journal={arXiv e-prints},
  pages={arXiv--2006},
  year={2020}
}


%%% Fixed/Broken REASONING Cites
@inproceedings{DBLP:conf/nips/WangD20,
  author    = {Mingzhe Wang and
               Jia Deng},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Learning to Prove Theorems by Learning to Generate Theorems},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/d2a27e83d429f0dcae6b937cf440aeb1-Abstract.html},
  timestamp = {Mon, 01 Feb 2021 18:33:35 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/WangD20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/pldi/EllisWNSMHCST21,
  author    = {Kevin Ellis and
               Catherine Wong and
               Maxwell I. Nye and
               Mathias Sabl{\'{e}}{-}Meyer and
               Lucas Morales and
               Luke B. Hewitt and
               Luc Cary and
               Armando Solar{-}Lezama and
               Joshua B. Tenenbaum},
  editor    = {Stephen N. Freund and
               Eran Yahav},
  title     = {DreamCoder: bootstrapping inductive program synthesis with wake-sleep
               library learning},
  booktitle = {{PLDI} '21: 42nd {ACM} {SIGPLAN} International Conference on Programming
               Language Design and Implementation, Virtual Event, Canada, June 20-25,
               20211},
  pages     = {835--850},
  publisher = {{ACM}},
  year      = {2021},
  url       = {https://doi.org/10.1145/3453483.3454080},
  doi       = {10.1145/3453483.3454080},
  timestamp = {Mon, 21 Jun 2021 13:42:02 +0200},
  biburl    = {https://dblp.org/rec/conf/pldi/EllisWNSMHCST21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{MillerGalanterPribram60,
  added-at = {2008-09-16T23:39:07.000+0200},
  address = {New York},
  author = {Miller, G. A. and E., Galanter and Pribram, K. H.},
  biburl = {https://www.bibsonomy.org/bibtex/2cf99dc730deecb73cb295e8a5d6d6fbb/brian.mingus},
  date-added = {2007-09-05 02:57:34 -0600},
  date-modified = {2007-09-05 02:59:06 -0600},
  description = {CCNLab BibTeX},
  interhash = {b0e28e8d160b8978a8c7d711280a8448},
  intrahash = {cf99dc730deecb73cb295e8a5d6d6fbb},
  keywords = {imported},
  publisher = {Holt},
  timestamp = {2008-09-16T23:40:42.000+0200},
  title = {Plans and the structure of behavior},
  year = 1960
}

@article{DBLP:journals/corr/abs-2103-03809,
  author    = {Xuezixiang Li and
               Qu Yu and
               Heng Yin},
  title     = {PalmTree: Learning an Assembly Language Model for Instruction Embedding},
  journal   = {CoRR},
  volume    = {abs/2103.03809},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.03809},
  archivePrefix = {arXiv},
  eprint    = {2103.03809},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-03809.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Jamnik2001MathematicalRW,
  title={Mathematical Reasoning with Diagrams},
  author={M. Jamnik},
  year={2001}
}

@inproceedings{hong21latent,
  author = {Hong, Joey and Dohan, David and Singh, Rishabh and Sutton, Charles and Zaheer, Manzil},
  booktitle = {International Conference in Machine Learning (ICML)},
  title = {Latent Programmer: Discrete Latent Codes for Program Synthesis},
  year = {2021}
}

%%% Fixed/Broken HEALTHCARE Cites
@article{tatonetti2012data,
  title={Data-driven prediction of drug effects and interactions},
  author={Tatonetti, Nicholas P and Patrick, P Ye and Daneshjou, Roxana and Altman, Russ B},
  journal={Science translational medicine},
  volume={4},
  number={125},
  pages={125ra31--125ra31},
  year={2012},
  publisher={American Association for the Advancement of Science}
}

@article{klasnja2012healthcare,
  title={Healthcare in the pocket: mapping the space of mobile-phone health interventions},
  author={Klasnja, Predrag and Pratt, Wanda},
  journal={Journal of biomedical informatics},
  volume={45},
  number={1},
  pages={184--198},
  year={2012},
  publisher={Elsevier}
}

%%% Fixed/Broken EDUCATION Cites
@article{suresh2021using,
  title={Using Transformers to Provide Teachers with Personalized Feedback on their Classroom Discourse: The TalkMoves Application},
  author={Suresh, Abhijit and Jacobs, Jennifer and Lai, Vivian and Tan, Chenhao and Ward, Wayne and Martin, James H and Sumner, Tamara},
  journal={arXiv preprint arXiv:2105.07949},
  year={2021}
}

@misc{shen2021mathbert,
      title={MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education}, 
      author={Jia Tracy Shen and Michiharu Yamashita and Ethan Prihar and Neil Heffernan and Xintao Wu and Dongwon Lee},
      year={2021},
      eprint={2106.07340},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bowen2012cost,
  title={The ``cost disease'' in higher education: is technology the answer?},
  author={Bowen, William G},
  journal={The Tanner Lectures Stanford University},
  year={2012}
}

@misc{friedman2020debt, 
    title={Student loan debt statistics in 2020: A record \$1.6 trillion}, url={https://www.forbes.com/sites/zackfriedman/2020/02/03/student-loan-debt-statistics/?sh=34191d3281fe}, 
    journal={Forbes}, 
    publisher={Forbes Magazine}, 
    author={Friedman, Zack}, 
    year={2020}, 
    month={Feb}
} 

%%% Fixed/Broken EVALUATION Cites
@article{levy2008,
title = {Expectation-based syntactic comprehension},
journal = {Cognition},
volume = {106},
number = {3},
pages = {1126-1177},
year = {2008},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2007.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027707001436},
author = {Roger Levy},
keywords = {Parsing, Frequency, Sentence processing, Information theory, Prediction, Syntax, Word order, Syntactic complexity},
abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159–166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.}
}

@article{frank2013,
  title={Reading time data for evaluating broad-coverage models of English sentence processing},
  author={S. Frank and Irene Fernandez Monsalve and Robin L. Thompson and G. Vigliocco},
  journal={Behavior Research Methods},
  year={2013},
  volume={45},
  pages={1182-1190}
}

@inproceedings{ettinger2016,
    title = "Evaluating vector space models using human semantic priming results",
    author = "Ettinger, Allyson  and
      Linzen, Tal",
    booktitle = "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP}",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2513",
    doi = "10.18653/v1/W16-2513",
    pages = "72--77",
}

@inproceedings{marvin2018,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@inproceedings{van-schijndel2018,
    title = "A Neural Model of Adaptation in Reading",
    author = "van Schijndel, Marten  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1499",
    doi = "10.18653/v1/D18-1499",
    pages = "4704--4710",
    abstract = "It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context. We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model. We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.",
}

@inproceedings{prasad2019,
    title = "Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models",
    author = "Prasad, Grusha  and
      van Schijndel, Marten  and
      Linzen, Tal",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1007",
    doi = "10.18653/v1/K19-1007",
    pages = "66--76",
    abstract = "Neural language models (LMs) perform well on tasks that require sensitivity to syntactic structure. Drawing on the syntactic priming paradigm from psycholinguistics, we propose a novel technique to analyze the representations that enable such success. By establishing a gradient similarity metric between structures, this technique allows us to reconstruct the organization of the LMs{'} syntactic representational space. We use this technique to demonstrate that LSTM LMs{'} representations of different types of sentences with relative clauses are organized hierarchically in a linguistically interpretable manner, suggesting that the LMs track abstract properties of the sentence.",
}

@article{ettinger2020,
    title = "What {BERT} Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
    author = "Ettinger, Allyson",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://aclanthology.org/2020.tacl-1.3",
    doi = "10.1162/tacl_a_00298",
    pages = "34--48",
    abstract = "Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction{---} and, in particular, it shows clear insensitivity to the contextual impacts of negation.",
}

@article{greenwald1998,
  title={{Measuring individual differences in implicit cognition: The implicit association test.}},
  author={Greenwald, Anthony G. and McGhee, Debbie E. and Schwartz, Jordan L.K.},
  journal={Journal of Personality and Social Psychology},
  volume={74},
  number={6},
  pages={1464},
  year={1998},
  publisher={American Psychological Association},
  url={https://psycnet.apa.org/record/1998-02892-004}
}

%%% Fixed/Broken THEORY Cites
@inproceedings{gunasekar2018implicit,
    title={Implicit bias of gradient descent on linear convolutional networks},
    author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
    booktitle={Advances in Neural Information Processing Systems},
    pages={9461--9471},
    year={2018}
}

@inproceedings{arora2019implicit,
    title={Implicit regularization in deep matrix factorization},
    author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
    booktitle={Advances in Neural Information Processing Systems},
    pages={7411--7422},
    year={2019}
}

@article{woodworth2020kernel,
    title={Kernel and rich regimes in overparametrized models},
    author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
    journal={arXiv preprint arXiv:2002.09277},
    year={2020}
}

@inproceedings{wei2020implicit, 
    title={The Implicit and Explicit Regularization Effects of Dropout}, 
    author={Wei, Colin and Kakade, Sham and Ma, Tengyu}, 
    booktitle={International Conference on Machine Learning}, 
    year={2020} 
}

@inproceedings{haochen2021shape,
    title={Shape matters: Understanding the implicit bias of the noise covariance},
    author={HaoChen, Jeff Z and Wei, Colin and Lee, Jason and Ma, Tengyu},
    booktitle={Conference on Learning Theory},
    pages={2315--2357},
    year={2021},
    organization={PMLR}
}

@article{blanc2019implicit,
    title={Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process},
    author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
    journal={arXiv preprint arXiv:1904.09080},
    year={2019}
}

@misc{damian2021label,
    title={Label Noise SGD Provably Prefers Flat Global Minimizers}, 
    author={Alex Damian and Tengyu Ma and Jason Lee},
    year={2021},
    eprint={2106.06530},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%%% Fixed/Broken INEQUITY/FAIRNESS CITES
@article{hooker2020,
  title={Characterising Bias in Compressed Models},
  author={Sara Hooker and Nyalleng Moorosi and Gregory Clark and Samy Bengio and Emily L. Denton},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.03058},
  url={https://arxiv.org/abs/2010.03058}
}

@inproceedings{renduchintala2021,
    title = "Gender bias amplification during Speed-Quality optimization in Neural Machine Translation",
    author = "Renduchintala, Adithya  and
      Diaz, Denise  and
      Heafield, Kenneth  and
      Li, Xian  and
      Diab, Mona",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.15",
    doi = "10.18653/v1/2021.acl-short.15",
    pages = "99--109",
    abstract = "Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.",
}

%%% Fixed/Broken MISUSE Cites
@article{diresta2021,
  author = {Ren{\'e}e DiResta and Shelby Grossman and Alexandra Siegel},
  institution = {Working Paper},
  title = {In-House vs. Outsourced Trolls:
How Digital Mercenaries Shape State Influence Strategies},
  year = {2021}
}

%%% Fixed/Broken LEGALITY Cites
 @misc{usco,
  author = {U.S. Copyright Office},
  howpublished = {\url{https://www.copyright.gov/fair-use/more-info.html}},
  title = {More Information on Fair Use},
  year = {2021},
  month = {May}
}

@article{wu2021medical,
  title={How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals},
  author={Wu, Eric and Wu, Kevin and Daneshjou, Roxana and Ouyang, David and Ho, Daniel E and Zou, James},
  journal={Nature Medicine},
  volume={27},
  number={4},
  pages={582--584},
  year={2021},
  publisher={Nature Publishing Group}
}

%%% Fixed/Broken ETHICS OF SCALE Cites
@Inbook{Arendt1987,
    author = {Arendt, Hannah},
    editor = {Bernauer, S. J. James W.},
    title = {Collective Responsibility},
    bookTitle = {Amor Mundi: Explorations in the Faith and Thought of Hannah Arendt},
    year = {1987},
    publisher = {Springer Netherlands},
    address = {Dordrecht},
    pages = {43--50}
}

@misc{zimmerman_2020, 
    title={If You Can Do Things with Words, You Can Do Things with Algorithms}, url={https://dailynous.com/2020/07/30/philosophers-gpt-3/#zimmermann}, 
    journal={Philosophers on GPT-3}, 
    publisher={Daily Nous}, 
    author={Zimmerman, Annette}, 
    year={2020}, 
    month={Jul}
}

@article{Rini2020deepfakes,
    title = {Deepfakes and the {Epistemic} {Backstop}},
    author = {Regina Rini},
    journal = {Philosopher's Imprint},
    year = {2020},
    volume = {20},
    number = {24},
    pages = {1-16}
}

@inproceedings{dimanov2020you,
  title={You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods.},
  author={Dimanov, Botty and Bhatt, Umang and Jamnik, Mateja and Weller, Adrian},
  booktitle={SafeAI@ AAAI},
  year={2020}
}

 @book{hidalgo2021how,
 author = {Hidalgo, },
 title = {How {Humans} {Judge} {Machines}},
 publisher = {The MIT Press},
 year = {2021},
 address = {Cambridge, Massachusetts},
 isbn = {9780262045520}
 }

% ==========================================================
% New Additions =>

@article{guo2020graphcodebert,
  title={Graphcodebert: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}

@article{yasunaga2021break,
  title={Break-It-Fix-It: Unsupervised Learning for Program Repair},
  author={Yasunaga, Michihiro and Liang, Percy},
  journal={arXiv preprint arXiv:2106.06600},
  year={2021}
}

@article{ling2016latent,
  title={Latent predictor networks for code generation},
  author={Ling, Wang and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Senior, Andrew and Wang, Fumin and Blunsom, Phil},
  journal={arXiv preprint arXiv:1603.06744},
  year={2016}
}

@inproceedings{svyatkovskiy2020intellicode,
  title={Intellicode compose: Code generation using transformer},
  author={Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1433--1443},
  year={2020}
}

@inproceedings{kim2021code,
  title={Code prediction by feeding trees to transformers},
  author={Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={150--162},
  year={2021},
  organization={IEEE}
}

@article{zugner2021language,
  title={Language-agnostic representation learning of source code from structure and context},
  author={Z{\"u}gner, Daniel and Kirschstein, Tobias and Catasta, Michele and Leskovec, Jure and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2103.11318},
  year={2021}
}
